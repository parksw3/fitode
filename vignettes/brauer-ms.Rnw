% interacttfssample.tex
% v1.05 - August 2017

\documentclass[]{interact}

\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

%\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required
%\setlength\parindent{24pt}% To increase paragraph indentation when line spacing is doubled
%\setlength\bibindent{2em}% To increase hanging indent in bibliography when line spacing is doubled

\usepackage[numbers,sort&compress]{natbib}% Citation support using natbib.sty
\bibpunct[, ]{[}{]}{,}{n}{,}{,}% Citation support using natbib.sty
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

%% DE preamable
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{lineno}\renewcommand\thelinenumber{\color{gray}\arabic{linenumber}}
\usepackage{placeins}
\newcommand{\thickredline}{{\color{red}\bigskip\begin{center}\linethickness{2mm}\line(1,0){250}\end{center}\bigskip}}
\newcommand{\bmb}[1]{{\color{red}$\langle$\emph{BMB: #1}$\rangle$}}
\newcommand{\swp}[1]{{\color{blue}$\langle$\emph{SWP: #1}$\rangle$}}
\newcommand{\djde}[1]{{\color{magenta}$\langle$\emph{DJDE: #1}$\rangle$}}
\newcommand{\needref}{{\color{orange}[NEED REF]}}
\newcommand{\term}[1]{{\bfseries\slshape#1}}
\newcommand{\dt}{{\rm d}t}
\newcommand{\ddt}[1]{\dfrac{{\rm d}#1}{{\rm d}t}}
\newcommand{\dbydt}[1]{{\rm d}#1/{\rm d}t}
\newcommand{\sech}{\,\textrm{sech}}
\newcommand{\Ipeak}{I_{\rm p}}
\newcommand{\tpeak}{t_{\rm p}}
\newcommand{\R}{{\mathcal R}}
\newcommand{\Tg}{T_{\rm g}}
\newcommand{\Reff}{\R_{\rm e}}
\usepackage{xspace}
\newcommand{\KM}{KM\xspace}
\usepackage{bm} % bold math
\newcommand{\betavec}{\bm{\beta}}
\newcommand{\betavechat}{\bm{\hat\beta}}
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\usepackage{xspace}
\newcommand{\KMcol}{blue\xspace}
\newcommand{\nlscol}{red\xspace}
\newcommand{\fitodecol}{yellow\xspace}
\newcommand{\Pop}{{\mathbb{P}}} % probability operator
%% tables
\usepackage{multirow}   % for tables
%\usepackage{subfig}
\usepackage{tabularx}   % pretty tables
% \usepackage{tabularray} % allows rowsep via tblr environment
%% table details
\renewcommand{\arraystretch}{1.5} % more interline spacing
%% https://tex.stackexchange.com/questions/12703/how-to-create-fixed-width-table-columns-with-text-raggedright-centered-raggedlef
\usepackage{ragged2e}
%%\usepackage{array} % for m{...}

\begin{document}

\articletype{ORIGINAL ARTICLE}% Specify the article type or omit as appropriate

\title{Fitting epidemic models to data -- a tutorial\\in memory of Fred
  Brauer}

\author{
\name{David J.\,D.~Earn\textsuperscript{a}\thanks{CONTACT D.J.D.~Earn. Email: earn@math.mcmaster.ca. ORCID:
    0000-0002-7562-1341, Twitter: @David.J.D.Earn},
    Sang Woo Park\textsuperscript{b}\thanks{S.W.~Park. ORCID 0000-0003-2202-3361, Twitter: @sang\_woo\_park, Mastodon: @sangwoopark@ecoevo.social},
    and Benjamin M.~Bolker\textsuperscript{a}\thanks{B.M.~Bolker. ORCID: 0000-0002-2127-0443, Twitter: @bolkerb, Mastodon: @bbolker@fediscience.org}}
\affil{\textsuperscript{a}Department of Mathematics and Statistics,
  McMaster University, Hamilton, Ontario, Canada, L8S 4K1; \textsuperscript{b}Department of Ecology and Evolutionary Biology, Princeton University, Princeton, NJ 08544}
}

\maketitle

\linenumbers

\bigskip \bigskip

\begin{abstract}
  Fred Brauer had no desire to touch data himself, but recognized that
  fitting models to data is usually necessary when attempting to apply
  infectious disease transmission models to real public health
  problems.  He was curious to know how one goes about fitting
  dynamical models to data, and why it can be hard.  Initially in
  response to Fred's questions, we developed a user-friendly
  \code{R} package that facilitates fitting ordinary differential
    equations to observed time series.  Here, we use the package
  (\code{fitode}) to provide a brief tutorial introduction to
  fitting compartmental epidemic models to a single observed time
  series.  We assume that, similar to Fred, the reader is familiar
  with dynamical systems from a mathematical perspective, but has
  little or no experience with statistical methodology or optimization
  techniques.
\end{abstract}

\begin{keywords}
  epidemic models; infectious diseases; ordinary differential
  equations; parameter estimation; maximum likelihood; \code{fitode}
\end{keywords}

\section{Introduction}

In their landmark 1927 paper, Kermack and McKendrick (KM)
\cite[p.\,713]{KermMcKe27} introduced the now-standard
susceptible-infected-removed (SIR) epidemic model,
\begin{linenomath*}
  \begin{subequations}\label{eq:SIR}
    \begin{align}
      \ddt{S} &= -\beta S I \,, \\
      \noalign{\vspace{5pt}}
      \ddt{I} &= \beta S I - \gamma I \,, \\
      \noalign{\vspace{5pt}}
      \ddt{R} &= \gamma I\,, \label{eq:SIR;R}
    \end{align}
  \end{subequations}
\end{linenomath*}
where $S$, $I$ and $R$ represent the numbers of individuals who are
susceptible, infected or removed\footnote{In the words of \KM
  \cite[p.\,701]{KermMcKe27}, ``removed from the number of those who
  are sick, by recovery or by death''.}, $\beta$ is the transmission
rate, and $\gamma$ is the recovery rate.  In that original paper,
\KM \cite[p.\,714]{KermMcKe27} also fit their model
to plague mortality data from an epidemic in Bombay (now Mumbai) that
occurred about 20 years before their paper was written.

In the century that has elapsed since publication of \KM's initial
paper, the field of mathematical epidemiology has expanded and
matured, and has been the subject of many books
\cite{Bart60,Bail75,AndeMay91,AndeBrit00b,DiekHees00,BrauCast01,Brau+19}
and review articles \cite{Heth00,Earn+02,Earn09}.  Researchers have
primarily focused on \term{compartmental models} like the SIR model, cast
either as differential equations following the tradition of \KM
\cite{KermMcKe27}, or as \term{stochastic processes} in the tradition
of Bartlett \cite{Bart60}.  In recent years, as the power of computers
has grown exponentially, there has been increasing interest in
\term{agent-based models}, which represent each individual as
a separate unit that can have unique properties \cite{eubank2004modelling}.

\bmb{I would normally object to ``exponentially'' on pedantic grounds but here it's
actually justified \ldots}

Throughout the history of the subject, and regardless of the modelling
frameworks they have exploited, mathematical epidemiologists have
frequently attempted to fit---or at least to compare---their models to
observed infectious disease data. Such fits have often been
na\"{\i}ve, with limited consideration of their quality. Over the
years, however, there has been a trend towards greater sophistication and
statistical rigour in parameter estimation for infectious disease
models; books that explain these methods have begun to appear in
recent decades \cite{Bolk08,bjornstadEpidemics2018}.  Careful
consideration of uncertainty is especially important when epidemic
models are used for the development and analysis of policy options for
infectious disease management \cite{elderd2006uncertainty}, a
challenge that has absorbed the attention of many mathematical
epidemiologists during the course of the present COVID-19 pandemic
\cite{Hill+21}.
\swp{Can't find appropriate papers to cite here...}
\djde{I added the only one that came to mind.  There are probably
other articles of this type commenting on modelling working for
PH during the pandemic.}

While visiting the University of British Columbia in 2014--2015, one
of us (DE) had many conversations with Fred Brauer about epidemic
models and how they can be used in practical applications.  While he
had no desire to analyze data himself, Fred was acutely aware that
fitting to data is essential if one wishes to apply epidemic
models to real public health problems, and he did want to understand
what was involved in doing so.

Fred's curiosity inspired us to develop user-friendly software for
fitting ordinary differential equation (ODE) models to observed time
series, with the goal of illustrating the process and challenges of
model fitting to Fred and others like him, i.e., individuals who are
comfortable with mathematical analysis of ODEs but have little or no
experience with statistics and parameter estimation.  Unfortunately,
we have lost the opportunity to present our work to Fred, but it seems
fitting (!) \bmb{cute}  to present such a tutorial in this volume dedicated to
Fred's memory.

\section{Kermack and McKendrick's fit}

We begin by revisiting \KM's \cite{KermMcKe27}
application of their SIR model \eqref{eq:SIR} to the epidemic of
plague in Bombay in 1905--1906.  The observed data (black dots in
Figure~\ref{fig:Bombay}) were weekly numbers of deaths from plague.

Referring to their version of Figure~\ref{fig:Bombay}, \KM
\cite[p.\,714]{KermMcKe27} argued that ``As at least 80 to 90 per
cent.\ of the cases reported terminate fatally, the ordinate may be
taken as approximately representing [$\dbydt{R}$] as a function of
$t$.''  Since (non-human) computers did not yet exist \cite{Camp09},
and an exact analytical form for this function could not be found,
they proceeded to assume \cite[p.\,713]{KermMcKe27} that
$\frac{\beta}{\gamma}R(t)\ll1$, which yields the approximate
analytical form,
\begin{linenomath*}
\begin{equation}\label{eq:sech}
\ddt{R} \approx a \sech^2{(\omega\,t - \phi)}\,.
\end{equation}
\end{linenomath*}
Noting that the basic reproduction number is
\begin{linenomath*}
\begin{equation}\label{eq:R0def}
\R_0 = \frac{N\beta}{\gamma} \,,
\end{equation}
\end{linenomath*}
where $N$ is the total population size,
and hence the effective reproduction number at time $t=0$
is
\begin{linenomath*}
\begin{equation}\label{eq:Reffdef}
\Reff = \frac{S_0\beta}{\gamma} \,,
\end{equation}
\end{linenomath*}
the parameters in equation~\eqref{eq:sech} can be
written\footnote{There is a typographical error in equation (31) of
  \KM \cite{KermMcKe27}: their factor $\sqrt{-q}$ should be $(-q)$ in
  their equivalent of the parameter we call $a$.  Baca\"er
  \cite[\S3]{bacaermodel2012} corrected this error without comment.}
\begin{linenomath*}
\begin{subequations}\label{eq:sechparams}
\begin{align}
  \omega &= \frac{\gamma}{2} \sqrt{(\Reff-1)^2 +
           \frac{2I_0}{S_0}\Reff^2}\,, \label{eq:omega} \\
  \phi &= \textrm{arctanh}\left(\frac{\Reff - 1}{2\,\omega/\gamma}
         \right)\,, \label{eq:phi} \\
  \text{and}\quad
  a &= \frac{2\,\omega^2 S_0}{\gamma\,\Reff^2} \label{eq:a} \,.
\end{align}
\end{subequations}
\end{linenomath*}
\KM then presented the parameter estimates that we have listed in the
KM column of Table~\ref{tab:Bombay}, and they plotted their
``calculated'' curve, which we have reproduced in \KMcol in
Figure~\ref{fig:Bombay}.

<<functions to compute KM parameters, echo=FALSE>>=
omega_fun <- function(Reff, gamma, S0, I0)
    {(gamma/2)*sqrt((Reff-1)^2+(2*I0/S0)*Reff^2)}
phi_fun <- function(Reff, gamma, S0, I0)
    {atanh((Reff-1)/(2*omega_fun(Reff,gamma,S0,I0)/gamma))}
a_fun <- function(Reff, gamma, S0, I0)
    {(2 * S0/(gamma * Reff^2))*omega_fun(Reff,gamma,S0,I0)^2}
@

\subsection{How to fit the model to the data}

The \KMcol curve in Figure~\ref{fig:Bombay} does appear to provide a
reasonable fit to the data, but \KM \cite{KermMcKe27} gave no
indication of how their parameter estimates were obtained.  \djde{When
  was least-squares trajectory matching first done?  Do we think they
  did that?  or just played with parameter values and fitted by eye?}
\swp{It seems unlikely that they had the ability to perform least
  squares back in 1927... I was able to find least squares SIR and
  SI3R(!) model fit from 1990 \cite[p.\,51]{kryscio1990modeling} but
  not sure if there's anything earlier.  They could have used human
  computers, as Ben notes.}\djde{According to
  \href{https://en.wikipedia.org/wiki/Least_squares}{Wikipedia}, the
  method of least squares goes back to Legendre and Gauss, a century
  before KM.}
  \swp{Ah yes I see. But maybe better to focus on model fitting in
  the context of epidemic models?}
  Whatever their process, they must have engaged in some
sort of \term{trajectory matching}, i.e., adjusting parameter values
until the model---equation~\eqref{eq:sech} in their case---is, by some
measure, close to the observed data points.  The most obvious metric
for this purpose is the Euclidean distance between the model curve and
the data.  Thus, the \term{objective function} that is natural to
minimize is
\begin{linenomath*}
\begin{equation}\label{eq:leastsquares}
\sum_{i=1}^n \big(f(t_i;\betavec) - y_i\big)^2 \,,
\end{equation}
\end{linenomath*}
where the observed data are the points $\{(t_i,y_i):i=1,\ldots,n\}$,
$f(t;\betavec)$ is the model \eqref{eq:sech}, and the parameter
vector for \KM's problem is
$\betavec=(a,\omega,\phi)$.  Minimizing \eqref{eq:leastsquares} with
respect to $\betavec$ would have required some heroic arithmetic
with a pencil and paper in 1927, but it is a simple task with the
aid of a computer.

In the following segment of R code, we fit equation \eqref{eq:sech} to
the Bombay plague data (which are included in the \code{fitode}
package that we describe below, as a data frame with columns
\code{week} and \code{mort}).  We exploit R's nonlinear least squares
function (\code{nls}), which attempts to minimize the distance
\eqref{eq:leastsquares} to the data, starting from an initial guess
(\code{start}).

<<nls-bombay>>=
sech <- function(x) {1/cosh(x)}
KM_approx <- function(t, a, omega, phi) {a * sech(omega*t - phi)^2}
KM.parameters <- c(a = 890, omega = 0.2, phi = 3.4)
nlsfit <- nls(mort ~ KM_approx(week, a, omega, phi),
              data = fitode::bombay,
              start = KM.parameters)
nls.parameters <- coef(nlsfit)
print(nls.parameters)
@

\noindent
Above, we chose as our starting guess the fitted parameter values of
\KM (see Table~\ref{tab:Bombay}).  The least
squares parameter values differ from KM's by a few percent:

<<percent-diff>>=
percent.difference <-
    100*(KM.parameters - nls.parameters)/nls.parameters
print(percent.difference)
@

\noindent
Our least squares fitted function is shown in \nlscol in
Figure~\ref{fig:Bombay}.

Starting from someone else's fit is not a great way to test the
method, but fortunately the least squares fit for this problem is not
very sensitive to the initial guess.  While not necessary for this
problem, to make a reasonable initial guess, it can help to think
about the meaning of parameters.  For example, in the case of
equation~\eqref{eq:sech}, it is useful to note that $a$ is the maximum
of the function~\eqref{eq:sech}, and if write $\omega t - \phi$ as
$\omega(t-\tpeak)$ then
\begin{linenomath*}
  \begin{equation}\label{eq:tpeak}
    \tpeak = \frac{\phi}{\omega}
  \end{equation}
\end{linenomath*}
is the time at which the maximum occurs; both $a$ and $\tpeak$ can be
guessed approximately by looking at the plotted observed data, and a
very rough guess is sufficient to converge on the same fit:

<<nls-bombay-2>>=
a.guess <- 1000    # crude "by eye" estimate of peak value
tpeak.guess <- 15  # crude "by eye" estimate of peak time
omega.guess <- 1   # wild guess
phi.guess <- omega.guess * tpeak.guess
nlsfit <- nls(mort ~ KM_approx(week, a, omega, phi),
              data = fitode::bombay,
              start = c(a = a.guess, omega = omega.guess,
                        phi = phi.guess))
print(nls.parameters <- coef(nlsfit))
@

However, if you experiment with initial guesses, you will find that if
you make a sufficiently \emph{bad} guess, then \code{nls} will fail.
For example, starting from $a=2000$, $\tpeak=5$, and $\omega=0.1$
yields a \code{singular gradient} error.  More interestingly, starting
from $a=500$, $\tpeak=5$, and $\omega=0.1$ yields $a=869$,
$\omega=-0.19$, $\phi=-3.48$, which is far from our fitted values and
illustrates a very important fact: there is \emph{not necessarily a
  unique best fit set of parameters!}  In this case, the alternative
solution exists because $\sech^2(x)$ is symmetric about the $y$ axis,
but in general, there can be multiple local minima that cause
\code{nls} to converge to points that may or may not represent equally
good fits to the data.  In general, the potential existence of
multiple local optima is something that makes fitting models to data
hard; you need to be cautious in interpreting any fit to which your
fitting algorithm has converged (Raue \emph{et al.}
\cite{raue2013lessons} give some suggestions for how to diagnose
multiple optima).

If you know that your parameters should be in a certain range, then
you can exclude values outside that range.  For example, to ensure
that all the parameters are non-negative (and exclude the alternative
fit above), you would add the \code{nls} option

<<eval=FALSE>>=
lower = c(a = 0, omega = 0, phi = 0)
@

\noindent
which would prevent convergence to negative $\omega$ and $\phi$.
Alternatively, you could write
\begin{linenomath*}
  \begin{equation}\label{eq:loglink}
    a=e^A,\quad \omega=e^\Omega,\quad \phi=e^\Phi \,,
  \end{equation}
\end{linenomath*}
and fit $A$, $\Omega$, and $\Phi$, which would guarantee positive $a$,
$\omega$, and $\phi$ without having to constrain the values of the
fitted parameters.  This last suggestion may just seem like a cute
trick, but there is more to it than that.  A much broader range of
optimization algorithms is available for unconstrained fitting;
numerical parameter values of very small magnitude can also lead to
numerical instability, so it is advantageous to link parameters that
must lie in a given range to unconstrained parameters that can be fit
more easily.  In equation~\eqref{eq:loglink}, the \term{link function}
that converts the parameters to the unconstrained scale is $\log$.
Another common link function is logit (the inverse of the logistic
function), which converts the unit interval $(0,1)$ to
$(-\infty,\infty)$, and is especially convenient when parameters
represent proportions or probabilities.  (Requiring positivity is so
common that the default setting in \code{fitode} is to use a log link
when fitting any parameter.)

<<KM-fitted-values, echo=FALSE>>=
parnames <- c("a", "omega", "phi")
dig <- 3 # number of significant digits for printing
for (p in parnames) {
    assign(paste0(p, ".KM"), signif(KM.parameters[[p]], dig))
}
## calc sig *after* taking the ratio ...
tpeak.KM <- with(as.list(KM.parameters), signif(phi / omega,dig))
@

<<nls-fitted-values, echo=FALSE>>=
for (p in parnames) {
    assign(paste0(p, ".nls"), signif(nls.parameters[[p]], dig))
}
tpeak.nls <- with(as.list(nls.parameters), signif(phi / omega,dig))
cc <- suppressMessages(confint(nlsfit))
ci_fmt  <- function(p, cc) (format(cc[p,], digits = 2)
    |> paste(collapse = ", ")
    |> sprintf(fmt = "(%s)")
)
for (p in parnames) {
    assign(paste0(p,".lwr"), cc[p, "2.5%"])
    assign(paste0(p,".upr"), cc[p, "97.5%"])
}
@

If we accept our fit as satisfactory, what can we infer about the
dynamics of plague that KM were attempting to capture with the SIR
model \eqref{eq:SIR}?  We need to convert the parameters of KM's
approximation \eqref{eq:sechparams} back to the original parameters
that are directly related to the mechanism of disease spread that the
model formalizes (i.e., $\beta$ and $\gamma$, and initial conditions
$S_0$ and $I_0$).

\FloatBarrier

The nonlinear algebraic relationships specified by
equation~\eqref{eq:sechparams} can be inverted
analytically\footnote{In (common) situations in which
  nonlinear algebraic equations cannot be solved analytically,
  they can still be solved numerically, for example with
  the \code{nleqslv} package in R.}
\cite[\S3]{bacaermodel2012}, to obtain
\begin{linenomath*}
\begin{subequations}\label{eq:invertparams}
\begin{align}
  \Reff &= \frac{1}{2}\bigg( 1 +
          \sqrt{1 + \frac{4\,\omega\,I_0 \sinh\!{(2\phi)}}{a}}\bigg) \,,
          \label{eq:Reff} \\
  \gamma &= \frac{2\,\omega\tanh{\phi}}{\Reff-1} \label{eq:gamma} \\
  S_0 &= \frac{2\,\Reff\,I_0 \sinh^2\!{\phi}}{(\Reff-1)^2} % Bacaer eq. (3)
      \label{eq:S0}
\end{align}
\end{subequations}
\end{linenomath*}
Since there are four original parameters ($\beta$, $\gamma$, $S_0$,
$I_0$) and only three parameters in \KM's approximation
\eqref{eq:sech} ($a$, $\omega$, $\phi$), one of the four original
parameters needs to be specified separately, and above we have taken
this to be the initial prevalence $I_0$.  From
equation~\eqref{eq:invertparams}, we can compute the transmission
rate,
\begin{linenomath*}
\begin{equation}\label{eq:beta}
  \beta = \frac{\Reff\gamma}{S_0} \,,
\end{equation}
\end{linenomath*}
and the mean intrinsic generation interval \cite{ChamDush15},
\begin{linenomath*}
\begin{equation}\label{eq:Tg}
  \Tg = \frac{1}{\gamma} \,,
\end{equation}
\end{linenomath*}
which is the same as the mean infectious period in this simple model
\cite{KrylEarn13,Cham+18}.  Table~\ref{tab:Bombay} lists the values of
the parameters as estimated by \KM and by us using \code{nls}.

\swp{The problem is that we're assuming S0 = 1e6 - 1. We can't do
  this.  We need much lower S0 to fit the data. So by assuming a very
  large S0, things go crazy.  So we're better off startng with
  assuming I0 = 1 and assuming the rest. That's the whole point of
  Bacaer's paper: that the required S0 to fit the data is so low that
  the standard SIR model is actually not appropriate for modeling the
  bombay epidemic} \djde{I'll return to this after reading Baca\"er
  \cite{bacaermodel2012}. Meanwhile I will note that Gani and Leach
  \cite{GaniLeac04} found $\R_0\simeq1.3\pm1.8$ and
  $\Tg\simeq6.8\pm2.2$ days, for ``modern'' pneumonic plague, and
  based on that $\Tg$ we \cite{Earn+20} found similar $\R_0$ for 17th
  century plague in London.  Baca\"er \cite{bacaermodel2012} seems to
  assume (like \KM) that plague in Bombay had a CFP of near 1.
  Anyway, the point of the discussion here will be to alert readers to
  be cautious for all sorts of reasons, and a good fit of a model to
  data does not necessarily mean that it is fair to make inferences
  from estimated parameter values.}  \djde{Note KM's caution about
  interpreting the param values.}

<<invert-KM-analytically, echo=FALSE>>=
invert_params <- function(I0, params) {
    with(as.list(params), {
        Reff <- (1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2
	q <- (Reff-1)/tanh(phi)
	gamma <- 2 * omega/q ## gamma has a unit of 1/week
	S0 <- 2 * Reff * I0 * sinh(phi)^2/(Reff - 1)^2
	beta <- Reff * gamma/S0
	c(Reff=Reff, S0=S0, gamma=gamma, Tg=1/gamma, beta=beta)
    })
}
N.KM <- 10^6
I0.KM <- 1
orig.params.KM <- invert_params(I0 = I0.KM, params = KM.parameters)
orig.params.KM <- signif(orig.params.KM,dig)
Reff.KM <- orig.params.KM[["Reff"]]
S0.KM <- orig.params.KM[["S0"]]
gamma.KM <- orig.params.KM[["gamma"]]
Tg.KM <- orig.params.KM[["Tg"]]
beta.KM <- orig.params.KM[["beta"]]
I0.nls <- 1
orig.params.nls <- invert_params(I0 = I0.nls, params = coef(nlsfit))
orig.params.nls <- signif(orig.params.nls,dig)
Reff.nls <- orig.params.nls[["Reff"]]
S0.nls <- orig.params.nls[["S0"]]
gamma.nls <- orig.params.nls[["gamma"]]
Tg.nls <- orig.params.nls[["Tg"]]
beta.nls <- orig.params.nls[["beta"]]
R0.nls <- Reff.nls/S0.nls*N.KM
@

<<delta-method-ci, echo=FALSE>>==
convert_express <- list(
    tpeak=expression(phi/omega),
    Reff=expression((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2),
    gamma=expression(2 * omega/((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2-1)/tanh(phi)),
    S0=expression(2 * ((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2) * I0 * sinh(phi)^2/(((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2) - 1)^2),
    beta=expression((((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2) - 1) * omega * tanh(phi)/(I0 * sinh(phi)^2)),
    Tg=expression(1/(2 * omega/((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2-1)/tanh(phi))),
    R0=expression((((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2) - 1)^2/(2 * I0 * sinh(phi)^2)*N)
)

##dg/dx
dgdx <- sapply(convert_express, function(x) {
    dd <- deriv(x, c("a", "omega", "phi"))
    ee <- eval(dd, c(as.list(nls.parameters), I0=I0.KM, N=N.KM))
    attr(ee, "gradient")
})

vcov <- vcov(nlsfit)
est_vcov <- t(dgdx) %*% vcov %*% dgdx
est_err <- sqrt(diag(est_vcov))
est_par <- c(tpeak=tpeak.nls, Reff=Reff.nls, gamma=gamma.nls, S0=S0.nls, beta=beta.nls, Tg=Tg.nls, R0=R0.nls)
z <- -qnorm((1-0.95)/2)

cc.delta <- matrix(
    c(est_par - z * est_err, est_par + z * est_err),
    ncol=2,
    dimnames=list(
        c("tpeak", "Reff", "gamma", "S0", "beta", "Tg", "R0"),
        c("2.5%", "97.5%")
    )
)

for (p in rownames(cc.delta)) {
    assign(paste0(p,".lwr"), cc.delta[p, "2.5%"])
    assign(paste0(p,".upr"), cc.delta[p, "97.5%"])
}
@

<<delta-method-band, echo=FALSE>>==
traj_express <- expression(a * 1/cosh(omega*t - phi)^2)

traj_ci <- function(t,
                    parameters,
                    vcov,
                    level=0.95) {
    traj <- eval(deriv(traj_express, c("a", "omega", "phi")), c(as.list(parameters), t=t))

    traj_dgdx <- attr(traj, "gradient")

    est_vcov <- traj_dgdx %*% vcov %*% t(traj_dgdx)
    est_err <- sqrt(diag(est_vcov))
    ll <- (1-level)/2
    z <- -qnorm(ll)

    dd <- data.frame(
        times=t,
        estimate=traj,
        lwr=traj-est_err*z,
        upr=traj+est_err*z
    )
    names(dd)[3:4] <- c(paste(100*ll, "%"), paste(100*(1-ll), "%"))

    dd
}
@

\subsection{Uncertainty}

To this point, we have addressed only an optimization problem.  We
solved it using the method of nonlinear least squares, which yields
estimates of the values of the parameters of the model
\eqref{eq:sech}.  But our best estimates are just that:
\emph{estimates}, not exact values of the parameters.

To quantify uncertainty in our estimates, we need a statistical
framework.  The typical output of such a framework is a
\term{confidence interval} (CI) within which our best estimate lies.
For example, the final column of Table~\ref{tab:Bombay} lists 95\% CIs
on our \code{nls} parameter estimates, and the \nlscol shaded region
in Figure~\ref{fig:Bombay} is a 95\% \term{confidence band} for the
fitted model curve.  \djde{We should probably also include the fitode
  confidence band in \fitodecol as I had as a placeholder before.}

\djde{What we do: Given estimated variances for the parameters, we use
  the delta method to calculate the variance of $f(t;\betavechat)$ at
  each observation time $t_i$ and use it to get a confidence band on
  the fitted curve.}  \djde{Available in \code{fitode} but we not what
  we doing here: Another posibility would be to use the estimated
  means and variances for $\betavec$ to define a multivariate normal,
  sample that many times and run the model, and draw the bands that
  contain 95\% of the simulations.}

One standard way to derive a confidence band is the following:
We imagine that the
model \eqref{eq:sech} is a perfect representation of
reality\djde{maybe discuss model misspecification later}, and we
consider the deviations from the model curve in
Figure~\ref{fig:Bombay} to be observation errors.  We then imagine
that observation error for each data point is independent and
identically distributed (iid), and drawn from a normal distribution
with zero mean and the same standard deviation $\sigma$ equal to the
standard deviation of the residuals (the differences between the model
curve and the observed data).  With this assumption, we could
na\"ively draw confidence bands by adding $\pm1.96\,\sigma$ to our
fitted model curve,\footnote{For a \code{Normal}$(0,\sigma)$
  distribution, 95\% of the probability lies between $-1.96\,\sigma$
  and $+1.96\,\sigma$.}  which would emphasize the folly in assuming
the observation error has the same absolute magnitude throughout the
epidemic (in particular the lower limit would go below zero early and
late in the epidemic).

\thickredline

\djde{we should explain the likelihood for the above, which will 
motivate other discussion} the probability
of observing the data $\{y_i\}$ is
%%
\begin{linenomath*}
\begin{equation}\label{eq:prob.data.given.model}
\Pop(\text{data} \mid \text{model})
=
\prod_{i=1}^n 
\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{\big(f(t_i;\betavec) - y_i\big)^2}{2\sigma^2}\right)
\,,
\end{equation}
\end{linenomath*}
%%

\djde{we need to get across that the delta method is a generalization
  of $\Delta f \approx f'(x)\Delta x$}

\djde{Ben, can you please: (1)
  fill in a brief description of what is actually being done to get
  confidence bands on the plotted curve, and add the confidence bands
  to the plot (or make another plot); and (2) explain how CIs on the
  parameter estimates are obtained and spew them here.  Daniel
  basically did it in \code{epidemic.Rnw} but without explanation.}
\swp{Here's my stab at it:}
 However, this na\"ive method of drawing confidence bands ignore uncertainties
associated with other parameter estimates (in this case, $\betavec = (\beta; \gamma; I(0); S(0))$).
In doing so, we need to first estimate the \term{variance covariance matrix} $\mathrm{Cov}(\betavec)$ for our parameter estimates
to characterize their uncertainties---this can be done by inverting the Hessian matrix
of the negative log-likelihood function
(also known as the \term{Fisher information matrix}) at its minimum.
Then, the Delta method allows us to approximate the variance covariance matrix
of any non-linear transfomration of parameters $g(\betavec)$, such as the basic reproduction number or the mortality trajectory \cite{Bolk08}:
\begin{linenomath*}
\begin{equation}
\mathrm{Var}(g(\betavec)) \approx \nabla g(\betavec)^T \cdot \mathrm{Cov}(\betavec) \cdot \nabla g(\betavec)
\end{equation}
\end{linenomath*}
Calculating the gradient $\nabla g(\betavec)$ is tricky when $g$ is a solution of an ODE model---this requires sensitivity equations to be simultaneously solved alongside the main differential equations \cite{raue2013lessons}.
We can then take the square root of the estimated variance to obtain the standard error ($\mathrm{SE}$) and calculate approximate confidence intervals by adding $\pm1.96\,\mathrm{SE}$ to $g(\betavec)$.
\swp{we might want to mention that profile likelihoods are a thing but are
probably too complicated for this paper and especially for trajectories?}

Bear in mind that at each step of this approach, we are making a very
specific, strong assumptions (when we ``imagine'').  To begin with, it
is absurd to suggest that KM's approximation \eqref{eq:sech} to the
solution of the SIR model \eqref{eq:SIR} is a perfect representation
of reality!  The ODEs themselves are far from realistic, not least
because they ignore the discrete nature of individuals and the
stochastic nature of infection and removal events\needref.


\djde{explain how least squares is max likelihood assuming the normal
  observation errors}
\djde{What is the standard way to get CIs on the parameters and confidence
  bands on the fit from the \code{nlsfit} object?}
\djde{How should we compute CIs for the derived parameters?  Delta
  method?}

\djde{Intuitively at least, fitting $\tpeak$ or $\phi$ is
likely to be much better than fitting the initial condition $I_0$.}

\begin{table}
  \begin{center}
    \caption{Parameters of \KM's \cite{KermMcKe27} approximation
      \eqref{eq:sech} to the SIR model \eqref{eq:SIR}, as estimated by
      them \cite{KermMcKe27} and by us using nonlinear least squares
      (\code{nls}).
      The $\R_0$ estimates assume the population of Bombay was
      $N=1$ million \cite[p.\,408]{bacaermodel2012}.
      \djde{Maybe we should list $N$ as a ``specified parameter'' so it
      is very explicit, rather than just burying it in $\R_0$.
      We could then list $\beta N$ rather than $\beta$, which would
      tidy the table and avoid an implicit factor of $1/N$ in $\beta$.}
     %%  (including 95\% confidence intervals).
     %%  Given values of the estimated parameters
     %% $(a,\omega,\phi)$, we specify $I_0$ and derive implied values of the
     %%  original parameters using equation~\eqref{eq:invertparams}.
     %%
     %% The horizontal line separates parameters that are directly
     %% estimated from those that are derived from the estimated
     %% parameters.
}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c | c | c c}
    \bfseries Estimated\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries equation} & {\footnotesize\bfseries units}
    & \bfseries KM & \bfseries \code{nls} & \bfseries 95\% CI \\\hline
    peak removal rate & $a$& \eqref{eq:a} & $\frac{1}{\textrm{weeks}}$ & \Sexpr{a.KM} &
      \Sexpr{signif(a.nls,dig)} & \Sexpr{ci_fmt("a", cc)} \\
    outbreak speed & $\omega$ & \eqref{eq:omega} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{omega.KM} & \Sexpr{signif(omega.nls,dig-1)} & \Sexpr{ci_fmt("omega", cc)} \\
    outbreak centre & $\phi$ & \eqref{eq:phi} & -- & \Sexpr{phi.KM} &
      \Sexpr{signif(phi.nls,dig)} & \Sexpr{ci_fmt("phi", cc)} \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Specified\break parameter \\\hline
    initial prevalence & $I_0$ & -- & -- &
      \Sexpr{I0.KM} & \Sexpr{I0.nls} & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Derived\break parameter \\\hline
    peak time & $\tpeak$ & \eqref{eq:tpeak} & weeks &
      \Sexpr{tpeak.KM} & \Sexpr{tpeak.nls} & \Sexpr{ci_fmt("tpeak", cc.delta)} \\
    effective reproduction number & $\Reff$
      & \eqref{eq:Reffdef}, \eqref{eq:Reff} & -- &
      \Sexpr{Reff.KM} & \Sexpr{Reff.nls} & \Sexpr{ci_fmt("Reff", cc.delta)} \\
    removal rate & $\gamma$ & \eqref{eq:gamma} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{gamma.KM} & \Sexpr{gamma.nls} & \Sexpr{ci_fmt("gamma", cc.delta)} \\
    initial\break susceptibles & $S_0$ & \eqref{eq:S0} & -- &
      \Sexpr{as.integer(S0.KM)} & \Sexpr{as.integer(S0.nls)} & \Sexpr{ci_fmt("S0", cc.delta)} \\
    transmission rate & $\beta$ & \eqref{eq:beta} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{beta.KM} & \Sexpr{beta.nls} & \Sexpr{ci_fmt("beta", cc.delta)} \\
    mean generation interval & $\Tg$ & \eqref{eq:Tg} & days &
      \Sexpr{signif(7*Tg.KM,dig)} & \Sexpr{signif(7*Tg.nls,dig)} & \Sexpr{ci_fmt("Tg", 7*cc.delta)} \\
    basic reproduction number & $\R_0$ & \eqref{eq:R0def} & -- &
      \Sexpr{signif(10^6*Reff.KM/S0.KM,dig)}
      & \Sexpr{signif(10^6*Reff.nls/S0.nls,dig)} & \Sexpr{ci_fmt("R0", cc.delta)}
  \end{tabular}
  \end{center}
  \label{tab:Bombay}
\end{table}

\section{Fitting the ODE}

Until now, we have focussed on fitting KM's approximation
\eqref{eq:sech} rather than actual solutions of the SIR model
\eqref{eq:SIR}.  If we had an exact analytical solution of the SIR
ODE~\eqref{eq:SIR} then we could proceed as above, replacing the
approximate analytical expression \eqref{eq:sech} with the exact
formula.  Since we do not have an exact solution, we must instead rely
on numerical solutions of the ODE.  In general, fitting numerical
solutions of ODEs to data introduces significant coding/computational
challenges, but conceptually the problem is the same as if we did have
an analytical formula.

The \code{fitode} package\footnote{\code{fitode} is available on CRAN
  and consequently can be installed easily via
  \code{install.packages("fitode")}.} does all the computational work
under the hood, and makes it as easy for a user to fit an ODE to data
as it was for us above to use \code{nls} to fit an analytical formula.
We illustrate the use of the package by fitting the SIR
model~\eqref{eq:SIR} to the Bombay plague epidemic.

We begin by loading the package
<<load fitode, message=FALSE>>=
library(fitode)
@
\noindent
and defining a model object:

\vbox{
<<sirmortmodel>>=
SIR_model <- odemodel(
    name="SIR model",
    model=list(
        S ~ - beta * S * I,
        I ~ beta * S * I - gamma * I,
        R ~ gamma * I
    ),
    observation = list(
        mort ~ ols(mean = gamma * I)
    ),
    initial=list(
        S ~ S0,
        I ~ I0,
        R ~ 0
    ),
    par=c("beta", "gamma", "S0", "I0")
)
@
}% end vbox

\noindent
In the model definition above:
\begin{description}
\item[\code{model}]specifies the vector field given by the ODE \eqref{eq:SIR}.
\item[\code{observation}]specifies that the observed data
  (\code{mort}) are assumed to arise from sampling from the normal
  distribution associated with an ordinary least squares (\code{ols})
  fit; the sampled distribution is $\texttt{Normal}(\mu,\sigma)$,
  where the mean is given by the fitted model curve ($\mu(t)=\gamma I(t)$,
  equation~\eqref{eq:SIR;R}) and the standard deviation is set to the
  residual standard deviation $\sigma(t)=\sqrt{\sum_{i=1}^n \big(f(t_i;\betavec) - y_i\big)^2/(n-1)}$,
\item[\code{initial}]conditions are expressed as the number of individuals.
\item[\code{par}]refers to the parameters that are to be fitted:
  $\beta$, $\gamma$, and initial conditions $S(0)$ and $I(0)$, which
  are to be fitted via the \code{fitode} function.
\end{description}

\thickredline

We take our previous parameter estimates from \code{nls} as our starting values:
<<sirmortstart,warning=FALSE,basefig=TRUE>>=
SIR_start <- c(beta=beta.nls, gamma=gamma.nls, I0=I0.KM, S0=S0.nls)

ss_SIR <- simulate(SIR_model,
    parms=SIR_start, times=bombay$week)

plot(bombay)
lines(ss_SIR$times, ss_SIR$I*gamma.nls)
@
We already have a pretty good fit.

Fit:
<<sirmortfit,cache=TRUE,warning=FALSE>>=
SIR_fit <- fitode(
    SIR_model,
    data = bombay,
    start = SIR_start,
    tcol = "week"
)
@

Confidence band on fit:
<<confband>>=
tmax <- 33
tvals <- seq(1,tmax,length=1000)
SIR_confband <- predict(SIR_fit, times=tvals, level=0.95)$mort
@

Plot:
<<sirmortfitplot,basefig=TRUE>>=
plot(SIR_fit, level=0.95)
@
Very good fit, but unrealistic parameters with mean infectious period of 0.3 days.
<<sirmortcoef>>=
coef(SIR_fit)
@
What happens if we try to assume a longer (and more realistic) infectious period?
<<sirmortfit2,cache=TRUE,warning=FALSE>>=
SIR_fit2 <- fitode(
    SIR_model,
    data = bombay,
    start = SIR_start,
    fixed = c(gamma = 1), # Tg = 1/gamma = 1 week
    tcol = "week"
)
@

Plot?
<<>>==
plot(SIR_fit2, level=0.95)
@
Coef?
<<>>==
coef(SIR_fit2)
@
Very very small $S(0)$.


\subsection{Correctly handling weekly mortality}

\swp{I don't think it's worth pursuing KM approx any further... we're trying to show how to fit an ODE, not KM approximation.}
\begin{linenomath*}
\begin{equation}
\int_{t_1}^{t_2} \ddt{R}\,\dt
= \int_{t_1}^{t_2}  a \sech^2{(\omega\,t - \phi)}\,\dt
= \frac{a}{\omega}\Big(\tanh{(\omega\,t_2 - \phi)} - \tanh{(\omega\,t_1 - \phi)}\Big)
\end{equation}
\end{linenomath*}

\swp{We should introduce nbinom here too. I think we can introduce both at the same time (instead of showing two or three separate fits)}

\swp{\code{diffname} argument is used to tell fitode which state variable needs to be 'diff'-ed.}

<<sirmortmodel_correct>>=
SIR_model_nb <- odemodel(
    name="SIR model",
    model=list(
        S ~ - beta * S * I,
        I ~ beta * S * I - gamma * I,
        R ~ gamma * I
    ),
    observation = list(
        mort ~ dnbinom(mu = R, size=phi)
    ),
    initial=list(
        S ~ S0,
        I ~ I0,
        R ~ 0
    ),
    diffnames="R",
    par=c("beta", "gamma", "S0", "I0", "phi"),
)
@

We need to add an extra row to fit the diff model:

<<sirmortfit_correct, cache=TRUE, warning=FALSE>>=
bombay2 <- rbind(
    c(times=bombay$week[1] -
          diff(bombay$week)[1], mort=NA),
    bombay
)

SIR_start_nb <- c(coef(SIR_fit), phi=10)

SIR_fit_nb <- fitode(
    SIR_model_nb,
    data = bombay2,
    start = SIR_start_nb,
    tcol = "week"
)

SIR_nb_confband <- predict(SIR_fit_nb, level=0.95)$mort
@

\section{Example for which KM approx is bad (via stochastic SIR)}

<<compute-stochastic-SIR-via-sirr>>=
sirstoch_fn <- system.file("vignette_data", "sirstoch.RData", package = "fitode")
if (file.exists(sirstoch_fn)) {
    load(sirstoch_fn)
} else {
    library(sirr)
    R0 <- 5
    N <- 2000
    mm <- create_SIRmodel(R0=R0, N=N)
    ii <- set_inits(mm)
    iii <- ii
    iii[2] <- exp(iii[2])
    names(iii) <- c("S","I","R")
    ## FIX: this should not be necessary, but it is a bug in sirr
    ##      associated with N != 1.
    mm1 <- create_SIRmodel(R0=R0, N=1)
    sir.det <- compute_SIRts(mm1)
    ## get associated params for KM approx:
    a.det <- a_fun(Reff=R0, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    omega.det <- omega_fun(Reff=R0, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    phi.det <- phi_fun(Reff=R0, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    ##
    tt <- 0:10
    sir.stoch <- get_ssa_soln(mm, stopcrit = NULL, inits=ii, times=tt)
    save(R0, N, mm, ii, iii, mm1, sir.det, a.det, omega.det, phi.det, tt,
         sir.stoch, file="sirstoch.RData")
}
@

Example is a Gillespie simulation (actually \code{adaptive.tau}), with
$(S_0,I_0,R_0)=(\Sexpr{iii})$, $\R_0=\Sexpr{R0}$,
$\gamma=\Sexpr{mm$gamma}$.


<<colours, echo=FALSE>>=
my.red <- "#E41A1C"     # colour-blind friendly red
my.blue <- "#377EB8"    # colour-blind friendly blue
my.green <- "#4DAF4A" # colour-blind friendly green
my.yellow <- "#FFD92F" # colour-blind friendly yellow
@

<<sirstochfit,cache=TRUE,warning=FALSE>>=
start.stoch <- c(beta=mm$beta, gamma=mm$gamma,
                 S0=iii["S"], I0=iii["I"])
names(start.stoch) <- c("beta","gamma","S0","I0")
print(start.stoch)

sir.stoch.data <- data.frame(
    time=0:10,
    mort=c(NA, diff(sir.stoch$R[!duplicated(ceiling(sir.stoch$time), fromLast = TRUE)]))
)

SIR_stoch_fit <- fitode(
    SIR_model_nb,
    data = sir.stoch.data,
    start = c(start.stoch, phi=10),
    fixed = c(gamma = 1), # Tg = 1/gamma = 1 week
    tcol = "time"
)
sss <- predict(SIR_stoch_fit, level=0.95)[[1]]
summary(SIR_stoch_fit)
@

\section{Influenza in Philadelphia, October 1918}
<<philapop, echo=TRUE>>=
## https://en.wikipedia.org/wiki/Demographics_of_Philadelphia
philapop <- data.frame(year = c(1910,1920), population = c(1549008, 1823779))
##' @param pop data frame
##' @param year year to interpolate to
pop_interp <- function( pop, year ) {
    slope <- (pop[2,2] - pop[1,2])/(pop[2,1] - pop[1,1])
    out <- round( pop[1,2] + (year-pop[1,1])*slope )
    return(out)
}
philapop1918 <- pop_interp( philapop, 1918 )
print(philapop1918)
## reduce this by case fatality proportion so we have
## the initial susceptible population who will be recorded
## as a death if they get infected:
philaS0 <- round(philapop1918 * 0.025)
print(philaS0)
@

<<read.phila.data>>=
## we have to cut the data when we're using nbinom
## beacuse there are long trailing zeroes..
## same with the beginning
phila1918a <- phila1918[as.Date("1918-09-10") < phila1918$date & phila1918$date < as.Date("1918-11-18"),]
phila1918b <- data.frame(
    date=c(as.Date("1918-09-09"), phila1918a$date),
    mort=c(NA, phila1918a$mort)
)

phila1918b$time <- lubridate::decimal_date(phila1918b$date)
@

<<xsummary>>=
## extended summary
## FIX: the estimate is OK, but the rest is wrong; use delta method
xsummary <- function(fit, ...) {
    ss <- summary(fit, ...)
    Reff <- ss["S0",]*ss["beta",]/ss["gamma",]
    return(rbind(ss,Reff))
}
@

<<phila.fit,cache=TRUE,warning=FALSE>>=
## time is in the unit of years
## so gamma=52 corresponds to a mean of 1 week
## I picked not-so-random starting values by hand
start.phila <- c(beta=4e-3, gamma=52*2,
                 S0=philaS0*0.9,
                 I0=3, phi=50)
names(start.phila) <- c("beta","gamma","S0","I0", "phi")
print(start.phila)

phila_fit <- fitode(
    SIR_model_nb,
    data = phila1918b,
    start = start.phila,
    tcol = "time"
)
ppp <- predict(phila_fit, level=0.95)[[1]]
@

<<philasummary>>=
##summary(phila_fit)
philaparms <- as.list(xsummary(phila_fit)[,"Estimate"])
@

<<philaKMparms>>=
## get KM approx parms associated with fitode fit:
a.phila <- with(philaparms, a_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
omega.phila <- with(philaparms, omega_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
phi.phila <- with(philaparms, phi_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
KM.approx.phila <- c(a=a.phila, omega=omega.phila, phi=phi.phila)
print(KM.approx.phila)
@

<<adjust philaKMparms>>=
## adjust because above is giving nls problems:
KM.start.phila <- c(a=300, omega=omega.phila, phi=phi.phila)
print(KM.start.phila)
@

<<phila.nlsfit>>=
## shift time to start at 0 before passing to nls
df <- phila1918b[,c("time","mort")]
df[,"time"] <- df[,"time"] - df[1,"time"]
phila.nlsfit <- nls(mort ~ KM_approx(time, a, omega, phi),
              data = df,
              start = KM.start.phila)
phila.nls.parameters <- coef(phila.nlsfit)
print(phila.nls.parameters)
@

\section{To Do}

\subsection*{Things to mention}

application of \code{fitsir} to song downloads, and how we
convinced ourselves that the SIR model was reasonable for that
problem \cite{Rosa+21}.

\paragraph*{notes from Ben.}

pedagogical refs: I would use Raue et al \cite{raue2013lessons} for
sensitivity equations and see how much of the rest is defined in
Bjornstad's book \cite{bjornstadEpidemics2018} (or fall back on mine).

One thing to keep in mind (maybe mention in your article) is that the
Bombay plague outbreak is arguably \emph{not} a good example of an SIR
model (despite the nice historical connection with K\&M
\cite{KermMcKe27}):
\begin{quote}
``So the 1906 epidemic is clearly not a good example of epidemic stopping because the number of susceptible humans has decreased under a
threshold, as suggested by \KM, but an example of
epidemic driven by seasonality.'' \cite{bacaermodel2012}
\end{quote}

\section{Discussion}

\begin{itemize}
\item appropriateness of fitting an SIR model to the Bombay plague
  data; Baca\"er's \cite{bacaermodel2012}, opinion, our issues with
  his opinion, potential to fit rat-flea-human model with the data
  that are available.
\item other types of models that should also be easy to deal with with
  fitode, e.g., SEIR or fancier compartmental ODEs
\item what kinds of things would likely cause trouble for fitode?
\item optimization challenges, where to go if you run into trouble
\item statistical challenges, where to go for further insights / more
  advanced approaches
\item fitting stochastic models, pomp
\item fitting ABMs
\item a bit of tribute to Fred
\end{itemize}

\bibliographystyle{tfs}
\bibliography{brauer-ms,fitode}

<<figfuns, echo=FALSE>>=
lwd <- 5
col.KM <- my.blue
col.nls <- my.red
col.fitode.ols <- my.yellow
col.fitode.nb <- my.green
transparent_colour <- function(col,alpha=150/255) {
    alpha <- round(alpha * 255)
    v <- col2rgb(col)[,1] # color as rgb vector
    tcol <- rgb(v["red"],v["green"],v["blue"],alpha=alpha,maxColorValue = 255)
    return(tcol)
}
col.fitodeCI <- transparent_colour(col.fitode.ols, alpha=0.4)
col.fitodenbCI <- transparent_colour(col.fitode.nb, alpha=0.4)
col.nlsCI <- transparent_colour(col.nls, alpha=0.4)
setup_plot <- function(xlab="", ylab="deaths",
                       at=100*(0:10), ...) {
    plot(NA, NA , bty="L", type="n",
         xaxs="i", yaxs="i", las=1,
         yaxt="n",
         xlab=xlab, ylab=ylab,
         ...
         )
    axis(side=2, at=at, las=1)
}
draw_confband <- function(confband, col = col.fitodeCI) {
    with(confband,{
        polygon(
            x = c(times, rev(times)),
            y = c(`2.5 %`, rev(`97.5 %`)),
            col = col,
            border = NA,
            xpd = NA
        )
    })
}
draw_legend <- function(lwd=5, pt.bg="white") {
    legend("topleft", bty="n", lwd=c(2,lwd,lwd,lwd*0.60),
       lty=c(NA,"solid","solid","dotted"),
       col=c("black",col.KM,col.nls,col.fitode.ols),
       pch=c(21,NA,NA,NA),
       pt.bg=c(pt.bg,NA,NA,NA),
       legend=c("observed data","KM","nls","fitode (ols)"))
}
@

\begin{figure}
<<Bombay-figure, echo=FALSE, fig.height=5, fig.show="hold", out.width="50%">>=
tvals <- seq(0, tmax, length.out=1000)
setup_plot(ylim = c(0, 1000), xlim=c(0,tmax),
         xlab="weeks", ylab="plague deaths")
draw_confband(traj_ci(tvals, nls.parameters, vcov(nlsfit)), col=col.nlsCI)
curve(KM_approx(x, a=a.KM, omega=omega.KM, phi=phi.KM), from=0, to=tmax, n=1000, add=TRUE,
      lwd=lwd, xpd=NA, col=col.KM)
curve(KM_approx(x, a=a.nls, omega=omega.nls, phi=phi.nls), from=0, to=tmax, n=1000, add=TRUE,
      lwd=lwd, xpd=NA, col=col.nls)
lines(estimate ~ times, data=SIR_confband, col=col.fitode.ols, lwd=lwd*0.60, lty="dotted")
points(mort ~ week, data = fitode::bombay, xpd=NA, pch=21, bg="white", lwd=2)
draw_legend()

setup_plot(ylim=c(5, 1000), xlim=c(0,tmax),
         xlab="weeks", ylab="plague deaths",
         at=c(25, 50, 100, 200, 400, 800))
draw_confband(SIR_nb_confband, col=col.fitodenbCI)
lines(estimate ~ times, data=SIR_nb_confband, col=col.fitode.nb, lwd=lwd)
lines(estimate ~ times, data=SIR_confband, col=col.fitode.ols, lwd=lwd*0.60, lty="dotted")
points(mort ~ week, data = fitode::bombay, xpd=NA, pch=21, bg="white", lwd=2)
legend("topleft", bty="n", lwd=c(2,lwd,lwd,lwd*0.60),
       lty=c(NA,"solid","dotted"),
       col=c("black",col.fitode.nb,col.fitode.ols),
       pch=c(21,NA,NA),
       pt.bg=c("white",NA,NA),
       legend=c("observed data","fitode (nbinom)","fitode (ols)"))
@
\caption{The plague epidemic in Bombay, 17 December 1905 to 21 July
  1906, used as an example by KM \cite[p.\,714]{KermMcKe27}.  The data
    (black dots) were digitized from \cite[Table~IX, p.\,753]{jhyg1907}
  The fitted
  curves are based on the KM approximation \eqref{eq:sech}; the
  associated parameter estimates are given in Table~\ref{tab:Bombay}.}
\label{fig:Bombay}
\end{figure}

\begin{figure}
<<plot_stochastic_SIR>>=
plot(NA, NA, bty="L", xlab="", ylab="", las=1, xaxs="i", yaxs="i",
     xlim=range(tt), ylim=range(sir.stoch$I))
draw_confband(sss, col=col.fitodenbCI)
lines(sir.det$tau, 1 * sir.det$I * N, col=my.red, lwd=lwd) ## dR/dt
curve(KM_approx(x, a=a.det, omega=omega.det, phi=phi.det), from=0, to=tmax, n=1000, add=TRUE,
      xpd=NA, col=my.blue, lwd=lwd)
lines(estimate ~ times, data=sss, col=my.green, lwd=lwd)
points(mort ~ time, data = sir.stoch.data, xpd=NA, pch=21, bg="white", lwd=2)
legend("topright", bty="n", lwd=c(2,lwd,lwd,lwd*0.60),
       lty=c(NA,"solid","solid","solid"),
       col=c("black",my.blue, my.red, my.green),
       pch=c(21,NA,NA,NA),
       pt.bg=c("white",NA,NA,NA),
       legend=c("simulated data","KM","deterministic solution", "fitode (nbinom)"))
@
\caption{
Deterministic fits to a stochastic SIR simulation.
}
\label{fig:stoch}
\end{figure}

\begin{figure}
<<phila-figure, echo=FALSE, fig.height=5>>=
f <- function(t) KM_approx(t, a=a.phila, omega=omega.phila, phi=phi.phila)
setup_plot(ylim = c(0, 800), xlim = range(phila1918b$time), ylab="daily P&I deaths")
draw_confband(ppp)
tvals <- with(phila1918b, seq(min(time),max(time), length=1000))
t0 <- min(tvals)
lines(tvals, f(tvals-t0), lwd=lwd, xpd=NA, col=col.KM)
lines(tvals,
      with(as.list(phila.nls.parameters),KM_approx(tvals-t0, a=a, omega=omega, phi=phi)),
      xpd=NA, lwd=lwd, col=col.nls)
lines(estimate ~ times, data=ppp, col=col.fitode.ols, lwd=lwd*0.60, lty="dotted")
points(mort ~ time, data = phila1918, xpd=NA, pch=21, bg="white", lwd=2)
draw_legend()
@
\caption{The main wave of the 1918 influenza epidemic in the city of
  Philadelphia, 1 September 1918 to 31 December 1918 \cite{Gold+09}.
  \djde{If we incluce this figure, we'll need another table
    similar to the Bombay plague table (Table~\ref{tab:Bombay}).}
}
\label{fig:phila}
\end{figure}

\end{document}
