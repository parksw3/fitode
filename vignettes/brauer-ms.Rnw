% interacttfssample.tex
% v1.05 - August 2017

\documentclass[]{interact}

\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

%\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required
%\setlength\parindent{24pt}% To increase paragraph indentation when line spacing is doubled
%\setlength\bibindent{2em}% To increase hanging indent in bibliography when line spacing is doubled

\usepackage[numbers,sort&compress]{natbib}% Citation support using natbib.sty
\bibpunct[, ]{[}{]}{,}{n}{,}{,}% Citation support using natbib.sty
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

%% DE preamable
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{lineno}\renewcommand\thelinenumber{\color{gray}\arabic{linenumber}}
\usepackage{placeins}
\newcommand{\thickredline}{{\color{red}\bigskip\begin{center}\linethickness{2mm}\line(1,0){250}\end{center}\bigskip}}
\newcommand{\bmb}[1]{{\color{red}$\langle$\emph{BMB: #1}$\rangle$}}
\newcommand{\swp}[1]{{\color{blue}$\langle$\emph{SWP: #1}$\rangle$}}
\newcommand{\djde}[1]{{\color{magenta}$\langle$\emph{DJDE: #1}$\rangle$}}
\newcommand{\needref}{{\color{orange}[NEED REF]}}
\newcommand{\term}[1]{{\bfseries\slshape#1}}
\newcommand{\ddt}[1]{\dfrac{{\rm d}#1}{{\rm d}t}}
\newcommand{\dbydt}[1]{{\rm d}#1/{\rm d}t}
\newcommand{\sech}{\,\textrm{sech}}
\newcommand{\Ipeak}{I_{\rm p}}
\newcommand{\tpeak}{t_{\rm p}}
\newcommand{\R}{{\mathcal R}}
\newcommand{\Tg}{T_{\rm g}}
\newcommand{\Reff}{\R_{\rm e}}
\usepackage{xspace}
\newcommand{\KM}{KM\xspace}
\usepackage{bm} % bold math
\newcommand{\boldbeta}{\bm{\beta}}
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}

\begin{document}

\articletype{ORIGINAL ARTICLE}% Specify the article type or omit as appropriate

\title{Fitting epidemic models to data -- a tutorial\\in memory of Fred
  Brauer}

\author{
\name{David J.\,D.~Earn\textsuperscript{a}\thanks{CONTACT
    D.J.D.~Earn. Email: earn@math.mcmaster.ca},
    Sang Woo Park\textsuperscript{b},
    and Benjamin M.~Bolker\textsuperscript{a}}
\affil{\textsuperscript{a}Department of Mathematics and Statistics,
  McMaster University, Hamilton, Ontario, Canada, L8S 4K1; \textsuperscript{b}Department of Ecology and Evolutionary Biology, Princeton University, Princeton, NJ 08544}
}

\maketitle

\linenumbers

\djde{The \href{https://www.tandfonline.com/action/authorSubmission?show=instructions&journalCode=tjbd20}{author instructions} say ``please also include ORCiDs and social media handles (Facebook, Twitter or LinkedIn)'' but it isn't clear where to put them.}

\bigskip \bigskip

\begin{abstract}
  Fred Brauer had no desire to touch data himself, but recognized that
  fitting models to data is usually necessary when attempting to apply
  infectious disease transmission models to real public health
  problems.  He was curious to know how one goes about fitting
  dynamical models to data, and why it can be hard.  Initially in
  response to Fred's questions, we developed a user-friendly
  \texttt{R} package that facilitates fitting ordinary differential
  equations to observed time series.  Here, we exploit the package
  (\texttt{fitode}) to provide a brief tutorial introduction to
  fitting compartmental epidemic models to a single observed time
  series.  We assume that, similar to Fred, the reader is familiar
  with dynamical systems from a mathematical perspective, but has
  little or no experience with statistical methodology or optimization
  techniques.
\end{abstract}

\begin{keywords}
  epidemic models; infectious diseases; ordinary differential
  equations; parameter estimation; maximum likelihood; \texttt{fitode}
\end{keywords}

\section{Introduction}

In their landmark 1927 paper, Kermack and McKendrick (KM)
\cite[p.\,713]{KermMcKe27} introduced the now-standard
susceptible-infected-removed (SIR) epidemic model,
\begin{linenomath*}
  \begin{subequations}\label{eq:SIR}
    \begin{align}
      \ddt{S} &= -\beta S I \,, \\
      \noalign{\vspace{5pt}}
      \ddt{I} &= \beta S I - \gamma I \,, \\
      \noalign{\vspace{5pt}}
      \ddt{R} &= \gamma I\,,
    \end{align}
  \end{subequations}
\end{linenomath*}
where $S$, $I$ and $R$ represent the numbers of individuals who are
susceptible, infected or removed\footnote{In the words of \KM
  \cite[p.\,701]{KermMcKe27}, ``removed from the number of those who
  are sick, by recovery or by death''.}, $\beta$ is the transmission
rate, and $\gamma$ is the recovery rate.  In that original paper,
\KM \cite[p.\,714]{KermMcKe27} also fit their model
to plague mortality data from an epidemic in Bombay (now Mumbai) that
occurred about 20 years before their paper was written.

In the century that has elapsed since publication of \KM's initial
paper, the field of mathematical epidemiology has expanded and
matured, and has been the subject of many books
\cite{Bart60,Bail75,AndeMay91,AndeBrit00b,DiekHees00,BrauCast01,Brau+19}
and review articles \cite{Heth00,Earn+02,Earn09}.  The focus has
tended to be on \term{compartmental models} like the SIR model, cast
either as differential equations following the tradition of \KM
\cite{KermMcKe27}, or as \term{stochastic processes} in the tradition
of Bartlett \cite{Bart60}.  In recent years, as the power of computers
has grown exponentially, there has been increasing interest in
\term{agent-based models}, which represent each individual as
a separate unit that can have unique properties \cite{eubank2004modelling}.

Throughout the history of the subject, and regardless of the modelling
frameworks they have exploited, mathematical epidemiologists have
frequently attempted to fit---or at least to compare---their models to
observed infectious disease data. \swp{Is this too strong?:} Such fits have often been fairly
na\"{\i}ve \swp{simple?}, with little or \swp{definitely too strong: or no} consideration of their quality; but over
the years, there has been a trend towards greater sophistication and
statistical rigour in parameter estimation for infectious disease
models, and books that explain these methods have begun to appear in
recent decades \cite{Bolk08,bjornstadEpidemics2018}.  Careful
consideration of uncertainty is especially important when epidemic
models are used for the development and analysis of policy options for
infectious disease management \cite{elderd2006uncertainty}, a challenge that has absorbed the
attention of most \swp{many? most seems too strong} mathematical epidemiologists during the course of the
present COVID-19 pandemic \swp{Can't find appropriate papers to cite here...}.

While visiting the University of British Columbia in 2014--2015, one
of us (DE) had many conversations with Fred Brauer about epidemic
models and how they can be used in practical applications.  While he
had no desire to analyze data himself, Fred was acutely aware that
fitting to data is essential if one wishes to apply epidemic
models to real public health problems, and he did want to understand
what was involved in doing so.

Fred's curiosity inspired us to develop user-friendly software for
fitting ordinary differential equation (ODE) models to observed time
series, with the goal of illustrating the process and challenges of
model fitting to Fred and others like him, i.e., individuals who are
comfortable with mathematical analysis of ODEs but have little or no
experience with statistics and parameter estimation.  Unfortunately,
we have lost the opportunity to present our work to Fred, but it seems
fitting (!) to present such a tutorial in this volume dedicated to
Fred's memory.

\section{Kermack and McKendrick's fit}

We begin by revisiting \KM's \cite{KermMcKe27}
application of their SIR model \eqref{eq:SIR} to the epidemic of
plague in Bombay in 1905--1906.  The observed data (black dots in
Figure~\ref{fig:Bombay}) were weekly numbers of deaths from plague.

Referring to their version of Figure~\ref{fig:Bombay}, \KM
\cite[p.\,714]{KermMcKe27} argued that ``As at least 80 to 90 per
cent.\ of the cases reported terminate fatally, the ordinate may be
taken as approximately representing [$\dbydt{R}$] as a function of
$t$.''  Since computers did not yet exist, and an exact analytical
form for this function could not be found, they proceeded to assume
\cite[p.\,713]{KermMcKe27} that $\frac{\beta}{\gamma}R(t)\ll1$, which
yields the approximate analytical form,
\begin{linenomath*}
\begin{equation}\label{eq:sech}
\ddt{R} \simeq a \sech^2{(\omega\,t - \phi)} \,,
\end{equation}
\end{linenomath*}
 where (writing $\Reff\equiv S_0\beta/\gamma$ for the effective reproduction
number at time $t=0$)
\swp{KM actually has a typo in their paper... we need to look at \cite{bacaermodel2012} for the correct equations.}
\begin{linenomath*}
\begin{subequations}\label{eq:sechparams}
\begin{align}
  \omega &= \frac{\gamma}{2} \sqrt{(\Reff-1)^2 +
           \frac{2I_0}{S_0}\Reff^2}\,,\\
  \phi &= \textrm{arctanh}\left(\frac{\Reff - 1}{2\omega/\gamma}
         \right)\,, \\
  \text{and}\quad
  a &= \frac{2 S_0}{\gamma \Reff^2} \omega^2  \,.
\end{align}
\end{subequations}
\end{linenomath*}
They then presented the parameter estimates that we have listed in the
KM column of Table~\ref{tab:Bombay}, and they plotted their
``calculated'' curve, which we have reproduced in red in
Figure~\ref{fig:Bombay}.  

The red curve does appear to provide a reasonable fit to the data, but
\KM \cite{KermMcKe27} gave no indication of how
their parameter estimates were obtained.  \djde{When was least-squares
  trajectory matching first done?  Do we think they did that?  or just
  played with parameter values and fitted by eye?} \swp{It seems unlikely that they had the ability to perform least squares back in 1927... I was able to find least squares SIR and SI3R(!) model fit from 1990 \cite[p.\,51]{kryscio1990modeling} but not sure if there's anything earlier.}  Whatever their
process, they must have engaged in some sort of \term{trajectory
  matching}, i.e., adjusting parameter values until the
model---equation~\eqref{eq:sech} in their case---is, by some measure,
as close as possible to the observed data points.  The most obvious
metric for this purpose is the Euclidean distance between the model
curve and the data.  Thus, the \term{objective function} that is
natural to minimize is
\begin{linenomath*}
\begin{equation}\label{eq:leastsquares}
\sum_{i=1}^n \big(f(t_i;\boldbeta) - y_i\big)^2 \,,
\end{equation}
\end{linenomath*}
where the observed data are the points $\{(t_i,y_i):i=1,\ldots,n\}$,
$f(t;\boldbeta)$ is the model \eqref{eq:sech}, and the parameter
vector for \KM's problem is
$\boldbeta=(a,\omega,\phi)$.  Minimizing \eqref{eq:leastsquares} with
respect to $\boldbeta$ would have required some heroic arithmetic
with a pencil and paper in 1927, but it is a simple task with the
aid of a computer.

In the following segment of R code, we fit equation \eqref{eq:sech} to
the Bombay plague data (which are built in to the \code{fitode}
package that we describe below, as a data frame with columns
\code{week} and \code{mort}).  We exploit R's nonlinear least squares
function (\code{nls}), which attempts to minimize the distance
\eqref{eq:leastsquares} to the data, starting from an initial guess
(\code{start}).

<<nls.bombay>>=
sech <- function(x) {1/cosh(x)}
KM_approx <- function(t, a, omega, phi) {a * sech(omega*t - phi)^2}
KM.parameters <- c(a = 890, omega = 0.2, phi = 3.4)
nlsfit <- nls(mort ~ KM_approx(week, a, omega, phi), 
              data = fitode::bombay,
              start = KM.parameters)
(fitted.parameters <- coef(nlsfit))
@ 

\noindent
Above, we chose as our starting guess the fitted parameter values of
\KM (see Table~\ref{tab:Bombay}).  The least
squares parameter values differ from KM's by a few percent:

<<percent.diff>>=
(percent.difference <- 100*(KM.parameters - fitted.parameters)
                           /fitted.parameters)
@ 

\noindent
Our least squares fitted function is shown in blue in
Figure~\ref{fig:Bombay}.

Starting from someone else's fit is not a great way to test the
method, but fortunately the least squares fit for this problem is not
very sensitive to the initial guess.  While not necessary for this
problem, to make a reasonable initial guess, it can help to think
about the meaning of parameters.  For example, in the case of
equation~\eqref{eq:sech}, it is useful to note that $a$ is the maximum
of the function~\eqref{eq:sech}, and if write $\omega t - \phi$ as
$\omega(t-\tpeak)$ then $\tpeak$ is the time at which the maximum
occurs; both $a$ and $\tpeak$ can be guessed approximately by looking
at the plotted observed data, and a very rough guess is sufficient
converge on the same fit:
%%
\djde{We could ``guess'' a value of $\omega$ by noting that
  $\sech^2{(\omega t)}\to e^{2\omega t}$ as $t\to-\infty$, so we could
  estimate $\omega$ as the slope of $\log(y_i/2)$\dots a little hard
  to do by eye from the data plot.}
  \swp{seems like an overkill to me. I don't think it hurts to say that we just chose 0.1}

<<nls.bombay.2>>=
a.guess <- 1000    # crude "by eye" estimate of peak value
tpeak.guess <- 15  # crude "by eye" estimate of peak time
omega.guess <- 0.1
phi.guess <- omega.guess * tpeak.guess
nlsfit <- nls(mort ~ KM_approx(week, a, omega, phi), 
              data = fitode::bombay, 
              start = c(a = a.guess, omega = omega.guess,
                        phi = phi.guess))
(fitted.parameters <- coef(nlsfit))
@ 

However, if you experiment with initial guesses, you will find that if
you make a sufficiently \emph{bad} guess, then \code{nls} will fail.
For example, starting from $a=2000$, $\tpeak=5$, and $\omega=0.1$
yields a \code{singular gradient} error.  More interestingly, starting
from $a=500$, $\tpeak=5$, and $\omega=0.1$ yields $a=869$,
$\omega=-0.19$, $\phi=-3.48$, which is far from our fitted values and
illustrates a very important fact: there is \emph{not necessarily a
  unique best fit set of parameters!}  In this case, the alternative
solution exists because $\sech^2(x)$ is symmetric about the $y$ axis, but
in general, there can be many local minima that cause \code{nls} to
converge to points that may or may not be equally good.

If you know that your parameters should be in a certain range, then
you can pass that information to \code{nls}.  For example, to ensure
that all the parameters are non-negative (and exclude the alternative
fit above), you would add the \code{nls} option

<<eval=FALSE>>=
lower = c(a = 0, omega = 0, phi = 0)
@ 

\noindent
which would prevent convergence to negative $\omega$ and $\phi$. \swp{Is this a good place to introduce link functions? Or too early? Or at least hint at it?  Example text:} \swp{Instead, we can also estimate the parameters on the log scale and convert them back to the the original scale by exponentiating the estimated parameters. This would allow parameter estimation to happen on an unconstrained scale (between negative infinity and infinity), providing a more stable fitting. As we discuss later, we can use \textbf{link functions} to convert parameters with constraints to unconstrained scales for more general cases.}
In general, the potential existence of multiple local optima is something
that makes fitting models to data hard, and you need to be cautious in
interpretting any fit to which your fitting algorithm has converged.

<<KM fitted values, echo=FALSE>>=
a.KM <- KM.parameters["a"]
omega.KM <- KM.parameters["omega"]
phi.KM <- KM.parameters["phi"]
tpeak.KM <- phi.KM / omega.KM
f <- function(t) KM_approx(t, a=a.KM, omega=omega.KM, phi=phi.KM)
@ 

\begin{figure}
<<Bombayfigure, echo=FALSE, fig.height=5>>=
tmax <- 33
tvals <- seq(0,tmax,length=1000)
col.KM <- "#E41A1C"
col.us <- "#377EB8"
bombay_plot <- function(type = "l", ...) {
    plot(NA, NA , bty="L", type="n", xlim=c(0,tmax), ylim=c(0,900),
         xaxs="i", yaxs="i", las=1, 
         yaxt="n",
         xlab="weeks", ylab="plague deaths",
         ...
         )
    axis(side=2, at=100*(0:9), las=1)
}
lwd <- 3
bombay_plot()
points(mort ~ week, data = fitode::bombay, xpd=NA, pch=19)
lines(tvals, f(tvals), lwd=lwd, xpd=NA, col=col.KM)
a <- fitted.parameters["a"]
omega <- fitted.parameters["omega"]
phi <- fitted.parameters["phi"]
tpeak <- phi/omega
dig <- 3 # number of significant digits for printing
lines(tvals, KM_approx(tvals, a=a, omega=omega, phi=phi),
      xpd=NA, lwd=lwd, col=col.us)
legend("topleft", bty="n", lwd=lwd, col=c(col.KM,col.us),
       legend=c("KM","us"))

@ 
\caption{The plague epidemic in Bombay, 17 December 1905 to 21 July
  1906.  The data (black dots) were digitized from the chart in
  \cite[p.\,714]{KermMcKe27}. The fitted curves are based on the KM
  approximation \eqref{eq:sech}; the associated parameter estimates
  are given in Table~\ref{tab:Bombay}.}
\label{fig:Bombay}
\end{figure}

If we accept our fit as satisfactory, what can we infer about the
dynamics of plague that KM were attempting to capture with the SIR
model \eqref{eq:SIR}?  We need to convert the parameters of KM's
approximation \eqref{eq:sechparams} back to the original parameters
that are directly related to the mechanism of disease spread that the
model formalizes (i.e., $\beta$ and $\gamma$, and initial conditions
$S_0$ and $I_0$).

\FloatBarrier

The nonlinear algebraic relationships specified by
equation~\eqref{eq:sechparams} can be inverted analytically following \cite{bacaermodel2012}. But since we have four variables ($I(0)$, $S(0)$, $\beta$, and $\gamma$) and are only given three parameters ($a$, $\omega$, and $\phi$), we begin by assuming $I(0) = 1$.

<<invert KM analytically>>=
invertpar <- function(I0, param) {
    with(as.list(param), {
        Reff <- (1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2
	q <- (Reff-1)/tanh(phi)
	gamma <- 2 * omega/q ## gamma has a unit of 1/week
	S0 <- 2 * Reff * I0 * sinh(phi)^2/(Reff - 1)^2
	beta <- Reff * gamma/S0

	c(Reff=Reff, S0=S0, Tg=7/gamma, beta=beta)
    })
}
invertpar(1, coef(nlsfit))
@

\swp{old text:} so we
need to do it numerically.  Unsurprisingly, there is an R package for
this kind of nonlinear algebraic inversion (\code{nleqslv}), which we
can use to write a function to invert equation~\eqref{eq:sechparams}.
\swp{The problem is that we're assuming S0 = 1e6 - 1. We can't do this.
We need much lower S0 to fit the data. So by assuming a very large S0,
things go crazy.  So we're better off startng with assuming I0 = 1 and
assuming the rest. That's the whole point of Bacaer's paper: that the
required S0 to fit the data is so low that the standard SIR model is
actually not appropriate for modeling the bombay epidemic}

<<invert nonlinear equations>>=
omega_fun <- function(Reff, gamma, S0, I0)
    {(gamma/2)*sqrt((Reff-1)^2+(2*I0/S0)*Reff^2)}
phi_fun <- function(Reff, gamma, S0, I0)
    {atanh((Reff-1)/(2*omega_fun(Reff,gamma,S0,I0)/gamma))}
a_fun <- function(Reff, gamma, S0, I0)
    {(2 * S0/(gamma * Reff^2))*omega_fun(Reff,gamma,S0,I0)^2}
objective_fun <- function(x, targets) {
    with(c(as.list(x),as.list(targets)), {
        c(omega_fun(Reff,gamma,S0,I0) - omega,
          phi_fun(Reff,gamma,S0,I0) - phi,
          a_fun(Reff,gamma,S0,I0) - a)
    })
}
find_orig_params <- function(orig.params.guess, fitted.params) {
    f <- function(x) objective_fun(x, targets=fitted.params)
    out <- nleqslv::nleqslv(x = orig.params.guess, fn = f)$x
    S0 <- out[["S0"]]
    return(c(out, beta=out[["Reff"]]*out[["gamma"]]/S0,
             Tg=1/out[["gamma"]]))
}
I0 <- 1
(orig.params.guess <- c(Reff=2, gamma=0.2, S0=1e6))
(objective_fun(x = orig.params.guess, targets = coef(nlsfit)))
(orig.params <- find_orig_params(orig.params.guess, fitted.params=coef(nlsfit)))
@ 

\djde{explain normal CIs from least squares and how least squares is max
likelihood in this case}

\djde{What is the standard way to get CIs on the parameters and confidence
  bands on the fit from the \texttt{nlsfit} object?}

\djde{What is the best way to invert the parameter relationships in
  \eqref{eq:sechparams} to get $\beta$ and $\gamma$?  And how should
  we compute CIs for the derived parameters?  Delta method?}

\djde{We must comment on whether the estimated $\beta$ and $\gamma$
  are actually reasonable given what we know about pneumonic plague,
  e.g., from Gani and Leach \cite{GaniLeac04}.
  But also note KM's caution about interpreting the param values.}

\djde{Intuitively at least, fitting $\tpeak$ or $\phi$ is
likely to be much better than fitting the initial condition $I_0$.}

\begin{table}
  \begin{center}
    \caption{Parameters of \KM's \cite{KermMcKe27}
      approximation \eqref{eq:sech} to the SIR model \eqref{eq:SIR},
      as estimated by them \cite{KermMcKe27} and by us (including 95\%
      confidence intervals).
      %% The horizontal line separates parameters that are directly
      %% estimated from those that are derived from the estimated
      %% parameters.
    }
    \medskip
  \begin{tabular}{l | c | c | c | c c}
    \bfseries Estimated parameter & \bfseries symbol & \bfseries units
    & \bfseries KM & \bfseries us & \bfseries 95\% CI \\\hline
    peak removal rate & $a$ & 1/weeks & \Sexpr{a.KM} & 
      \Sexpr{signif(a,dig)} & (,)\\
    outbreak speed & $\omega$ & 1/weeks & 
      \Sexpr{omega.KM} & \Sexpr{signif(omega,dig-1)} & (,) \\
    outbreak centre & $\phi$ & -- & \Sexpr{phi.KM} & 
      \Sexpr{signif(phi,dig)} & (,) \\
    \noalign{\vspace{10pt}}
    \bfseries Derived parameter \\\hline
    peak time & $\tpeak = {\phi}/{\omega}$ & weeks & 
      \Sexpr{tpeak.KM} & \Sexpr{signif(tpeak,dig)} & (,) \\
    transmission rate & $\beta$ & 1/weeks & \\
    removal rate & $\gamma$ & 1/weeks & \\
    mean generation interval & $\Tg = {1}/{\gamma}$ & weeks & \\
    basic reproduction number & $\R_0 = {\beta}/{\gamma}$ & -- &
  \end{tabular}
  \end{center}
  {\footnotesize\emph{Note:} Given $a$, $\omega$, and $\phi$, we
    obtain $\beta$ and $\gamma$ by numerically inverting the
    relationships in \eqref{eq:sechparams}.  \djde{This will be a
      nuisance.}}
  \label{tab:Bombay}
\end{table}

\subsection*{Things to mention}

application of \texttt{fitsir} to song downloads, and how we
convinced ourselves that the SIR model was reasonable for that
problem \cite{Rosa+21}.

\paragraph*{notes from Ben.}

pedagogical refs: I would use Raue et al \cite{raue2013lessons} for
sensitivity equations and see how much of the rest is defined in
Bjornstad's book \cite{bjornstadEpidemics2018} (or fall back on mine).

One thing to keep in mind (maybe mention in your article) is that the
Bombay plague outbreak is arguably \emph{not} a good example of an SIR
model (despite the nice historical connection with K\&M
\cite{KermMcKe27}):
\begin{quote}
``So the 1906 epidemic is clearly not a good example of epidemic stopping because the number of susceptible humans has decreased under a
threshold, as suggested by \KM, but an example of
epidemic driven by seasonality.'' \cite{bacaermodel2012}
\end{quote}

\bibliographystyle{tfs}
\bibliography{brauer-ms,fitode}

\end{document}
