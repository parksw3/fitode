% interacttfssample.tex
% v1.05 - August 2017

\documentclass[]{interact}

\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

%\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required
%\setlength\parindent{24pt}% To increase paragraph indentation when line spacing is doubled
%\setlength\bibindent{2em}% To increase hanging indent in bibliography when line spacing is doubled

\usepackage[numbers,sort&compress]{natbib}% Citation support using natbib.sty
\bibpunct[, ]{[}{]}{,}{n}{,}{,}% Citation support using natbib.sty
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

%% DE preamable
\usepackage{pdflscape} % landscape mode that rotates page (unlike lscape)
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{lineno}\renewcommand\thelinenumber{\color{gray}\arabic{linenumber}}
\usepackage{placeins}
\newcommand{\thickredline}{{\color{red}\bigskip\begin{center}\linethickness{2mm}\line(1,0){250}\end{center}\bigskip}}
\newcommand{\bmb}[1]{{\color{red}$\langle$\emph{BMB: #1}$\rangle$}}
\newcommand{\swp}[1]{{\color{blue}$\langle$\emph{SWP: #1}$\rangle$}}
\newcommand{\djde}[1]{{\color{magenta}$\langle$\emph{DJDE: #1}$\rangle$}}
\newcommand{\needref}{{\color{orange}[NEED REF]}}
\newcommand{\term}[1]{{\bfseries\slshape#1}}
\newcommand{\dt}{{\rm d}t}
\newcommand{\ddt}[1]{\dfrac{{\rm d}#1}{{\rm d}t}}
\newcommand{\dbydt}[1]{{\rm d}#1/{\rm d}t}
\newcommand{\sech}{\,\textrm{sech}}
\newcommand{\Ipeak}{I_{\rm p}}
\newcommand{\tpeak}{t_{\rm p}}
\newcommand{\R}{{\mathcal R}}
\newcommand{\Tg}{T_{\rm g}}
\newcommand{\Reff}{\R_{\rm e}}
\usepackage{xspace}
\newcommand{\KM}{KM\xspace}
\usepackage{bm} % bold math
\newcommand{\thetavec}{{\bm{\theta}}}
\newcommand{\thetavechat}{{\bm{\hat\theta}}}
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\usepackage{xspace}
\newcommand{\KMcol}{blue\xspace}
\newcommand{\nlscol}{red\xspace}
\newcommand{\fitodecol}{yellow\xspace}
\newcommand{\Pop}{{\mathbb{P}}} % probability operator
\newcommand{\lik}{{\mathcal L}}
\newcommand{\transpose}{{\textsf{T}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
%% BMB: can decide whether this should be \mathrm{Norm} or \cal{N} or ...
\newcommand{\normdist}{\ensuremath\mathrm{Normal}}
%% tables
\usepackage{multirow}   % for tables
%\usepackage{subfig}
\usepackage{tabularx}   % pretty tables
% \usepackage{tabularray} % allows rowsep via tblr environment
%% table details
\renewcommand{\arraystretch}{1.5} % more interline spacing
%% https://tex.stackexchange.com/questions/12703/how-to-create-fixed-width-table-columns-with-text-raggedright-centered-raggedlef
\usepackage{ragged2e}
%%\usepackage{array} % for m{...}


<<setup, echo=FALSE, include=FALSE>>=
## set default base-graphics plotting options
knitr::knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       oldpar <- par(bty="l",las=1)
                       on.exit(par(oldpar))
                   } else { }
})
knitr::opts_chunk$set(error = FALSE)
@
\begin{document}

\articletype{ORIGINAL ARTICLE}% Specify the article type or omit as appropriate

\title{Fitting epidemic models to data -- a tutorial\\in memory of Fred
  Brauer}

\author{
\name{David J.\,D.~Earn\textsuperscript{a}\thanks{CONTACT D.J.D.~Earn. Email: earn@math.mcmaster.ca. ORCID:
    0000-0002-7562-1341, Twitter: @David.J.D.Earn},
    Sang Woo Park\textsuperscript{b}\thanks{S.W.~Park. ORCID 0000-0003-2202-3361, Twitter: @sang\_woo\_park, Mastodon: @sangwoopark@ecoevo.social},
    and Benjamin M.~Bolker\textsuperscript{a,c}\thanks{B.M.~Bolker. ORCID: 0000-0002-2127-0443, Twitter: @bolkerb, Mastodon: @bbolker@fediscience.org}}
\affil{\textsuperscript{a}Department of Mathematics and Statistics,
  McMaster University, Hamilton, Ontario, Canada, L8S 4K1; \textsuperscript{b}Department of Ecology and Evolutionary Biology, Princeton University, Princeton, NJ 08544; \textsuperscript{c}Department of Biology,
  McMaster University, Hamilton, Ontario, Canada, L8S 4K1}
}

\maketitle

\linenumbers

\bigskip \bigskip

\begin{abstract}
  Fred Brauer had no desire to touch data himself, but recognized that
  fitting models to data is usually necessary when attempting to apply
  infectious disease transmission models to real public health
  problems.  He was curious to know how one goes about fitting
  dynamical models to data, and why it can be hard.  Initially in
  response to Fred's questions, we developed a user-friendly \code{R}
  package that facilitates fitting ordinary differential equations to
  observed time series.  Here, we use the package (\code{fitode}) to
  provide a brief tutorial introduction to fitting compartmental
  epidemic models to a single observed time series.  We assume that,
  like Fred, the reader is familiar with dynamical systems from a
  mathematical perspective, but has limited experience (if any) with
  statistical methodology or optimization techniques.
\end{abstract}

\begin{keywords}
  epidemic models; infectious diseases; ordinary differential
  equations; parameter estimation; maximum likelihood; \code{fitode}
\end{keywords}

\section{Introduction}

In their landmark 1927 paper, Kermack and McKendrick (KM)
\cite[p.\,713]{KermMcKe27} introduced the now-standard
susceptible-infected-removed (SIR) epidemic model,
\begin{linenomath*}
  \begin{subequations}\label{eq:SIR}
    \begin{align}
      \ddt{S} &= -\beta S I \,, \\
      \noalign{\vspace{5pt}}
      \ddt{I} &= \beta S I - \gamma I \,, \\
      \noalign{\vspace{5pt}}
      \ddt{R} &= \gamma I\,, \label{eq:SIR;R}
    \end{align}
  \end{subequations}
\end{linenomath*}
where $S$, $I$ and $R$ represent the numbers of individuals who are
susceptible, infected or removed\footnote{In the words of \KM
  \cite[p.\,701]{KermMcKe27}, ``removed from the number of those who
  are sick, by recovery or by death''.}, $\beta$ is the transmission
rate, and $\gamma$ is the recovery rate.  In that original paper,
\KM \cite[p.\,714]{KermMcKe27} also fit their model
to plague mortality data from an epidemic in Bombay (now Mumbai) that
occurred about 20 years before their paper was written.

In the century that has elapsed since publication of \KM's initial
paper, the field of mathematical epidemiology has expanded and
matured, and has been the subject of many books
\cite{Bart60,Bail75,AndeMay91,AndeBrit00b,DiekHees00,BrauCast01,Brau+19}
and review articles \cite{Heth00,Earn+02,Earn09}.  Researchers have
primarily focused on \term{compartmental models} like the SIR model, cast
either as differential equations following the tradition of \KM
\cite{KermMcKe27}, or as \term{stochastic processes} in the tradition
of Bartlett \cite{Bart60}.  In recent years, as the power of computers
has grown exponentially, there has been increasing interest in
\term{agent-based models}, which represent each individual as
a separate unit that can have unique properties \cite{eubank2004modelling}.

Throughout the history of the subject, and regardless of the modelling
frameworks they have exploited, mathematical epidemiologists have
frequently attempted to fit---or at least to compare---their models to
observed infectious disease data. Such fits have often been
na\"{\i}ve, with limited consideration of their quality. Over the
years, however, there has been a trend towards greater sophistication and
statistical rigour in parameter estimation for infectious disease
models; books that explain these methods have begun to appear in
recent decades \cite{Bolk08,bjornstadEpidemics2018}.  Careful
consideration of uncertainty is especially important when epidemic
models are used for the development and analysis of policy options for
infectious disease management \cite{elderd2006uncertainty}, a
challenge that has absorbed the attention of many mathematical
epidemiologists during the course of the present COVID-19 pandemic
\cite{Broo+21,Hill+21,Nixo+22}.

While visiting the University of British Columbia in 2014--2015, one
of us (DE) had many conversations with Fred Brauer about epidemic
models and how they can be used in practical applications.  While he
had no desire to analyze data himself, Fred was acutely aware that
fitting to data is essential if one wishes to apply epidemic
models to real public health problems, and he did want to understand
what was involved in doing so.

Fred's curiosity inspired us to develop user-friendly software for
fitting ordinary differential equation (ODE) models to observed time
series, with the goal of illustrating the process and challenges of
model fitting to Fred and others like him, i.e., individuals who are
comfortable with mathematical analysis of ODEs but have little or no
experience with statistics and parameter estimation.  Unfortunately,
we have lost the opportunity to present our work to Fred, but it seems
fitting (!) to present such a tutorial in this volume dedicated to
Fred's memory.

\section{Kermack and McKendrick's fit}

We begin by revisiting \KM's \cite{KermMcKe27}
application of their SIR model \eqref{eq:SIR} to the epidemic of
plague in Bombay in 1905--1906.  The observed data (dots in
Figure~\ref{fig:Bombay}) were weekly numbers of deaths from plague.

Referring to their version of Figure~\ref{fig:Bombay}, \KM
\cite[p.\,714]{KermMcKe27} argued that ``As at least 80 to 90 per
cent.\ of the cases reported terminate fatally, the ordinate may be
taken as approximately representing [$\dbydt{R}$] as a function of
$t$.''  Since (non-human) computers did not yet exist \cite{Camp09},
and an exact analytical form for this function could not be found,
they proceeded to assume \cite[p.\,713]{KermMcKe27} that
$\frac{\beta}{\gamma}R(t)\ll1$, which yields the approximate
analytical form,
\begin{linenomath*}
\begin{equation}\label{eq:sech}
\ddt{R} \approx a \sech^2{(\omega\,t - \phi)}\,.
\end{equation}
\end{linenomath*}
Noting that the \term{basic reproduction number} is
\begin{linenomath*}
\begin{equation}\label{eq:R0def}
\R_0 = \frac{N\beta}{\gamma} \,,
\end{equation}
\end{linenomath*}
where $N$ is the total population size,
and hence the \term{effective reproduction number} at time $t=0$
is
\begin{linenomath*}
\begin{equation}\label{eq:Reffdef}
\Reff = \frac{S_0\beta}{\gamma} \,,
\end{equation}
\end{linenomath*}
the parameters in equation~\eqref{eq:sech} can be
written\footnote{There is a typographical error in equation (31) of
  \KM \cite{KermMcKe27}: their factor $\sqrt{-q}$ should be $(-q)$ in
  their equivalent of the parameter we call $a$.  Baca\"er
  \cite[\S3]{bacaermodel2012} corrected this error without comment.}
\begin{linenomath*}
\begin{subequations}\label{eq:sechparams}
\begin{align}
  \omega &= \frac{\gamma}{2} \sqrt{(\Reff-1)^2 +
           \frac{2I_0}{S_0}\Reff^2}\,, \label{eq:omega} \\
  \phi &= \textrm{arctanh}\left(\frac{\Reff - 1}{2\,\omega/\gamma}
         \right)\,, \label{eq:phi} \\
  \text{and}\quad
  a &= \frac{2\,\omega^2 S_0}{\gamma\,\Reff^2} \label{eq:a} \,.
\end{align}
\end{subequations}
\end{linenomath*}
\KM then presented the parameter estimates that we have listed in the
KM column of Table~\ref{tab:Bombay}, and they plotted their
``calculated'' curve, which we have reproduced in \KMcol in
Figure~\ref{fig:Bombay}.

<<functions to compute KM parameters, echo=FALSE>>=
omega_fun <- function(Reff, gamma, S0, I0)
    {(gamma/2)*sqrt((Reff-1)^2+(2*I0/S0)*Reff^2)}
phi_fun <- function(Reff, gamma, S0, I0)
    {atanh((Reff-1)/(2*omega_fun(Reff,gamma,S0,I0)/gamma))}
a_fun <- function(Reff, gamma, S0, I0)
    {(2 * S0/(gamma * Reff^2))*omega_fun(Reff,gamma,S0,I0)^2}
@

\section{How to fit the model to the data}

The \KMcol curve in Figure~\ref{fig:Bombay} does appear to provide a
reasonable fit to the data, but \KM \cite{KermMcKe27} gave no
indication of how their parameter estimates were obtained.
Whatever their process, they must have engaged in some
sort of \term{trajectory matching}, i.e., adjusting parameter values
until the model---equation~\eqref{eq:sech} in their case---is, by some
measure, close to the observed data points.  The most obvious metric
for this purpose is the Euclidean distance between the model curve and
the data.  Thus, the natural \term{objective function} to
minimize is
\begin{linenomath*}
\begin{equation}\label{eq:leastsquares}
\sum_{i=1}^n \big(f(t_i;\thetavec) - y_i\big)^2 \,,
\end{equation}
\end{linenomath*}
where the observed data are the points $\{(t_i,y_i):i=1,\ldots,n\}$,
$f(t;\thetavec)$ is the model \eqref{eq:sech}, and the parameter
vector for \KM's problem is
$\thetavec=(a,\omega,\phi)$.  Minimizing \eqref{eq:leastsquares} with
respect to $\thetavec$ would have required some heroic arithmetic
with a pencil and paper in 1927, but it is a simple task with the
aid of a modern computer.

In the following segment of R code, we fit equation \eqref{eq:sech} to
the Bombay plague data (which are included in the \code{fitode}
package that we describe below, as a data frame with columns
\code{week} and \code{mort}).  We exploit R's nonlinear least squares
function (\code{nls}), which attempts to minimize the distance
\eqref{eq:leastsquares} to the data, starting from an initial guess
(\code{start}).

<<nls-bombay>>=
sech <- function(x) {1/cosh(x)}
KM_approx <- function(t, a, omega, phi) {a * sech(omega*t - phi)^2}
KM.parameters <- c(a = 890, omega = 0.2, phi = 3.4)
nlsfit <- nls(mort ~ KM_approx(week, a, omega, phi),
              data = fitode::bombay,
              start = KM.parameters)
nls.parameters <- coef(nlsfit)
print(nls.parameters)
@

\noindent
Above, we chose as our starting guess the fitted parameter values of
\KM.  Our least squares parameter values differ from KM's by a few
percent (see Table~\ref{tab:Bombay}).  The least squares fitted
function is shown in \nlscol in Figure~\ref{fig:Bombay}.

Starting from someone else's fit is not a great way to test the
method, but fortunately the least squares fit for this problem is not
very sensitive to the initial guess.  To make a reasonable initial guess,
it often helps to think
about the meaning of parameters.  For example, in the case of
equation~\eqref{eq:sech}, it is useful to note that $a$ is the maximum
of the function, and if write $\omega t - \phi$ as
$\omega(t-\tpeak)$ then
\begin{linenomath*}
  \begin{equation}\label{eq:tpeak}
    \tpeak = \frac{\phi}{\omega}
  \end{equation}
\end{linenomath*}
is the \term{peak time} (at which the maximum occurs); both $a$ and
$\tpeak$ can be guessed approximately by looking at the plotted data.
Assuming $I_0/S_0\ll1$, $\omega$ is half the initial exponential
growth rate, so it can be approximated easily by plotting the data on
a log scale, estimating the initial slope, and dividing by 2.
\djde{Intuitively at least, fitting $\tpeak$ or $\phi$ is likely to be
  much better than fitting the initial condition $I_0$.  Is this worth
  commenting on?}  Very rough guesses for $a$, $\tpeak$ and $\omega$
are sufficient to converge on the same fit:

<<nls-bombay-2>>=
a.guess <- 1000    # crude "by eye" estimate of peak value,
tpeak.guess <- 15  # peak time,
omega.guess <- 1   # and half the initial growth rate
phi.guess <- omega.guess * tpeak.guess
nlsfit <- nls(mort ~ KM_approx(week, a, omega, phi),
              data = fitode::bombay,
              start = c(a = a.guess, omega = omega.guess,
                        phi = phi.guess))
print(nls.parameters <- coef(nlsfit))
@

\noindent
However, if you experiment with initial guesses, you will find that if
you make a sufficiently \emph{bad} guess, then \code{nls} will fail.
For example, starting from $a=2000$, $\tpeak=5$, and $\omega=0.1$
yields a \code{singular gradient} error.  More interestingly, starting
from $a=500$, $\tpeak=5$, and $\omega=0.1$ yields $a=869$,
$\omega=-0.19$, $\phi=-3.48$, which is far from our fitted values and
illustrates a very important fact: there is \emph{not necessarily a
  unique best fit set of parameters!}  In this case, the alternative
solution exists because $\sech^2(x)$ is symmetric about the $y$ axis,
but in general, there can be multiple local minima that cause
nonlinear optimizers to converge to points that may or may not represent equally
good fits to the data.  The potential existence of
multiple local optima makes fitting models to data
hard; you need to be cautious, and use common sense,
in interpreting the solutions found by your software (always
plot the solutions! Raue \emph{et al.}
\cite{raue2013lessons} give suggestions for how to diagnose
and handle multiple optima).

If you know that your parameters should be in a certain range, then
you can exclude values outside that range.  For example, to ensure
that all the parameters are non-negative (and exclude the alternative
fit above), you would add the \code{nls} option

<<eval=FALSE>>=
lower = c(a = 0, omega = 0, phi = 0)
@

\noindent
which would prevent convergence to negative $\omega$ and $\phi$.
Alternatively, you could write
\begin{linenomath*}
  \begin{equation}\label{eq:loglink}
    a=e^A,\quad \omega=e^\Omega,\quad \phi=e^\Phi \,,
  \end{equation}
\end{linenomath*}
and fit $A$, $\Omega$, and $\Phi$, which would guarantee positive $a$,
$\omega$, and $\phi$ without having to constrain the values of the
fitted parameters.  This last suggestion may just seem like a cute
trick, but there is more to it than that.  A much broader range of
optimization algorithms is available for unconstrained fitting;
numerical parameter values of very small magnitude can also lead to
numerical instability, so it is advantageous to link parameters that
must lie in a given range to unconstrained parameters that can be fit
more easily.  In equation~\eqref{eq:loglink}, the \term{link function}
that converts the parameters to the unconstrained scale is $\log$.
Another common link function is logit (the inverse of the logistic
function), which converts the unit interval $(0,1)$ to
$(-\infty,\infty)$, and is especially convenient when parameters
represent proportions or probabilities.  (Requiring positivity is so
common that the default setting in \code{fitode} is to use a log link
when fitting any parameter.)

<<KM-fitted-values, echo=FALSE>>=
dig <- 3 # number of significant digits for printing
assignfun <- function(par,
                      text=".KM") {
    parnames <- names(par)

    for (p in parnames) {
        assign(paste0(p, text), signif(par[[p]], dig),
               envir=globalenv())
    }
}

assignfun(KM.parameters)
tpeak.KM <- with(as.list(KM.parameters), signif(phi / omega,dig))
@

<<nls-fitted-values, echo=FALSE>>=
assignfun(nls.parameters, text=".nls")
tpeak.nls <- with(as.list(nls.parameters), signif(phi / omega,dig))
cc <- suppressMessages(confint(nlsfit))
## would like to use built-in knitr/Sexpr{} magic but ...
## from https://stackoverflow.com/questions/8366324/r-sweave-formatting-numbers-with-sexpr-in-scientific-notation
format_sn0 <- function(x, digits = dig,
                      max_ord = 2,
                      ensuremath = TRUE) {

    if (x==0) return("0")
    ord <- floor(log(abs(x), 10))
    if (abs(ord) <= abs(max_ord)) return(as.character(signif(x, dig)))
    x <- x / 10^ord
    x <- signif(x, digits=dig)
    r <- sprintf("%s\\times 10^{%s}", x, ord)
    if (ensuremath) r <- sprintf("\\ensuremath{%s}", r)
    return(r)
}
format_sn <- Vectorize(format_sn0,
                       vectorize.args = "x")
## format_sn <- knitr::knit_hooks$get("inline")
ci_fmt  <- function(p, cc, sn = TRUE, ...) {
    ffun <- if (sn) format_sn else format
    ( ffun(cc[p,], ...)
    |> paste(collapse = ", ")
    |> sprintf(fmt = "(%s)")
    )
}
assignfun(cc[,"2.5%"], text=".lwr")
assignfun(cc[,"97.5%"], text=".upr")
@

If we accept our fit as satisfactory, what can we infer about the
dynamics of plague that \KM were attempting to capture with the SIR
model \eqref{eq:SIR}?  We need to convert the parameters of KM's
approximation \eqref{eq:sechparams} back to the original parameters
that are directly related to the mechanism of disease spread formalized by the model (i.e., $\beta$ and $\gamma$, and initial conditions
$S_0$ and $I_0$).

\FloatBarrier

The nonlinear algebraic relationships specified by
equation~\eqref{eq:sechparams} can be inverted
analytically\footnote{In (common) situations in which
  nonlinear algebraic equations cannot be solved analytically,
  they can still be solved numerically, for example with
  the \code{nleqslv} package in R.}
\cite[\S3]{bacaermodel2012}, to obtain
\begin{linenomath*}
\begin{subequations}\label{eq:invertparams}
\begin{align}
  \Reff &= \frac{1}{2}\bigg( 1 +
          \sqrt{1 + \frac{4\,\omega\,I_0 \sinh\!{(2\phi)}}{a}}\bigg) \,,
          \label{eq:Reff} \\
  \gamma &= \frac{2\,\omega\tanh{\phi}}{\Reff-1} \,, \label{eq:gamma} \\
  S_0 &= \frac{2\,\Reff\,I_0 \sinh^2\!{\phi}}{(\Reff-1)^2} \,. % Bacaer eq. (3)
      \label{eq:S0}
\end{align}
\end{subequations}
\end{linenomath*}
Since there are four original parameters ($\beta$, $\gamma$, $S_0$,
$I_0$) and only three parameters in \KM's approximation
\eqref{eq:sech} ($a$, $\omega$, $\phi$), one of the four original
parameters needs to be specified separately; above we have taken
this to be the initial prevalence $I_0$.  From
equation~\eqref{eq:invertparams}, we can compute the transmission
rate,
\begin{linenomath*}
\begin{equation}\label{eq:beta}
  \beta = \frac{\Reff\gamma}{S_0} \,,
\end{equation}
\end{linenomath*}
and the mean intrinsic generation interval \cite{ChamDush15},
\begin{linenomath*}
\begin{equation}\label{eq:Tg}
  \Tg = \frac{1}{\gamma} \,,
\end{equation}
\end{linenomath*}
which is the same as the mean infectious period in this simple model
\cite{KrylEarn13,Cham+18}.  Table~\ref{tab:Bombay} lists the values of
the parameters as estimated by \KM and by us using \code{nls}.

<<invert-KM-analytically, echo=FALSE>>=
invert_params <- function(I0, params) {
    with(as.list(params), {
        tpeak <- phi / omega
        Reff <- (1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2
	q <- (Reff-1)/tanh(phi) ## this is problematic. Not sure why.
    gamma <- 2 * omega/q ## gamma has a unit of 1/week
	S0 <- 2 * Reff * I0 * sinh(phi)^2/(Reff - 1)^2
	beta <- Reff * gamma/S0
	c(tpeak=tpeak, Reff=Reff, S0=S0, gamma=gamma, Tg=1/gamma, beta=beta)
    })
}
N.KM <- 10^6
I0.KM <- 1
orig.params.KM <- invert_params(I0 = I0.KM, params = KM.parameters)
orig.params.KM <- signif(orig.params.KM,dig)
assignfun(orig.params.KM)
R0.KM <- Reff.KM/S0.KM*N.KM
I0.nls <- 1
orig.params.nls <- invert_params(I0 = I0.nls, params = coef(nlsfit))
orig.params.nls <- signif(orig.params.nls,dig)
assignfun(orig.params.nls, ".nls")
R0.nls <- Reff.nls/S0.nls*N.KM
@

<<delta-method-ci, echo=FALSE>>==
convert_express <- list(
    tpeak=expression(phi/omega),
    Reff=expression((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2),
    gamma=expression(2 * omega/((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2-1)/tanh(phi)),
    S0=expression(2 * ((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2) * I0 * sinh(phi)^2/(((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2) - 1)^2),
    beta=expression((((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2) - 1) * omega * tanh(phi)/(I0 * sinh(phi)^2)),
    Tg=expression(1/(2 * omega/((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2-1)/tanh(phi))),
    R0=expression((((1 + sqrt(1 + 4 * omega * I0 * sinh(2 * phi)/a))/2) - 1)^2/(2 * I0 * sinh(phi)^2)*N)
)

KMcifun <- function(fit,
                    I0,
                    N) {
    param <- coef(fit)
    ##dg/dx
    dgdx <- sapply(convert_express, function(x) {
        dd <- deriv(x, c("a", "omega", "phi"))
        ee <- eval(dd, c(as.list(param), I0=I0, N=N))
        attr(ee, "gradient")
    })

    vcov <- vcov(fit)
    est_vcov <- t(dgdx) %*% vcov %*% dgdx
    est_err <- sqrt(diag(est_vcov))
    est_par <- sapply(convert_express, eval, c(as.list(param), I0=I0, N=N))
    z <- -qnorm((1-0.95)/2)

    matrix(
        c(est_par - z * est_err, est_par + z * est_err),
        ncol=2,
        dimnames=list(
            c("tpeak", "Reff", "gamma", "S0", "beta", "Tg", "R0"),
            c("2.5%", "97.5%")
        )
    )
}

cc.delta <- KMcifun(nlsfit, I0=I0.KM, N=N.KM)
assignfun(cc.delta[,"2.5%"], text=".lwr")
assignfun(cc.delta[,"97.5%"], text=".upr")
@

<<delta-method-band, echo=FALSE>>==
traj_express <- expression(a * 1/cosh(omega*t - phi)^2)

traj_ci <- function(t,
                    parameters,
                    vcov,
                    level=0.95) {
    traj <- eval(deriv(traj_express, c("a", "omega", "phi")), c(as.list(parameters), t=t))

    traj_dgdx <- attr(traj, "gradient")

    est_vcov <- traj_dgdx %*% vcov %*% t(traj_dgdx)
    est_err <- sqrt(diag(est_vcov))
    ll <- (1-level)/2
    z <- -qnorm(ll)

    dd <- data.frame(
        times=t,
        estimate=traj,
        lwr=traj-est_err*z,
        upr=traj+est_err*z
    )
    names(dd)[3:4] <- c(paste(100*ll, "%"), paste(100*(1-ll), "%"))

    dd
}
@

\paragraph*{Correctly handling weekly mortality.}

Something we have glossed over is that we have fitted observed weekly
mortality to the \emph{instantaneous} rate, $\dbydt{R}$
\eqref{eq:sech}, which is not observed.  We did this because it is
what \KM did, and we wanted to be able to compare formal nonlinear
least squares fits to \KM's results\footnote{In his reanalysis of
  \KM's results, Baca\"er \cite{bacaermodel2012} also retained this
  conceptual error.}.  Weekly mortality reported at time $t_i$ should
really be modelled as the aggregation of $\dbydt{R}$ over the
preceding week, i.e., it would be better to define
\begin{linenomath*}
  \begin{subequations}\label{eq:sechcorrect}
    \begin{align}
      f(t_i;\thetavec) &=
                         \int_{t_{i-1}}^{t_i} \ddt{R}\,\dt
\label{eq:correct.mortality} \\
                       &= \int_{t_{i-1}}^{t_i}  a \sech^2{(\omega\,t - \phi)}\,\dt \\
                       &= \frac{a}{\omega}\Big(\tanh{(\omega\,t_i - \phi)}
                         - \tanh{(\omega\,t_{i-1} - \phi)}\Big) \,.
    \end{align}
  \end{subequations}
\end{linenomath*}
Indeed, whether we are fitting to mortality or incidence or another
instantaneous rate, we should be integrating over the observation
interval, which is precisely what we do below when fitting to the ODEs
directly.

\section{Uncertainty}

To this point, we have addressed only an optimization problem.  We
solved it using the method of nonlinear least squares, which yields
estimates of the values of the parameters of the model
\eqref{eq:sech}.  But our best estimates are just that:
\emph{estimates}, not known values of the parameters.

To quantify uncertainty in our estimates, we need a statistical
framework.  The typical output of such a framework is a
\term{confidence interval} (CI) within which our best estimate lies.
For example, the final column of Table~\ref{tab:Bombay} lists 95\% CIs
on our \code{nls} parameter estimates, and the pink shaded region in
the top panel of Figure~\ref{fig:Bombay} is a 95\% \term{confidence
  band}, which shows CIs for each point of the fitted model curve.

To understand how to estimate CIs, we will start by thinking about the
probability of observing the data $\{y_i\}$.  We imagine that the
model \eqref{eq:sech} is a perfect representation of reality, and we
consider the deviations from the model curve in
Figure~\ref{fig:Bombay} to be observation errors.  We then imagine
that observation error for each data point is independent and
identically distributed (iid), and drawn from a Normal distribution
with zero mean and standard deviation $\sigma$ equal to the standard
deviation of the residuals (the differences between the model curve
and the observed data).  Then the probability of the data given the
model is
%%
\begin{linenomath*}
\begin{equation}\label{eq:prob.data.given.model}
\Pop(\text{data} \mid \text{model})
=
\prod_{i=1}^n
\left[\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{\big(f(t_i;\thetavec) - y_i\big)^2}{2\sigma^2} \right) \Delta{y_i}
\right]
\,.
\end{equation}
\end{linenomath*}
%%

Using these assumptions we can adopt a \emph{maximum likelihood} framework,
where we consider parameter values that maximize the probability
of observing the data (\ref{eq:prob.data.given.model}) to be the best
\cite{Bolk08}.
We define the \term{likelihood} $\lik$ of a set of
parameter values $\thetavec$ as
\begin{linenomath*}
\begin{equation}\label{eq:lik}
\lik(\thetavec)
= \Pop (\{y_i\} \mid \thetavec) \,.
\end{equation}
\end{linenomath*}
Maximizing $\lik$ with respect to $\thetavec$ or, equivalently,
minimizing the negative log-likelihood,
yields an estimate,
\begin{linenomath*}
\begin{subequations}\label{eq:negloglik}
\begin{align}
\thetavechat
&= \argmax_\thetavec \lik(\thetavec) \\
&= \argmin_\thetavec \big(\!\!-\log{\lik(\thetavec)}\big) \\
&= \argmin_\thetavec\Big(
\sum_{i=1}^n \big(f(t_i;\thetavec) - y_i\big)^2
\;+\; \text{constant}
\Big) \\
&= \argmin_\thetavec\sum_{i=1}^n \big(f(t_i;\thetavec) - y_i\big)^2
 \,,
\end{align}
\end{subequations}
\end{linenomath*}
which---lo and behold---agrees exactly with \eqref{eq:leastsquares},
the least squares solution!  The standard way of expressing this is to
say that the least squares solution $\thetavechat$ is the
\term{maximum likelihood estimate} (MLE) of $\thetavec$, under the
assumption of independent, identically distributed
(i.e., mean-zero, constant-variance) Normal observation errors in the time series.

Having introduced the idea of maximum likelihood, we can do better by
making a more realistic assumption about the error distribution.  We
will then end up with a different likelihood function to maximize, and
obtain a different $\thetavechat$, but the basic idea is the same.

So what is a better assumption about the observation error
distribution, and how can we use the likelihood function to estimate
uncertainty in $\thetavechat$ and on the fitted trajectory?

Our data are actually non-negative, discrete counts of deaths (or
cases in other epidemiological contexts),
so a continuous, real-valued Normal distribution is
somewhat unrealistic. More importantly, we expect (and can see
in the plots of our fitted curves) that the
magnitude of error in the observations will vary over the course of
the epidemic; the error might be $\pm 2$ at the beginning of the
epidemic when mortality is low and $\pm 50$ at the peak.

We could address both of these problems by using a
Poisson distribution of observations with mean equal to the fitted
model trajectory [equation~\eqref{eq:SIR;R}]. This handles
discrete observations and allows the variance to change as a function
of the mean. However, the Poisson distribution assumes
\term{equidispersion}---the variance is equal to the mean---while
typical observation errors are \term{overdispersed}, meaning that
the variance is greater than the mean. Ignoring overdispersion
will underestimate the uncertainty in the parameters and lead
to overly narrow confidence intervals on parameters and predictions.
The negative binomial distribution is one way to generalize
the Poisson to allow for overdispersion \citep{linden2011using}.

The probability mass function for the \term{negative binomial
  distribution} (for counts $x=0,1,2,\ldots$) is
\begin{linenomath*}
\begin{equation}\label{eq:NB}
  \texttt{NB}(x;\mu,k) =
  \frac{\Gamma(k+x)}{\Gamma(k) x!}
  \left(\frac{k}{k+\mu}\right)^k
  \left(\frac{\mu}{k+\mu}\right)^x \,
\end{equation}
\end{linenomath*}
where the predicted variance of a particular observation
is given by $\mu_i (1+\mu_i/k)$.
The maximum likelihood estimate is, therefore,
\begin{linenomath*}
  \begin{equation}
\label{eq:negbinomMLE}
\begin{split}
 \thetavechat
 &= \argmin_\thetavec\sum_{i=1}^n \Big(\!\!
                -\log\Gamma\big(y_i+k\big)
                +\log\Gamma(k) + \log(x!) \\
                &\hspace{3cm}
                -k \log\left(\frac{k}{k+\mu_i(\thetavec)}\right)
                -y_i \log\left(\frac{\mu_i(\thetavec)}{k+\mu_i(\thetavec)}\right)
                \Big)
  \,,
\end{split}
  \end{equation}
\end{linenomath*}
with  $\mu_i(\thetavec) = f(t_i;\thetavec)$.

%% where, as for the likelihood associated with Normal errors, we assume
%% $\sigma^2$ is computed as the variance of the residuals across the full
%% time series.\djde{Is that what's done, or is $\sigma^2$ added to
%%   $\thetavec$ as a parameter to fit?  or even $\sigma_i^2$ for each
%%   i?}
%% \swp{It is what's done. There's a version where $\sigma^2$ is added
%% to $\thetavec$ but it is not used when we call the ols function. $\sigma_i^2$
%% doesn't make sense because we have one residual per $i$.}

Regardless of the form of the likelihood function, we can use it to
obtain CIs on the MLE $\thetavechat$.  A relatively simple approach is
to use the the curvature of $-\log{\lik(\thetavec)}$ at $\thetavechat$
to infer parameter values of a multivariate Normal distribution for
$\thetavec$.  At $\thetavechat$, the shape of $-\log{\lik}$ is
described by its \term{Hessian matrix} (the matrix of second order
partial derivatives of $-\log{\lik}$, also known as the \term{Fisher
  information matrix}), and the inverse of the Hessian is the
\term{variance-covariance matrix} $\mathrm{Cov}(\thetavec)$ that
specifies the desired multivariate Normal with mean $\thetavechat$.
This relationship between $\mathrm{Cov}(\thetavec)$ and the Hessian of
$-\log{\lik}$ is, admittedly, not obvious!  See \citep[\S6.5]{Bolk08}
for a heuristic explanation or \cite{Wassermanall2010} for a rigorous
(if terse) explanation.\djde{It would be nice to be more
  self-contained than this, if we can think of a useful way of doing
  so.  Can you give a section or page number in Wasserman?}
\bmb{probably chapter 9 ``Parametric Inference''. SpringerLink via McMaster
  \href{https://link-springer-com.libaccess.lib.mcmaster.ca/book/10.1007/978-0-387-21736-9}{won't give me sufficient access to see exactly what's in there}}

\hypertarget{DeltaMethod}{}
The diagonal elements of $\mathrm{Cov}(\thetavec)$ are the (estimated)
variances of the parameter estimates, so we can take their (positive)
square roots to get the standard error (SE) and compute approximate
95\% confidence intervals by adding $\pm1.96\,\mathrm{SE}$ to
$\thetavechat$.  To obtain CIs on functions of the fitted parameters
(e.g., $\R_0$ or $\gamma$ if our model is KM's approximation
\eqref{eq:sech}), we build on the idea that if the error in a
parameter $a$ is $\Delta a$, then the associated error in a
(differentiable) function $g(a)$ is $\Delta{g}\approx g'(a)\Delta{a}$.
Given a (smooth) nonlinear function $g(\thetavec)$ of the parameters,
the \term{Delta Method} \cite{Dorf38,VerH12} expands
$\textrm{Var}(g(\thetavec))$ to first order about $\thetavechat$,
which gives us the variance-covariance matrix of $g(\thetavec)$
\cite{Bolk08}.  In particular, the variance of $g(\thetavec)$ is
\begin{linenomath*}
\begin{subequations}\label{eq:DeltaMethod}
\begin{align}
\mathrm{Var}(g(\thetavec))
&\approx
\mathrm{Var}\big[g(\thetavechat)
   + (\nabla g)(\thetavechat)\cdot(\thetavec-\thetavechat)\big] \\
&=
\mathrm{Var}\big[(\nabla g)(\thetavechat)\cdot(\thetavec-\thetavechat)\big] \\
&=
\mathbb{E}\big[\big((\nabla g)(\thetavechat)\cdot(\thetavec-\thetavechat)\big)^2\big] \\
&=
\,\mathbb{E}\big[
(\nabla g)(\thetavechat)^\transpose
  (\thetavec-\thetavechat)
  (\thetavec-\thetavechat)^\transpose
  (\nabla g)(\thetavechat)
  \big]\,
\\
&=
(\nabla g)(\thetavechat)^\transpose
\,\mathbb{E}\big[
  (\thetavec-\thetavechat)
  (\thetavec-\thetavechat)^\transpose
  \big]\,
(\nabla g)(\thetavechat) \\
&=
(\nabla g)(\thetavechat)^\transpose
\,\textrm{Cov}(\thetavec)\,
(\nabla g)(\thetavechat)
\end{align}
\end{subequations}
\end{linenomath*}
We can again get the 95\% CIs by taking square roots and computing
$g(\thetavechat) \pm1.96\,\mathrm{SE}$.

Given a fit of KM's approximation \eqref{eq:sech} to the time series
data, which yields $\thetavechat=(\hat{a},\hat{\omega},\hat{\phi})$, we
can apply the Delta method \eqref{eq:DeltaMethod} to the nonlinear
relationships \eqref{eq:invertparams} to obtain CIs on
$g(\thetavechat)=(\hat{\Reff}, \hat{\gamma},\hat{S_0})$.  This is
precisely how we obtained the CIs on the derived parameters listed in
Table~\ref{tab:Bombay}.  Perhaps less obviously, we can also use the
Delta method to obtain CIs on the fitted trajectory at each
observation time $t_i$ (and hence obtain a confidence band) by
considering $g(\thetavec)=f(t_i;\thetavec)$.  This is how we obtained
the pink confidence band shown in Figure~\ref{fig:Bombay}.

\section{Fitting the ODE}

Until now, we have focused on fitting KM's approximation
\eqref{eq:sech} rather than actual solutions of the SIR model
\eqref{eq:SIR}.  If we had an exact analytical solution of the SIR
ODE~\eqref{eq:SIR} then we could proceed as above, replacing the
approximate analytical expression \eqref{eq:sech} with the exact
formula.  Since we do not have an exact solution, we must instead rely
on numerical solutions of the ODE.  In general, fitting numerical
solutions of ODEs to data introduces significant coding/computational
challenges, but conceptually the problem is the same as if we did have
an analytical formula.  We can still use the Delta method
\eqref{eq:DeltaMethod} to estimate uncertainty, but calculating the
gradient $(\nabla g)(\thetavec)$ is not straightforward if $g$ is a
numerical solution of an ODE; we must simultaneously solve a set of
\term{sensitivity equations} alongside the main differential equations
\cite{raue2013lessons}. Sensitivity equations are an auxiliary set of
ODEs that define the time derivatives of the derivatives of the state
variables with respect to the parameters, i.e.,
$\frac{\rm d}{{\rm d}t}\big(\frac{\partial
  x(t,\theta_i)}{\partial\theta_i}\big)$, where $\theta_i$ refers to
the components of $\thetavec$\djde{Ben, I changed total derivative wrt
  $\theta$ to partial derivative wrt $\theta_i$, which made more sense
  to me, but I haven't looked at the reference.}. They can easily be
derived using the chain rule; by solving them in parallel with the
main set of ODEs, we get a computationally efficient and numerically
stable way to calculate the overall gradients of the log-likelihood
with respect to the parameters.

The \code{fitode} package\footnote{\code{fitode} is available on
  \href{https://cran.r-project.org/}{CRAN}and consequently can be
  installed via \code{install.packages("fitode")}.} does all of this
computational work under the hood, and makes it as easy for a user to
fit an ODE to data as it was for us to use \code{nls} above to fit a
curve based on an analytical formula.  We illustrate the use of the
package by fitting the SIR model~\eqref{eq:SIR} to the Bombay plague
epidemic.

We begin by loading the package
<<load fitode, message=FALSE>>=
library(fitode)
@
\noindent
and defining a model object:

\vbox{
<<sirmortmodel_correct>>=
SIR_model <- odemodel(
    name="SIR model",
    model=list(
        S ~ - beta * S * I,
        I ~ beta * S * I - gamma * I,
        R ~ gamma * I
    ),
    observation = list(
        mort ~ dnbinom(mu = R, size = k)
    ),
    diffnames="R",
    initial=list(
        S ~ S0,
        I ~ I0,
        R ~ 0
    ),
    par=c("beta", "gamma", "S0", "I0", "k")
)
@
}% end vbox
\noindent
In the model definition above:
\begin{description}
\item[\code{model}]specifies the vector field given by the ODE \eqref{eq:SIR}.
\item[\code{observation}]specifies that the observed data
  (\code{mort}) are assumed to arise from sampling from the negative
  binomial distribution [\code{dnbinom}, equation~\eqref{eq:NB}] with
  overdispersion parameter $k$; the mean of the distribution is given by
  the fitted model trajectory [equation~\eqref{eq:correct.mortality}],
\begin{linenomath*}
\begin{equation}
  \mu(t_i) = \int_{t_{i-1}}^{t_i} \ddt{R}\,\dt
  = R(t_i) - R(t_{i-1})
  \label{eq:correct.mortality2}
 \,,
\end{equation}
\end{linenomath*}
Fitting to such differences is implemented by using the
\code{diffnames} argument to specify the state variable for which
consecutive differences are to be used (so, if the focal variable is
$R$ then \code{fitode} fits to $R(t_i) - R(t_{i-1})$ rather than
$R(t_i)$).  Ordinary least squares can be implemented by simply
changing the \code{observation} argument to
{\tt
  \hlstd{mort}
  \!\!{\color{black}\~{}}\!\!
  \hlkwd{ols}\hlstd{(}\hlkwc{mean} \hlstd{= R)}
}
\item[\code{initial}]conditions are expressed as numbers of individuals.
\item[\code{par}]refers to the parameters that are to be fitted:
  $\beta$, $\gamma$, initial conditions $S(0)$ and $I(0)$, and the overdispersion parameter $k$.
\end{description}
Since we are taking the difference $\mu(t_i) = R(t_i) - R(t_{i-1})$ to calculate the mortality trajectory, we have to add an extra row representing $t_0$ to the data set in order to compute $\mu(t_1) = R(t_1) - R(t_0)$:
<<sirmortfit_data, cache=TRUE, warning=FALSE>>=
bombay2 <- rbind(
    c(times=bombay$week[1] -
          diff(bombay$week)[1], mort=NA),
    bombay
)
@
Taking our previous parameter estimate from \code{nls} as our starting values, we can fit the model by calling the \code{fitode}: function
<<sirmortfit,cache=TRUE,warning=FALSE>>=
SIR_start <- c(beta=beta.nls, gamma=gamma.nls, I0=I0.KM, S0=S0.nls, k=50)
SIR_fit <- fitode(
    model = SIR_model,
    data = bombay2,
    start = SIR_start,
    fixed = list(gamma=gamma.nls),
    tcol = "week"
)
@
In the fitting function above:
\begin{description}
\item[\code{model}]specifies the ODE model to be fitted.
\item[\code{data}]specifies the data.
\item[\code{start}]specifies starting parameter set for the optimization.
\item[\code{fixed}]specifies parameter values to be fixed (and therefore not estimated). Here, we chose to assume that the recovery rate is known (using the \code{fixed} argument) due to parameter unidentifiability. In short, this means that we can obtain nearly identical fits across a wide range of $\gamma$. We discuss this issue in detail in Supplementary Materials.
\item[\code{tcol}]specifies the time column of the data frame.
\end{description}
The resulting fits are plotted in Figure~\ref{fig:Bombay}).
The estimated coefficients and the associated confidence intervals can be
obtained via \code{coef} and \code{confint} functions (Table~\ref{tab:bombay.fitode});
the latter function also allows users to obtain confidence intervals for derived parameters using the Delta method.
We note that \code{fitode} gives sharp prediction, rather than smooth curves, because we are calculating daily mortality at a discrete time scale using equation~\ref{eq:correct.mortality2}.

\swp{overall, the bombay data set is a really bad example for fitting SIR model. Not sure what else to do or say.}

<<sirmortfitcoef, include=FALSE>>==
coef.bombay.fitode.nb <- coef(SIR_fit)
ci.bombay.fitode.nb <- confint(SIR_fit)[,-1]
assignfun(coef.bombay.fitode.nb, ".bombay.fitode.nb")
@

<<sirmortconfint, include=FALSE>>==
ci.derived.bombay.fitode.nb <- confint(SIR_fit,
        parm=list(
            Reff~beta*S0/gamma.nls,
            R0~beta*N.KM/gamma.nls,
            Tg~1/gamma.nls
        ))
assignfun(ci.derived.bombay.fitode.nb[,1], ".bombay.fitode.nb")
@

<<confband, include=FALSE>>=
tmax <- 33
tvals <- 0:tmax
SIR_confband <- predict(SIR_fit, times=tvals, level=0.95)$mort
@

% OLS:
<<sirmortmodel, include=FALSE>>=
SIR_model_ols <- odemodel(
    name="SIR model",
    model=list(
        S ~ - beta * S * I,
        I ~ beta * S * I - gamma * I,
        R ~ gamma * I
    ),
    observation = list(
        mort ~ ols(mean = R)
    ),
    initial=list(
        S ~ S0,
        I ~ I0,
        R ~ 0
    ),
    par=c("beta", "gamma", "S0", "I0"),
    diffnames="R"
)
@

% Also fixing gamma for now:
<<sirmortfit_correct, cache=TRUE, warning=FALSE, include=FALSE>>=
SIR_start_ols <- coef(SIR_fit)[-5]

SIR_fit_ols <- fitode(
    SIR_model_ols,
    data = bombay2,
    start = SIR_start_ols,
    fixed = list(gamma=gamma.nls),
    tcol = "week"
)
@

<<sirmortfitcoefols, include=FALSE>>==
coef.bombay.fitode.ols <- coef(SIR_fit_ols)
ci.bombay.fitode.ols <- confint(SIR_fit_ols)[,-1]
assignfun(coef.bombay.fitode.ols, ".bombay.fitode.ols")
@

% \swp{This is also broken. Having trouble calculating vcov...}
<<sirmortfit_ols_confband, eval=FALSE, include=FALSE>>==
SIR_ols_confband <- predict(SIR_fit_ols, times=tvals, level=0.95)$mort
@

<<bombay_likelihood_fit, cache=TRUE, warning=FALSE, message=FALSE, include=FALSE>>==
if (file.exists("bombay-likelihood.rda")) {
    load("bombay-likelihood.rda")
} else {
    nfits <- 100
    gammavec <- seq(1, 28, length.out=nfits)
    loglikvec <- rep(NA, nfits)
    fitlist <- vector('list', length(gammavec))

    for (i in 1:nfits) {
        if (i==1) {
            start_ll <- coef(SIR_fit2)
        } else {
            start_ll <- coef(fitlist[[which.max(loglikvec[1:(i-1)])]])
        }

        fitlist[[i]] <- try(fitode(
            SIR_model,
            data = bombay2,
            start = start_ll,
            fixed = list(gamma=gammavec[i]),
            tcol = "week"
        ))

        if (!inherits(fitlist[[i]], "try-error")) {
            loglikvec[i] <- logLik(fitlist[[i]])
        }
    }
    save("gammavec", "loglikvec", "fitlist", file="bombay-likelihood.rda")
}
@

<<bombay_likelihood_plot, cache=TRUE, include=FALSE>>==
plot(gammavec, loglikvec,
     xlab="gamma (1/week)",
     ylab="log likelihood",
     ylim=c(-140, -135.5))
@

\section{Example for which KM approx is bad (via stochastic SIR)}

<<compute-stochastic-SIR-via-sirr, include=FALSE>>=
sirstoch_fn <- system.file("vignette_data", "sirstoch.RData", package = "fitode")
if (file.exists(sirstoch_fn)) {
    load(sirstoch_fn)
} else {
    library(sirr)
    R0 <- 5
    N <- 2000
    mm <- create_SIRmodel(R0=R0, N=N)
    ii <- set_inits(mm)
    iii <- ii
    iii[2] <- exp(iii[2])
    names(iii) <- c("S","I","R")
    ## FIX: this should not be necessary, but it is a bug in sirr
    ##      associated with N != 1.
    mm1 <- create_SIRmodel(R0=R0, N=1)
    sir.det <- compute_SIRts(mm1)
    ## get associated params for KM approx:
    a.det <- a_fun(Reff=R0*iii["S"]/2000, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    omega.det <- omega_fun(Reff=R0*iii["S"]/2000, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    phi.det <- phi_fun(Reff=R0*iii["S"]/2000, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    ##
    tt <- 0:10
    sir.stoch <- get_ssa_soln(mm, stopcrit = NULL, inits=ii, times=tt)
    save(R0, N, mm, ii, iii, mm1, sir.det, a.det, omega.det, phi.det, tt,
         sir.stoch, file="sirstoch.RData")
}
@

\swp{need a proper time unit?}
We now apply the same methodologies to a simulated mortality curve.
In particular, we simulated a stochastic SIR model using an adaptive tau algorithm, with
$(S_0,I_0,R_0)=(\Sexpr{iii})$, $\R_0=\Sexpr{R0}$,
$\gamma=\Sexpr{mm$gamma}/\textrm{week}$.
The simulated daily mortality curve is shown in Figure~\ref{fig:stoch} alongside the smooth deterministic trajectory of epidemic mortality ($dR/dt$).
For this particular stochastic realization, the epidemic takes off slowly, resulting in a time shifted difference between the deterministic and stochastic trajectories.

In this case, the Kermack and McKendrick's method gives a poor approximation of the dynamics using original parameters.
The approximation predicts a much lower epidemic peak and a shorter duration of the epidemic.
\swp{Parameter values are a little screwed up at the moment. Doesn't seem like it's a trivial fix.}
Nonetheless, we can still fit the KM approximation to the simulated epidemic and obtain a visually good fit.
\swp{we don't want to interpret these parameter estimates as being ``wrong'' until we figure out the parameter conversion issues.}

On the other hand, the SIR model gives a much better fit.
We estimate a much lower initial prevalence to match the delayed onset of an epidemic;
otherwise, all other parameter estimates are nearly identical to the original parameters.

<<colours, include=FALSE>>=
my.red <- "#E41A1C"     # colour-blind friendly red
my.blue <- "#377EB8"    # colour-blind friendly blue
my.green <- "#4DAF4A" # colour-blind friendly green
my.yellow <- "#FFD92F" # colour-blind friendly yellow
@

<<sirstochdata, include=FALSE>>==
## convert to daily mortality
mort.stoch <- diff(sir.stoch$R[!duplicated(ceiling(sir.stoch$time*7),fromLast = TRUE)])
mort.time <- ceiling(sir.stoch$time[!duplicated(ceiling(sir.stoch$time*7),fromLast = TRUE)]*7)
mort.which <- which(diff(mort.time)==1)

sir.stoch.data <- data.frame(
    time=mort.time[mort.which]/7,
    mort=c(NA, mort.stoch[head(mort.which, -1)])
)
@

<<sirnlsfit, cache=TRUE, warning=FALSE, include=FALSE>>==
nlsfit.stoch <- nls(mort ~ KM_approx(time, a, omega, phi),
              data = sir.stoch.data,
              start = c(a = unname(a.det), omega = unname(omega.det),
                        phi = unname(phi.det)))

nls.stoch.parameters <- coef(nlsfit.stoch)
assignfun(nls.stoch.parameters, text=".nls.stoch")

orig.params.KM.stoch <- invert_params(I0 = exp(ii[[2]]), params = c(a=unname(a.det), phi=unname(phi.det), omega=unname(omega.det)))
orig.params.KM.stoch <- signif(orig.params.KM.stoch)
assignfun(orig.params.KM.stoch, ".KM.stoch")

orig.params.nls.stoch <- invert_params(I0 = exp(ii[[2]]), params = coef(nlsfit.stoch))
orig.params.nls.stoch <- signif(orig.params.nls.stoch)
assignfun(orig.params.nls.stoch, ".nls.stoch")

cc.nls.stoch <- suppressMessages(confint(nlsfit.stoch))

cc.delta.stoch <- KMcifun(nlsfit.stoch, I0 = exp(ii[[2]]), N=N)
assignfun(cc.delta.stoch[,"2.5%"], text=".lwr.nls.stoch")
assignfun(cc.delta.stoch[,"97.5%"], text=".upr.nls.stoch")

@

<<sirstochfit, cache=TRUE, warning=FALSE, include=FALSE>>=
start.stoch <- c(beta=mm$beta, gamma=mm$gamma,
                 S0=iii["S"], I0=iii["I"])
names(start.stoch) <- c("beta","gamma","S0","I0")
print(start.stoch)

SIR_stoch_fit <- fitode(
    SIR_model,
    data = sir.stoch.data,
    start = c(start.stoch, k=10),
    fixed = c(gamma = 1), # Tg = 1/gamma = 1 week
    tcol = "time"
)
sss <- predict(SIR_stoch_fit, level=0.95)[[1]]
summary(SIR_stoch_fit)
@

<<stochestassign, include=FALSE>>==
coef.stoch.fitode <- coef(SIR_stoch_fit)
ci.stoch.fitode <- confint(SIR_stoch_fit)[,-1]
assignfun(coef.stoch.fitode, ".stoch.fitode")
@

% Confint on derived parameers:
<<sirstochconfint, include=FALSE>>==
ci.derived.stoch.fitode <- confint(SIR_stoch_fit,
        parm=list(
            Reff~beta*S0/1,
            R0~beta*2000/1
        ))
assignfun(ci.derived.stoch.fitode[,1], ".stoch.fitode")
@

\section{Influenza in Philadelphia, October 1918}

Finally, we analyze the influenza outbreak in Philadelphia, 1918.

<<philapop, echo=TRUE, include=FALSE>>=
## https://en.wikipedia.org/wiki/Demographics_of_Philadelphia
philapop <- data.frame(year = c(1910,1920), population = c(1549008, 1823779))
##' @param pop data frame
##' @param year year to interpolate to
pop_interp <- function( pop, year ) {
    slope <- (pop[2,2] - pop[1,2])/(pop[2,1] - pop[1,1])
    out <- round( pop[1,2] + (year-pop[1,1])*slope )
    return(out)
}
philapop1918 <- pop_interp( philapop, 1918 )
print(philapop1918)
## reduce this by case fatality proportion so we have
## the initial susceptible population who will be recorded
## as a death if they get infected:
philaS0 <- round(philapop1918 * 0.025)
print(philaS0)
@

<<read.phila.data, include=FALSE>>=
## we have to cut the data when we're using nbinom
## because there are long trailing zeroes..
## same with the beginning
phila1918a <- subset(phila1918, as.Date("1918-09-10") < date &
                                date < as.Date("1918-11-18"))
phila1918b <- data.frame(
    date=c(as.Date("1918-09-10"), phila1918a$date),
    mort=c(NA, phila1918a$mort)
)
phila1918b$time <- lubridate::decimal_date(phila1918b$date)
@

<<phila.fit,cache=TRUE,warning=FALSE, include=FALSE>>=
## time is in the unit of years
## so gamma=52 corresponds to a mean of 1 week
## not-so-random starting values chosen by hand
start.phila <- c(beta=4e-3, gamma=52*2,
                 S0=philaS0*0.9,
                 I0=3, k=50)
names(start.phila) <- c("beta","gamma","S0","I0", "k")
print(start.phila)

phila_fit <- fitode(
    SIR_model,
    data = phila1918b,
    start = start.phila,
    tcol = "time"
)
ppp <- predict(phila_fit, level=0.95)[[1]]
@

<<philaassign, include=FALSE>>==
coef.phila.fitode <- coef(phila_fit)
ci.phila.fitode <- confint(phila_fit)[,-1]
assignfun(coef.phila.fitode, ".phila.fitode")
@

% Confint on derived parameers:
<<sirphilaconfint, include=FALSE>>==
ci.derived.phila.fitode <- confint(phila_fit,
        parm=list(
            Reff~beta*S0/gamma,
            R0~beta*philaS0/gamma,
            Tg~1/gamma
        ))
assignfun(ci.derived.phila.fitode[,1], ".phila.fitode")
philaparms <- as.list(c(coef.phila.fitode, ci.derived.phila.fitode[,1]))
@

<<philaKMparms, include=FALSE>>=
## get KM approx parms associated with fitode fit:
a.phila <- with(philaparms, a_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
omega.phila <- with(philaparms, omega_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
phi.phila <- with(philaparms, phi_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
KM.approx.phila <- c(a=a.phila, omega=omega.phila, phi=phi.phila)
print(KM.approx.phila)
@

<<adjust philaKMparms, include=FALSE>>=
## adjust because above is giving nls problems:
KM.start.phila <- c(a=a.phila, omega=omega.phila, phi=phi.phila)
print(KM.start.phila)
@

<<phila.nlsfit, include=FALSE>>=
## shift time to start at 0 before passing to nls
df <- phila1918b[,c("time","mort")]
df[,"time"] <- df[,"time"] - df[1,"time"]
phila.nlsfit <- nls(mort ~ KM_approx(time, a, omega, phi),
              data = df,
              start = KM.start.phila)
phila.nls.parameters <- coef(phila.nlsfit)
ci.phila.nls <- confint(phila.nlsfit)
assignfun(phila.nls.parameters, ".phila.nls")
print(phila.nls.parameters)
@

<<philanlsR0, include=FALSE>>==
cc.phila.delta <- KMcifun(phila.nlsfit, I0=I0.phila.fitode, N=philaS0)
orig.params.phila <- invert_params(I0=I0.phila.fitode, coef(phila.nlsfit))
orig.params.phila <- signif(orig.params.phila,dig)
assignfun(orig.params.phila, text = ".phila.nls")
R0.phila.nls <- Reff.phila.nls/S0.phila.nls*philaS0
@

\section{To Do}

\subsection*{Things to mention}

application of \code{fitsir} to song downloads, and how we
convinced ourselves that the SIR model was reasonable for that
problem \cite{Rosa+21}.

\paragraph*{notes from Ben.}

pedagogical refs: I would use Raue et al \cite{raue2013lessons} for
sensitivity equations and see how much of the rest is defined in
Bjornstad's book \cite{bjornstadEpidemics2018} (or fall back on mine).

One thing to keep in mind (maybe mention in your article) is that the
Bombay plague outbreak is arguably \emph{not} a good example of an SIR
model (despite the nice historical connection with K\&M
\cite{KermMcKe27}):
\begin{quote}
``So the 1906 epidemic is clearly not a good example of epidemic stopping because the number of susceptible humans has decreased under a
threshold, as suggested by \KM, but an example of
epidemic driven by seasonality.'' \cite{bacaermodel2012}
\end{quote}

\subsection{DJDE's weird idea of introducing na\"ive confidence bands
  via residual errors only}

One standard way to derive a confidence band is the following: We
imagine that the model \eqref{eq:sech} is a perfect representation of
reality, and we consider the deviations from the model curve in
Figure~\ref{fig:Bombay} to be observation errors.  We then imagine
that observation error for each data point is independent and
identically distributed (iid), and drawn from a Normal distribution
with zero mean and standard deviation $\sigma$ equal to the
standard deviation of the residuals (the differences between the model
curve and the observed data).  With this assumption, we could
na\"ively draw confidence bands by adding $\pm1.96\,\sigma$ to our
fitted model curve,\footnote{For a $\normdist(0,\sigma)$
  distribution, 95\% of the probability lies between $-1.96\,\sigma$
  and $+1.96\,\sigma$.}  which would emphasize the folly in assuming
the observation error has the same absolute magnitude throughout the
epidemic (in particular the lower limit would go below zero early and
late in the epidemic).
\bmb{From my point of view this is a little bit weird because it is the
  \emph{opposite} of what is traditionally done in statistics. A \emph{confidence interval}
  on the predictions takes only uncertainty in the parameters into account;
  a \emph{prediction interval} accounts for both parametric uncertainty and
  residual error/observation uncertainty (in traditional/simple approaches, we
  don't consider stochastic processes at all, so parametric uncertainty and
  residual error are the only components of uncertainty). What you're suggesting here,
  taking account only of the residual error, isn't something I've ever seen anyone
  do. That's not to say that we can't proceed this way, but it seems weird. Also,
  opening the can of worms of model misspecification at this point also seems dangerous;
  I'd rather come back to it in a later part of the discussion.}

\subsection*{Model misspecification}

Bear in mind that at each step of this approach, we are making a very
specific, strong assumptions (when we ``imagine'').  To begin with, it
is absurd to suggest that KM's approximation \eqref{eq:sech} to the
solution of the SIR model \eqref{eq:SIR} is a perfect representation
of reality!  The ODEs themselves are far from realistic, not least
because they ignore the discrete nature of individuals and the
stochastic nature of infection and removal events \cite{Bart60,AndeBrit00b}.
Nevertheless, we will continue to focus on estimating uncertainty in
parameter estimates for a given model, since it will be necessary even
if we ignore uncertainty in the specification of the model itself.

\subsection*{DJDE's pseudo-Bayesian introduction to maximum likelihood}

One way to get at this is to use Bayes rule for conditional
probabilities, which tells us that
\begin{linenomath*}
\begin{equation}\label{eq:BayesRule}
\Pop(\text{model} \mid \text{data})
= \dfrac{\Pop(\text{data} \mid \text{model})\cdot\Pop(\text{model})}{\Pop(\text{data})}
\,.
\end{equation}
\end{linenomath*}
The probability of the data is not known to us, but it can't depend on
the values of the parameters of the model that we happen to be
proposing, so we can think of $\Pop(\text{data})$ as a nuisance
constant in equation~\eqref{eq:BayesRule}.  Consequently, if we ignore it,
we still have a function that tells us something about how likely it is
that our model is good.  With this motivation, we define the
\term{likelihood} (not probability) of the model given the data to be
\begin{linenomath*}
\begin{equation}\label{eq:lik.generic}
\lik(\text{model} \mid \text{data})
= \Pop (\text{data} \mid \text{model}) \cdot \Pop(\text{model})\,.
\end{equation}
\end{linenomath*}
i.e.,
\begin{linenomath*}
\begin{equation}\label{eq:likB}
\lik(\thetavec)
= \Pop (\{y_i\} \mid \thetavec) \cdot \Pop(\thetavec)\,.
\end{equation}
\end{linenomath*}
If we have no prior information about the values of the model
parameters then $\Pop(\text{model})$ can be considered another
nuisance constant that we can ignore.  In that situation, maximizing
$\lik$ with respect to $\thetavec$ or, equivalently, minimizing the
negative log-likelihood
\begin{linenomath*}
\begin{equation}\label{eq:negloglikB}
\min_\thetavec\big(\!\!-\log{\lik(\thetavec)}\big) = \min_\thetavec\Big(
\sum_{i=1}^n \big(f(t_i;\thetavec) - y_i\big)^2
\;+\; \text{constant}
\Big)
\end{equation}
\end{linenomath*}
yields an estimate, say $\thetavechat$, that---lo and behold---agrees
exactly with \eqref{eq:leastsquares}, the least squares solution!  The
standard way of expressing this is to say that the least squares
solution $\thetavechat$ is the \term{maximum likelihood estimate} (MLE)
of $\thetavec$, under the assumption of independent,
identically distributed (i.e. mean-zero, constant-$\sigma$)
Normal observation errors in
the time series.

Having introduced the maximum likelihood framework, we can improve
our estimates by making a more realistic assumption about
the error distribution and/or taking advantage of any prior knowledge
we have that might affect our notion of the most likely parameter
values (i.e., \emph{not} ignoring $\Pop(\text{model})$, and instead
specifying it as a probability distribution on the model's
parameters).
\bmb{I don't mind talking about priors if we're actually going to cover them,
  but I feel like it might make more sense to introduce them later, starting as
  a way to regularize/make optimization more robust.}
We will then end up with a different likelihood function
to maximize, and obtain a different $\thetavechat$, but the basic idea
is the same.

So what is a better assumption about the observation error distribution,
and how do we get both better confidence bands on the the model fit
and CIs on the estimated parameter values $\thetavechat$?

(in our previous example, the assumption of normality is allowing the
lower limit of the confidence band on the fitted model to drop below
zero when the number of deaths is small)

\section*{Priors}

Having introduced the idea of maximum likelihood, we can do better
than the na\"ive approach by making a more realistic assumption about
the error distribution and/or taking advantage of any prior knowledge
we have that might affect our notion of the most likely parameter
values (i.e., \emph{not} ignoring $\Pop(\text{model})$, and instead
specifying it as a probability distribution on the model's
parameters).
\bmb{I don't mind talking about priors if we're actually going to cover them,
  but I feel like it might make more sense to introduce them later, starting as
  a way to regularize/make optimization more robust.}
We will then end up with a different likelihood function
to maximize, and obtain a different $\thetavechat$, but the basic idea
is the same.

\section{Discussion}

\begin{itemize}
\item appropriateness of fitting an SIR model to the Bombay plague
  data; Baca\"er's \cite{bacaermodel2012}, opinion, our issues with
  his opinion, potential to fit rat-flea-human model with the data
  that are available.
\item other types of models that should also be easy to deal with with
  fitode, e.g., SEIR or fancier compartmental ODEs
\item what kinds of things would likely cause trouble for fitode?
\item optimization challenges, where to go if you run into trouble
\item statistical challenges, where to go for further insights / more
  advanced approaches
\item fitting stochastic models, pomp
\item fitting ABMs
\item a bit of tribute to Fred
\end{itemize}

\subsection{Comments moved from elsewhere that might be helpful in Discussion}

We chose to fix $I_0$ rather than the (known) population size
($N\simeq10^6$).  \swp{The problem is that we're assuming S0 = 1e6 -
  1. We can't do this.  We need much lower S0 to fit the data. So by
  assuming a very large S0, things go crazy.  So we're better off
  starting with assuming I0 = 1 and assuming the rest. That's the
  whole point of Bacaer's paper: that the required S0 to fit the data
  is so low that the standard SIR model is actually not appropriate
  for modeling the bombay epidemic} \djde{I'll return to this after
  reading Baca\"er \cite{bacaermodel2012}. Meanwhile I will note that
  Gani and Leach \cite{GaniLeac04} found $\R_0\simeq1.3\pm1.8$ and
  $\Tg\simeq6.8\pm2.2$ days, for ``modern'' pneumonic plague, and
  based on that $\Tg$ we \cite{Earn+20} found similar $\R_0$ for 17th
  century plague in London.  Baca\"er \cite{bacaermodel2012} seems to
  assume (like \KM) that plague in Bombay had a CFP of near 1.
  Anyway, the point of the discussion here will be to alert readers to
  be cautious for all sorts of reasons, and a good fit of a model to
  data does not necessarily mean that it is fair to make inferences
  from estimated parameter values.}  \djde{Note KM's caution about
  interpreting the param values.}

Alternatives to the \hyperlink{DeltaMethod}{Delta Method}.
\djde{Available in \code{fitode} but we not what we doing here:
  Another possibility would be to use the estimated means and variances
  for $\thetavec$ to define a multivariate Normal, sample that many
  times and run the model, and draw the bands that contain 95\% of the
  simulations.}
  \swp{This is not what we actually do in fitode. We do something called
  importance sampling where we draw from multivariate normal and then
  do some weighting... probably a bit too much to explain in this paper
  because we have to go all the way to metropolis hastings..}

\swp{we might want to mention that profile likelihoods are a thing but are
probably too complicated for this paper and especially for trajectories?}
\djde{mentioning profile likelihoods in passing, with a reference,
  would be good; we could add that here or in the Discussion.}




\bibliographystyle{tfs}
\bibliography{brauer-ms,fitode}

<<figfuns, echo=FALSE>>=
lwd <- 5
col.KM <- my.blue
col.nls <- my.red
col.fitode.nb <- my.yellow
col.fitode.ols <- my.green
transparent_colour <- function(col,alpha=150/255) {
    alpha <- round(alpha * 255)
    v <- col2rgb(col)[,1] # color as rgb vector
    tcol <- rgb(v["red"],v["green"],v["blue"],alpha=alpha,maxColorValue = 255)
    return(tcol)
}
col.fitodenbCI <- transparent_colour(col.fitode.nb, alpha=0.4)
col.fitodeCI <- transparent_colour(col.fitode.ols, alpha=0.4)
col.nlsCI <- transparent_colour(col.nls, alpha=0.4)
setup_plot <- function(xlab="", ylab="deaths",
                       at=100*(0:10), ...) {
    plot(NA, NA , bty="L", type="n",
         xaxs="i", yaxs="i", las=1,
         yaxt="n",
         xlab=xlab, ylab=ylab,
         ...
         )
    axis(side=2, at=at, las=1)
}
draw_confband <- function(confband, col = col.fitodenbCI) {
    with(confband,{
        polygon(
            x = c(times, rev(times)),
            y = c(`2.5 %`, rev(`97.5 %`)),
            col = col,
            border = NA,
            xpd = NA
        )
    })
}
draw_legend <- function(lwd=5, pt.bg="white") {
    legend("topleft", bty="n", lwd=c(2,lwd,lwd,lwd*0.60),
       lty=c(NA,"solid","solid","solid"),
       col=c("black",col.KM,col.nls,col.fitode.nb),
       pch=c(21,NA,NA,NA),
       pt.bg=c(pt.bg,NA,NA,NA),
       legend=c("observed data","KM","nls","fitode (nbinom)"))
}
@

<<testing, include=FALSE, results = "hide">>=
format_sn0(cc["a", 1])
ci_fmt("a", cc)
@

\begin{landscape}
\begin{table}
  \begin{center}
    \caption{Parameters of \KM's \cite{KermMcKe27} approximation
      \eqref{eq:sech} to the SIR model \eqref{eq:SIR}, as estimated by
      them \cite{KermMcKe27} and by us using nonlinear least squares
      (\code{nls}).
      The $\R_0$ estimates assume the population of Bombay was
      $N=1$ million \cite[p.\,408]{bacaermodel2012}.
      \djde{Maybe we should list $N$ as a ``specified parameter'' so it
      is very explicit, rather than just burying it in $\R_0$.
      We could then list $\beta N$ rather than $\beta$, which would
      tidy the table and avoid an implicit factor of $1/N$ in $\beta$.}
      \bmb{that seems useful, but the slightly weird thing is that the interpretation of $\beta$ can get tricky if the FOI is scaled as $\beta I/N$ but $S(0) < N$ \ldots}
     %%  (including 95\% confidence intervals).
     %%  Given values of the estimated parameters
     %% $(a,\omega,\phi)$, we specify $I_0$ and derive implied values of the
     %%  original parameters using equation~\eqref{eq:invertparams}.
     %%
     %% The horizontal line separates parameters that are directly
     %% estimated from those that are derived from the estimated
     %% parameters.
}\label{tab:Bombay}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c | c | c c}
  \bfseries Assumed\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries equation} & {\footnotesize\bfseries units}
    & \bfseries KM & \bfseries \code{nls} & \bfseries 95\% CI \\\hline
    initial prevalence & $I_0$ & -- & -- &
      \Sexpr{I0.KM} & \Sexpr{I0.nls} & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Estimated\break parameter \\\hline
    peak removal rate & $a$& \eqref{eq:a} & $\frac{1}{\textrm{weeks}}$ & \Sexpr{a.KM} &
      \Sexpr{signif(a.nls,dig)} & \Sexpr{ci_fmt("a", cc)} \\
    outbreak speed & $\omega$ & \eqref{eq:omega} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{omega.KM} & \Sexpr{signif(omega.nls,dig-1)} & \Sexpr{ci_fmt("omega", cc)} \\
    outbreak centre & $\phi$ & \eqref{eq:phi} & -- & \Sexpr{phi.KM} &
      \Sexpr{signif(phi.nls,dig)} & \Sexpr{ci_fmt("phi", cc)} \\
    \noalign{\vspace{10pt}}
    \bfseries Derived\break parameter \\\hline
    peak time & $\tpeak$ & \eqref{eq:tpeak} & weeks &
      \Sexpr{tpeak.KM} & \Sexpr{tpeak.nls} & \Sexpr{ci_fmt("tpeak", cc.delta)} \\
    effective reproduction number & $\Reff$
      & \eqref{eq:Reffdef}, \eqref{eq:Reff} & -- &
      \Sexpr{Reff.KM} & \Sexpr{Reff.nls} & \Sexpr{ci_fmt("Reff", cc.delta)} \\
    removal rate & $\gamma$ & \eqref{eq:gamma} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{gamma.KM} & \Sexpr{gamma.nls} & \Sexpr{ci_fmt("gamma", cc.delta)} \\
    initial\break susceptibles & $S_0$ & \eqref{eq:S0} & -- &
      \Sexpr{as.integer(S0.KM)} & \Sexpr{as.integer(S0.nls)} & \Sexpr{ci_fmt("S0", cc.delta, max_ord = 6)} \\
    transmission rate & $\beta$ & \eqref{eq:beta} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{beta.KM} & \Sexpr{beta.nls} & \Sexpr{ci_fmt("beta", cc.delta)} \\
    mean generation interval & $\Tg$ & \eqref{eq:Tg} & days &
      \Sexpr{signif(7*Tg.KM,dig)} & \Sexpr{signif(7*Tg.nls,dig)} & \Sexpr{ci_fmt("Tg", 7*cc.delta)} \\
    basic reproduction number & $\R_0$ & \eqref{eq:R0def} & -- &
      \Sexpr{signif(10^6*Reff.KM/S0.KM,dig)}
      & \Sexpr{signif(10^6*Reff.nls/S0.nls,dig)} & \Sexpr{ci_fmt("R0", cc.delta)}
  \end{tabular}
  \end{center}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}
  \begin{center}
    \caption{Parameter estimates from \code{fitode} fits of the SIR
      model \eqref{eq:SIR} to the Bombay plague data (see
      Figure~\ref{fig:Bombay}).  The $\R_0$ estimates assume the
      population of Bombay was $N=1$ million
      \cite[p.\,408]{bacaermodel2012}.}\label{tab:bombay.fitode}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c c | c c}
  \bfseries Assumed\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries units}
    & \bfseries \code{dnbinom} & \bfseries 95\% CI & \bfseries \code{ols} & \bfseries 95\% CI \\\hline
    recovery rate & $\gamma$ & $\frac{1}{\textrm{weeks}}$ & \Sexpr{gamma.nls} &
      -- & \Sexpr{gamma.nls} & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Estimated\break parameter \\\hline
    transmission rate & $\beta$ & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{beta.bombay.fitode.nb} & \Sexpr{ci_fmt("beta", ci.bombay.fitode.nb)}  & \Sexpr{beta.bombay.fitode.ols} & ?? \\
    initial\break susceptibles & $S_0$& -- &
      \Sexpr{as.integer(S0.bombay.fitode.nb)} & \Sexpr{ci_fmt("S0", ci.bombay.fitode.nb, max_ord = 6)} & \Sexpr{as.integer(S0.bombay.fitode.ols)} & ?? \\
      initial prevalence & $I_0$ & -- &
      \Sexpr{I0.bombay.fitode.nb} & \Sexpr{ci_fmt("I0", ci.bombay.fitode.nb)} & \Sexpr{I0.bombay.fitode.ols} & ?? \\
      overdispersion\break parameter & $k$ & -- &
      \Sexpr{k.bombay.fitode.nb} & \Sexpr{ci_fmt("k", ci.bombay.fitode.nb)} & -- & -- \\
      \noalign{\vspace{10pt}}
      \bfseries Derived\break parameter \\\hline
      effective reproduction number & $\Reff$ & -- &
      \Sexpr{Reff.bombay.fitode.nb} &  \Sexpr{ci_fmt("Reff", ci.derived.bombay.fitode.nb[,-1])} & -- & -- \\
      mean generation interval & $\Tg$ & days &
      \Sexpr{signif(7*Tg.bombay.fitode.nb,dig)} & \Sexpr{ci_fmt("Tg", 7*ci.derived.bombay.fitode.nb[,-1])} & -- & -- \\
    basic reproduction number & $\R_0$ & -- &
      \Sexpr{signif(R0.bombay.fitode.nb,dig)}
      &  \Sexpr{ci_fmt("R0", ci.derived.bombay.fitode.nb[,-1])} & -- & --
  \end{tabular}
  \end{center}
\end{table}
\end{landscape}

\begin{figure}
\centering
<<Bombay-figure, echo=FALSE, fig.height=5, fig.show="hold", out.width="95%">>=
tvals <- seq(0, tmax, length.out=1000)
setup_plot(ylim = c(0, 1000), xlim=c(0,tmax),
         xlab="weeks", ylab="plague deaths")
draw_confband(traj_ci(tvals, nls.parameters, vcov(nlsfit)), col=col.nlsCI)
##DE: skipping fitode confband because there is too much overlap:
##draw_confband(SIR_confband, col=col.fitodenbCI)
curve(KM_approx(x, a=a.KM, omega=omega.KM, phi=phi.KM), from=0, to=tmax, n=1000, add=TRUE,
      lwd=lwd, xpd=NA, col=col.KM)
curve(KM_approx(x, a=a.nls, omega=omega.nls, phi=phi.nls), from=0, to=tmax, n=1000, add=TRUE,
      lwd=lwd, xpd=NA, col=col.nls)
lines(estimate ~ times, data=SIR_confband, col=col.fitode.nb, lwd=lwd*0.60, lty="dotted")
points(mort ~ week, data = fitode::bombay, xpd=NA, pch=21, bg="white", lwd=2)
draw_legend()

setup_plot(ylim=c(0, 1000), xlim=c(0,tmax), #DE: ymin was 5, not sure why
         xlab="weeks", ylab="plague deaths"
         ##, at=c(25, 50, 100, 200, 400, 800)
         )
draw_confband(SIR_confband, col=col.fitodenbCI)
lines(estimate ~ times, data=SIR_confband, col=col.fitode.nb, lwd=lwd)
curve(KM_approx(x, a=a.nls, omega=omega.nls, phi=phi.nls), from=0, to=tmax, n=250, add=TRUE,
      lwd=lwd*0.60, xpd=NA, col=col.nls, lty="dotted")
## lines(estimate ~ times, data=SIR_ols_confband, col=col.fitode.ols, lwd=lwd*0.60, lty="dotted")
points(mort ~ week, data = fitode::bombay, xpd=NA, pch=21, bg="white", lwd=2)
@
\caption{The plague epidemic in Bombay, 17 December 1905 to 21 July
  1906, used as an example by KM \cite[p.\,714]{KermMcKe27}.  The data
  (dots) were digitized from \cite[Table~IX, p.\,753]{jhyg1907}.
  \emph{Top panel:} The blue and red curves show the \KM approximation
  \eqref{eq:sech}, as fitted by \KM (blue) and by us using \code{nls}
  (red, with pink confidence band).  The associated parameter
  estimates are given in Table~\ref{tab:Bombay}.  The dotted yellow
  curve shows the \code{fitode} fit, for which the associated
  parameter estimates are given Table~\ref{tab:bombay.fitode}.
  \emph{Bottom panel:} The yellow curve is identical to the yellow
  dotted curve in the top panel, and the yellow band shows the
  \code{fitode} confidence band.  The dotted red curve is identical to
  the solid red curve in the top panel.  We have separated the two
  panels because the confidence band overlap would make the plots
  difficult to interpret.\djde{We need a legend in the bottom panel.}
  \djde{Ben's comment in the caption to Figure~\ref{fig:stoch} applies
    here too.  We need to explain why the \code{fitode} estimate and
    confidence band is jagged, or make it smooth (which I realize is
    easy for the estimate and a can of worms for the confidence
    band).}  }
\label{fig:Bombay}
\end{figure}


\begin{landscape}
\begin{table}
  \begin{center}
    \caption{Parameters of \KM's \cite{KermMcKe27} approximation
      \eqref{eq:sech} to the SIR model \eqref{eq:SIR}, as estimated by
      us using original parameters that were used to simulate the stochastic SIR model and using nonlinear least squares
      (\code{nls}).
}\label{tab:stoch.nls}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c | c | c c}
  \bfseries Assumed\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries equation} & {\footnotesize\bfseries units}
    & \bfseries KM & \bfseries \code{nls} & \bfseries 95\% CI \\\hline
    initial prevalence & $I_0$ & -- & -- &
      \Sexpr{I0.KM} & \Sexpr{I0.nls} & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Estimated\break parameter \\\hline
    peak removal rate & $a$& \eqref{eq:a} & $\frac{1}{\textrm{weeks}}$ & \Sexpr{a.det} &
      \Sexpr{signif(a.nls.stoch,dig)} & \Sexpr{ci_fmt("a", cc.nls.stoch)} \\
    outbreak speed & $\omega$ & \eqref{eq:omega} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{omega.det} & \Sexpr{signif(omega.nls.stoch,dig-1)} & \Sexpr{ci_fmt("omega", cc.nls.stoch)} \\
    outbreak centre & $\phi$ & \eqref{eq:phi} & -- & \Sexpr{phi.det} &
      \Sexpr{signif(phi.nls.stoch,dig)} & \Sexpr{ci_fmt("phi", cc.nls.stoch)} \\
    \noalign{\vspace{10pt}}
    \bfseries Derived\break parameter \\\hline
    peak time & $\tpeak$ & \eqref{eq:tpeak} & weeks &
      \Sexpr{tpeak.KM.stoch} & \Sexpr{tpeak.nls.stoch} & \Sexpr{ci_fmt("tpeak", cc.delta.stoch)} \\
    effective reproduction number & $\Reff$
      & \eqref{eq:Reffdef}, \eqref{eq:Reff} & -- &
      \Sexpr{Reff.KM.stoch} & \Sexpr{Reff.nls.stoch} & \Sexpr{ci_fmt("Reff", cc.delta.stoch)} \\
    removal rate & $\gamma$ & \eqref{eq:gamma} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{gamma.KM.stoch} & \Sexpr{gamma.nls.stoch} & \Sexpr{ci_fmt("gamma", cc.delta.stoch)} \\
    initial\break susceptibles & $S_0$ & \eqref{eq:S0} & -- &
      \Sexpr{as.integer(S0.KM.stoch)} & \Sexpr{as.integer(S0.nls.stoch)} & \Sexpr{ci_fmt("S0", cc.delta.stoch, max_ord = 6)} \\
    transmission rate & $\beta$ & \eqref{eq:beta} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{beta.KM.stoch} & \Sexpr{beta.nls.stoch} & \Sexpr{ci_fmt("beta", cc.delta.stoch)} \\
    mean generation interval & $\Tg$ & \eqref{eq:Tg} & days &
      \Sexpr{signif(7*Tg.KM.stoch,dig)} & \Sexpr{signif(7*Tg.nls.stoch,dig)} & \Sexpr{ci_fmt("Tg", 7*cc.delta.stoch)} \\
    basic reproduction number & $\R_0$ & \eqref{eq:R0def} & -- &
      \Sexpr{signif(2000*Reff.KM.stoch/S0.KM,dig)}
      & \Sexpr{signif(2000*Reff.nls.stoch/S0.nls.stoch,dig)} & \Sexpr{ci_fmt("R0", cc.delta.stoch)}
  \end{tabular}
  \end{center}
\end{table}
\end{landscape}

\begin{landscape}
\begin{table}
  \begin{center}
    \caption{Parameter estimates from \code{fitode} fits of the SIR
      model \eqref{eq:SIR} to a stochastic SIR simulation (see
      Figure~\ref{fig:stoch}).}\label{tab:stoch.fitode}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c c}
  \bfseries Assumed\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries units}
    & \bfseries \code{dnbinom} & \bfseries 95\% CI \\\hline
    recovery rate & $\gamma$ & $\frac{1}{\textrm{weeks}}$ & 1 &
      -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Estimated\break parameter \\\hline
    transmission rate & $\beta$ & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{beta.stoch.fitode} & \Sexpr{ci_fmt("beta", ci.stoch.fitode)} \\
    initial\break susceptibles & $S_0$& -- &
      \Sexpr{as.integer(S0.stoch.fitode)} & \Sexpr{ci_fmt("S0", ci.stoch.fitode, max_ord = 6)}\\
      initial prevalence & $I_0$ & -- &
      \Sexpr{I0.stoch.fitode} & \Sexpr{ci_fmt("I0", ci.stoch.fitode)}\\
      overdispersion\break parameter & $k$ & -- &
      \Sexpr{k.stoch.fitode} & \Sexpr{ci_fmt("k", ci.stoch.fitode)}\\
      \noalign{\vspace{10pt}}
      \bfseries Derived\break parameter \\\hline
      effective reproduction number & $\Reff$ & -- &
      \Sexpr{Reff.stoch.fitode} &  \Sexpr{ci_fmt("Reff", ci.derived.stoch.fitode[,-1])} \\
    basic reproduction number & $\R_0$ & -- &
      \Sexpr{signif(R0.stoch.fitode,dig)}
      &  \Sexpr{ci_fmt("R0", ci.derived.stoch.fitode[,-1])}
  \end{tabular}
  \end{center}
\end{table}
\end{landscape}

\begin{figure}
<<plot_stochastic_SIR, echo=FALSE, fig.height=5, fig.show="hold", out.width="95%">>=
plot(NA, NA, bty="L", xlab="", ylab="", las=1, xaxs="i", yaxs="i",
     xlim=range(tt), ylim=range(sir.stoch.data$mort, na.rm=TRUE))
draw_confband(sss, col=col.fitodenbCI)
lines(sir.det$tau, 1 * sir.det$I * N/7, col=my.green, lwd=lwd) ## dR/dt
curve(KM_approx(x, a=a.det, omega=omega.det, phi=phi.det)/7, from=0, to=tmax, n=1000, add=TRUE,
      xpd=NA, col=my.blue, lwd=lwd)
# lines(estimate ~ times, data=sss, col=col.fitode.nb, lwd=lwd)
points(mort ~ time, data = sir.stoch.data, xpd=NA, pch=21, bg="white", lwd=2)
legend("topright", bty="n", lwd=c(2,lwd,lwd),
       lty=c(NA,"solid","solid"),
       col=c("black",col.KM,my.green),
       pch=c(21,NA,NA),
       pt.bg=c("white",NA,NA),
       legend=c("observed data","KM","deterministic"))

plot(NA, NA, bty="L", xlab="", ylab="", las=1, xaxs="i", yaxs="i",
     xlim=range(tt), ylim=range(sir.stoch.data$mort, na.rm=TRUE))
draw_confband(traj_ci(tvals, nls.stoch.parameters, vcov(nlsfit.stoch)), col=col.nlsCI)
draw_confband(sss, col=col.fitodenbCI)
curve(KM_approx(x, a=a.nls.stoch, omega=omega.nls.stoch, phi=phi.nls.stoch), from=0, to=tmax, n=1000, add=TRUE,
      xpd=NA, col=col.nls, lwd=lwd, lty="dashed")
lines(estimate ~ times, data=sss, col=col.fitode.nb, lwd=lwd)
points(mort ~ time, data = sir.stoch.data, xpd=NA, pch=21, bg="white", lwd=2)
legend("topright", bty="n", lwd=c(2,lwd,lwd),
       lty=c(NA,"dashed","solid"),
       col=c("black",col.nls,col.fitode.nb),
       pch=c(21,NA,NA),
       pt.bg=c("white",NA,NA),
       legend=c("observed data","KM (nls)","fitode (nbinom)"))
@
\caption{ Deterministic fits to a stochastic SIR simulation.
\emph{Top panel:} The blue curve shows the \KM approximation
  \eqref{eq:sech} calculated by by converting the original
  parameters that were used to simulate the SIR model.
    The green curve shows the deterministic trajectory
    of the same SIR model.
    \emph{Bottom panel:} The red curve shows the \KM approximation
  \eqref{eq:sech}, fitted using \code{nls}.
    The yellow curve shows the SIR model fit using \code{fitode}.
  \bmb{These are better than they were, but \ldots although I
    understand that fitode will give a sharp set of predictions (1)
    we'll need to explain that (I've thought about making a sensible
    smooth prediction curve for incidence, but it's a can of worms);
    (2) why do the deterministic and KM curves look
    time-shifted?}\djde{The time shift is stochastic time to tipping
    point -- it's just one realization and happens to take off}}
\label{fig:stoch}
\end{figure}


\begin{landscape}
\begin{table}
  \begin{center}
    \caption{Parameter estimates for the 1918 influenza epidemic in
      Philadelphia, based on \KM's \cite{KermMcKe27} approximation
      \eqref{eq:sech} to the SIR model \eqref{eq:SIR}, using nonlinear
      least squares (\code{nls}).  The $\R_0$ estimates assume the
      population of Philadelphia in 1918 was\dots
    See Figure~\ref{fig:phila}.}\label{tab:philanls}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c | c c}
    \bfseries Assumed\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries equation} & {\footnotesize\bfseries units}
    & \bfseries \code{nls} & \bfseries 95\% CI \\\hline
    initial prevalence & $I_0$ & -- & -- &
      \Sexpr{I0.phila.fitode} & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Estimated\break parameter \\\hline
    peak removal rate & $a$& \eqref{eq:a} & $\frac{1}{\textrm{years}}$ &
      \Sexpr{signif(a.phila.nls,dig)} & \Sexpr{ci_fmt("a", ci.phila.nls)} \\
    outbreak speed & $\omega$ & \eqref{eq:omega} & $\frac{1}{\textrm{years}}$ &
      \Sexpr{signif(omega.phila.nls,dig-1)} & \Sexpr{ci_fmt("omega", ci.phila.nls)} \\
    outbreak centre & $\phi$ & \eqref{eq:phi} & -- &
      \Sexpr{signif(phi.phila.nls,dig)} & \Sexpr{ci_fmt("phi", ci.phila.nls)} \\
    \noalign{\vspace{10pt}}
    \bfseries Derived\break parameter \\\hline
    peak time & $\tpeak$ & \eqref{eq:tpeak} & years &
      \Sexpr{tpeak.phila.nls} & \Sexpr{ci_fmt("tpeak", cc.phila.delta)} \\
    effective reproduction number & $\Reff$
      & \eqref{eq:Reffdef}, \eqref{eq:Reff} & -- &
      \Sexpr{Reff.phila.nls} & \Sexpr{ci_fmt("Reff", cc.phila.delta)} \\
    removal rate & $\gamma$ & \eqref{eq:gamma} & $\frac{1}{\textrm{years}}$ &
      \Sexpr{gamma.phila.nls} & \Sexpr{ci_fmt("gamma", cc.phila.delta)} \\
    initial\break susceptibles & $S_0$ & \eqref{eq:S0} & -- &
       \Sexpr{as.integer(S0.phila.nls)} & \Sexpr{ci_fmt("S0", cc.phila.delta, max_ord = 6)} \\
    transmission rate & $\beta$ & \eqref{eq:beta} & $\frac{1}{\textrm{years}}$ &
       \Sexpr{beta.phila.nls} & \Sexpr{ci_fmt("beta", cc.phila.delta)} \\
    mean generation interval & $\Tg$ & \eqref{eq:Tg} & years &
      \Sexpr{signif(Tg.phila.nls,dig)} & \Sexpr{ci_fmt("Tg", cc.phila.delta)} \\
    basic reproduction number & $\R_0$ & \eqref{eq:R0def} & -- &
       \Sexpr{signif(R0.phila.nls,dig)} & \Sexpr{ci_fmt("R0", cc.phila.delta)}
  \end{tabular}
  \end{center}
\end{table}
\end{landscape}


\begin{landscape}
\begin{table}
  \begin{center}
    \caption{Parameter estimates from \code{fitode} fits
      of the SIR model \eqref{eq:SIR} to reported
      pneumonia and influenza mortality during the main
      wave of the 1918 influenza pandemic in the city
      of Philadelphia.}\label{tab:phila.fitode}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c c}
  \bfseries Estimated\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries units}
    & \bfseries \code{dnbinom} & \bfseries 95\% CI \\\hline
    transmission rate & $\beta$ & $\frac{1}{\textrm{years}}$ &
      \Sexpr{beta.phila.fitode} & \Sexpr{ci_fmt("beta", ci.phila.fitode)} \\
      recovery rate & $\gamma$ & $\frac{1}{\textrm{years}}$ & \Sexpr{gamma.phila.fitode} &
      -- \\
    initial\break susceptibles & $S_0$& -- &
      \Sexpr{as.integer(S0.phila.fitode)} & \Sexpr{ci_fmt("S0", ci.phila.fitode, max_ord = 6)}\\
      initial prevalence & $I_0$ & -- &
      \Sexpr{I0.phila.fitode} & \Sexpr{ci_fmt("I0", ci.phila.fitode)}\\
      overdispersion\break parameter & $k$ & -- &
      \Sexpr{k.phila.fitode} & \Sexpr{ci_fmt("k", ci.phila.fitode)}\\
      \noalign{\vspace{10pt}}
      \bfseries Derived\break parameter \\\hline
      effective reproduction number & $\Reff$ & -- &
      \Sexpr{Reff.phila.fitode} &  \Sexpr{ci_fmt("Reff", ci.derived.phila.fitode[,-1])} \\
      mean generation interval & $\Tg$ & years &
      \Sexpr{signif(7*Tg.phila.fitode,dig)} & \Sexpr{ci_fmt("Tg", 7*ci.derived.phila.fitode[,-1])} \\
    basic reproduction number & $\R_0$ & -- &
      \Sexpr{signif(R0.phila.fitode,dig)}
      &  \Sexpr{ci_fmt("R0", ci.derived.phila.fitode[,-1])}
  \end{tabular}
  \end{center}
\end{table}
\end{landscape}


\begin{figure}
<<phila-figure, echo=FALSE, fig.height=5>>=
setup_plot(ylim = c(0, 800), xlim = range(phila1918b$time), ylab="daily P&I deaths")
draw_confband(ppp)
tvals <- with(phila1918b, seq(min(time),max(time), length=1000))
t0 <- min(tvals)
curve(KM_approx(x-t0, a=a.phila, omega=omega.phila, phi=phi.phila)/365, from=t0, to=max(tvals), n=1000, add=TRUE,
      xpd=NA, col=my.blue, lwd=lwd)
lines(tvals,
      with(as.list(phila.nls.parameters),KM_approx(tvals-t0, a=a, omega=omega, phi=phi)),
      xpd=NA, lwd=lwd, col=col.nls)
lines(estimate ~ times, data=ppp, col=col.fitode.nb, lwd=lwd*0.60, lty="dotted")
points(mort ~ time,
       data = phila1918b, xpd=NA, pch=21, bg="white", lwd=2)
draw_legend()
@
\caption{The main wave of the 1918 influenza epidemic in the city of
  Philadelphia, 1 September 1918 to 31 December 1918 \cite{Gold+09}.
  \djde{If we include this figure, we'll need another table
    similar to the Bombay plague table (Table~\ref{tab:Bombay}).}
}
\label{fig:phila}
\end{figure}

\end{document}
