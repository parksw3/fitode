% interacttfssample.tex
% v1.05 - August 2017

\documentclass[]{interact}

% https://tex.stackexchange.com/questions/2441/how-to-add-a-forced-line-break-inside-a-table-cell
\usepackage{makecell}
\usepackage{epstopdf}% To incorporate .eps illustrations using PDFLaTeX, etc.
\usepackage[caption=false]{subfig}% Support for small, `sub' figures and tables
%\usepackage[nolists,tablesfirst]{endfloat}% To `separate' figures and tables from text if required

%\usepackage[doublespacing]{setspace}% To produce a `double spaced' document if required
%\setlength\parindent{24pt}% To increase paragraph indentation when line spacing is doubled
%\setlength\bibindent{2em}% To increase hanging indent in bibliography when line spacing is doubled

\usepackage[numbers,sort&compress]{natbib}% Citation support using natbib.sty
\bibpunct[, ]{[}{]}{,}{n}{,}{,}% Citation support using natbib.sty
\renewcommand\bibfont{\fontsize{10}{12}\selectfont}% Bibliography support using natbib.sty

\theoremstyle{plain}% Theorem-like structures provided by amsthm.sty
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}

\theoremstyle{remark}
\newtheorem{remark}{Remark}
\newtheorem{notation}{Notation}

%%%%%%%%%%%%%%
%% CLEVEREF %%
%%%%%%%%%%%%%%
%% order matters: hyperref before cleverref, and biblatex last
\usepackage{hyperref}
%% cleveref package for convenient hyper-referencing/citing:
\usepackage[nameinlink,capitalize]{cleveref}
%% The default cref format includes a space before the number, e.g.,
%% "ยง 3" rather than "ยง3", so when using ยง it looks better to
%% define the cref format explicitly:
%% https://tex.stackexchange.com/questions/247538/remove-space-in-the-default-cref-command
\crefformat{section}{\S#2#1#3}
%% A space is still included when multiple sections are referenced,
%% but I think removing the space for multiple section refs would look
%% worse.
\crefname{section}{\S}{\S\S}
\Crefname{section}{Section}{Sections}
\crefname{equation}{Equation}{Equations}
\Crefname{equation}{Equation}{Equations}
\crefname{figure}{Figure}{Figures}
\Crefname{figure}{Figure}{Figures}
\Crefname{appsec}{Appendix}{appendices}

%% other DE preamable
\usepackage{pdflscape} % landscape mode that rotates page (unlike lscape)
\usepackage{xcolor}
\usepackage{lineno}\renewcommand\thelinenumber{\color{gray}\arabic{linenumber}}
\usepackage{placeins}
\newcommand{\thickredline}{{\color{red}\bigskip\begin{center}\linethickness{2mm}\line(1,0){250}\end{center}\bigskip}}
\newcommand{\bmb}[1]{{\color{red}$\langle$\emph{BMB: #1}$\rangle$}}
\newcommand{\swp}[1]{{\color{blue}$\langle$\emph{SWP: #1}$\rangle$}}
\newcommand{\djde}[1]{{\color{magenta}$\langle$\emph{DJDE: #1}$\rangle$}}
%%\newcommand{\djde}[1]{\relax}
\newcommand{\needref}{{\color{orange}[NEED REF]}}
\newcommand{\term}[1]{{\bfseries\slshape#1}}

%% derivatives
%% note that \d is a built-in accent macro
\newcommand{\dee}{{\rm d}}
\newcommand{\dd}[2]{{\frac{\dee{#1}}{\dee{#2}}}}
\newcommand{\dbyd}[2]{{{\dee{#1}}/{\dee{#2}}}}
\newcommand{\ddx}[1]{\dd{#1}{x}}
\newcommand{\ddt}[1]{\dd{#1}{t}}
\newcommand{\ddtau}[1]{\dd{#1}{\tau}}
\newcommand{\dbydx}[1]{\dbyd{#1}{x}}
\newcommand{\dbydt}[1]{\dbyd{#1}{t}}
\newcommand{\dbydtau}[1]{\dbyd{#1}{\tau}}
\newcommand{\dt}{\dee t}

\newcommand{\sech}{\,\textrm{sech}}
\newcommand{\Ipeak}{I_{\rm p}}
\newcommand{\tpeak}{t_{\rm p}}
\newcommand{\R}{{\mathcal R}}
\newcommand{\Tg}{T_{\rm g}}
\newcommand{\Reff}{\R_{\rm e}}
\usepackage{xspace}
\newcommand{\KM}{KM\xspace}
\usepackage{bm} % bold math
\newcommand{\fvec}{{\bm{f}}}
\newcommand{\Phivec}{{\bm{\Phi}}}
\newcommand{\xvec}{{\bm{x}}}
\newcommand{\thetavec}{{\bm{\theta}}}
\newcommand{\thetavechat}{{\bm{\hat\theta}}}
%%\newcommand{\gradtheta}{\nabla_{\!\!\thetavec}}
\newcommand{\gradtheta}{\nabla_{{}_{\!\!\thetavec}}}
%%\newcommand{\gradx}{\nabla_{\!\!\xvec}}
\newcommand{\gradx}{\nabla_{{}_{\!\!\xvec}}}
\newcommand{\code}[1]{\texttt{\detokenize{#1}}}
\usepackage{xspace}
\newcommand{\KMcol}{blue\xspace}
\newcommand{\nlscol}{red\xspace}
\newcommand{\fitodecol}{yellow\xspace}
\newcommand{\Pop}{{\mathbb{P}}} % probability operator
\newcommand{\lik}{{\mathcal L}}
\newcommand{\transpose}{{\textsf{T}}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\normdist}{\ensuremath\mathrm{Normal}}
%% tables
\usepackage{multirow}   % for tables
%\usepackage{subfig}
\usepackage{tabularx}   % pretty tables
% \usepackage{tabularray} % allows rowsep via tblr environment
%% table details
\renewcommand{\arraystretch}{1.5} % more interline spacing
%% https://tex.stackexchange.com/questions/12703/how-to-create-fixed-width-table-columns-with-text-raggedright-centered-raggedlef
\usepackage{ragged2e}
%%\usepackage{array} % for m{...}
\newcommand\ttbackslash{{\tt\char`\\}}
\newcommand{\macro}[1]{{\tt\ttbackslash#1}}
\newcommand{\traj}{x}
\newcommand{\ie}{i.e., }
\newcommand{\sens}{{\mathsf S}}
\newcommand{\sensmat}{{\bm{\sens}}}
\newcommand{\tindex}{{\ell}}
\newcommand{\ti}{t_\tindex}
\newcommand{\timo}{t_{\tindex-1}}
\newcommand{\tindexmax}{{n_t}}
\newcommand{\xindex}{{i}}
\newcommand{\xindexmax}{{n_{{}_x}}}
\newcommand{\thetaindex}{{j}}
\newcommand{\thetaindexmax}{{n_{{}_\theta}}}
\newcommand{\colfitodeols}{grey\xspace} % orange
%%initial state:
\newcommand{\Rinit}{R_{\rm i}}

<<setup, echo=FALSE, include=FALSE>>=
## set default base-graphics plotting options
knitr::knit_hooks$set(basefig=function(before, options, envir) {
                   if (before) {
                       oldpar <- par(bty="l",las=1)
                       on.exit(par(oldpar))
                   } else { }
})
knitr::opts_chunk$set(error = FALSE)
## B&W format: https://stackoverflow.com/questions/22666475/how-to-achieve-blackwhite-friendly-syntax-highlighting-in-pdfs-produced-with-kn
##knitr::opts_knit$set( out.format="latex" )
##knitr::knit_theme$set("print")
## override BMB's finicky settings
options(warnPartialMatchArgs = FALSE,
          warnPartialMatchAttr = FALSE,
          warnPartialMatchDollar = FALSE)

@
\begin{document}

\articletype{ORIGINAL ARTICLE}% Specify the article type or omit as appropriate
\articletype{Article type: \ EDUCATION}

\title{Fitting epidemic models to data -- a tutorial\\in memory of Fred
  Brauer}

\author{
  \name{David J.\,D.~Earn\textsuperscript{a}\thanks{%%CONTACT
    D.J.D.~Earn. \href{mailto:earn@math.mcmaster.ca}{earn@math.mcmaster.ca}. ORCID: 0000-0002-7562-1341}, %%%Twitter: @David.J.D.Earn},
    Sang Woo Park\textsuperscript{b}\thanks{S.W.~Park. \href{mailto:swp2@princeton.edu}{swp2@princeton.edu}. ORCID 0000-0003-2202-3361,} %%% Mastodon: @sangwoopark@ecoevo.social},
    and Benjamin M.~Bolker\textsuperscript{a,c}\thanks{B.M.~Bolker. \href{mailto:bolkerb@mcmaster.ca}{bolkerb@mcmaster.ca}. ORCID: 0000-0002-2127-0443}, %%%Twitter: @bolkerb, Mastodon: @bbolker@fediscience.org}}
  }
  \affil{\textsuperscript{a}Department of Mathematics and Statistics, McMaster University,\\\ Hamilton, Ontario, Canada, L8S 4K1;\\\medskip
    \textsuperscript{b}Department of Ecology and Evolutionary Biology, Princeton University,\\\ Princeton, NJ 08544;\\\medskip
    \textsuperscript{c}Department of Biology, McMaster University,\\\ Hamilton, Ontario, Canada, L8S 4K1}
}

\maketitle

\linenumbers

\bigskip \bigskip

\djde{Notation: $S_{\rm i}$ etc would be better than $S_0$ to further
  reduce potential confusion with $\R_0$.  I often do this.  Not sure
  why I didn't here.}

\djde{Peak prevalence $I_{\rm p}=a/\gamma$ might be something we want
  to mention, so the meaning of $a$ is $I_{\rm p}\times\Tg$.  ``Peak
  removal rate'' might not be the best description of $a$.}

\begin{abstract}
  Fred Brauer was an eminent mathematician who studied dynamical
  systems, especially differential equations.  He made many
  contributions to mathematical epidemiology, a field that is strongly
  connected to data, but he had no desire to touch data.
  Nevertheless, he recognized that fitting models to data is usually
  necessary when attempting to apply infectious disease transmission
  models to real public health problems.  He was curious to know how
  one goes about fitting dynamical models to data, and why it can be
  hard.  Initially in response to Fred's questions, we developed a
  user-friendly \code{R} package, \code{fitode}, that facilitates
  fitting ordinary differential equations to observed time series.
  Here, we use this package to provide a brief tutorial introduction
  to fitting compartmental epidemic models to a single observed time
  series.  We assume that, like Fred, the reader is familiar with
  dynamical systems from a mathematical perspective, but has limited
  experience with statistical methodology or optimization techniques.
  \djde{We might want to mention in the abstract that we also explain
    \code{nls}, and that we do more than provide recipes in the
    tutorial, \ie we actually explain pedagogically where they come
    from.}
\end{abstract}

\begin{keywords}
  epidemic models; infectious diseases; ordinary differential
  equations; parameter estimation; maximum likelihood; \code{fitode}
\end{keywords}

Submitted to \emph{Bulletin of Mathematical Biology}

\section{Introduction}

In their landmark 1927 paper, Kermack and McKendrick (\KM)
\cite[p.\,713]{KermMcKe27} introduced the now-standard
susceptible-infected-removed (SIR) epidemic model,
\begin{linenomath*}
  \begin{subequations}\label{eq:SIR}
    \begin{align}
      \ddt{S} &= -\beta S I \,, \\
      \noalign{\vspace{5pt}}
      \ddt{I} &= \beta S I - \gamma I \,, \label{eq:SIR;I} \\
      \noalign{\vspace{5pt}}
      \ddt{R} &= \gamma I\,, \label{eq:SIR;R}
    \end{align}
  \end{subequations}
\end{linenomath*}
where $S$, $I$ and $R$ represent the numbers of individuals who are
susceptible, infected or removed\footnote{In the words of \KM
  \cite[p.\,701]{KermMcKe27}, ``removed from the number of those who
  are sick, by recovery or by death''.}, $\beta$ is the transmission rate, and $\gamma$ is
the recovery rate.  In that original paper, \KM
\cite[p.\,714]{KermMcKe27} also fit their model to plague mortality
data from an epidemic in Bombay (now Mumbai) that occurred about 20
years before their paper was written.

In the century that has elapsed since publication of \KM's initial
paper, the field of mathematical epidemiology has expanded and
matured, and has been the subject of many books
\cite{Bart60,Bail75,AndeMay91,AndeBrit00b,DiekHees00,BrauCast01,Brau+19}
and review articles \cite{Heth00,Earn+02,Earn08,Earn09}.  Researchers
have primarily focused on \term{compartmental models} like the SIR
model, cast either as differential equations following the tradition
of \KM \cite{KermMcKe27}, or as \term{stochastic processes} in the
tradition of McKendrick \cite{McKe26} and Bartlett \cite{Bart60}.  In
recent years, as the power of computers has grown, some researchers
have turned to \term{agent-based models}, which represent each
individual as a separate unit that can have unique properties
\cite{eubank2004modelling}.

Throughout the history of the subject, and regardless of the modelling
frameworks they have exploited, mathematical epidemiologists have
frequently attempted to fit---or at least to compare---their models to
observed infectious disease data. Such fits have often been
na\"{\i}ve, with limited consideration of their quality. Over the
years, however, there has been a trend towards greater sophistication
and statistical rigour in parameter estimation for infectious disease
models; books that explain these methods have begun to appear in
recent decades \cite{Bolk08,bjorn2018}.  Careful
consideration of uncertainty is especially important when epidemic
models are used for the development and analysis of policy options for
infectious disease management \cite{elderd2006uncertainty}, a
challenge that began to absorb the attention of many mathematical
epidemiologists as soon as the emergence of SARS-CoV-2 ignited the
COVID-19 pandemic \cite{Broo+21,Hill+21,Nixo+22}.

While visiting the University of British Columbia in 2014--2015, one
of us (DE) had many conversations with Fred Brauer about epidemic
models and how they can be used in practical applications.  While he
had no desire to analyze data himself, Fred was acutely aware that
fitting to data is essential if one wishes to apply epidemic
models to real public health problems, and he did want to understand
what was involved in doing so.

Fred's curiosity inspired us to develop user-friendly software for
fitting ordinary differential equation (ODE) models to observed time
series, with the goal of illustrating the process and challenges of
model fitting to Fred and others like him, i.e., individuals who are
comfortable with mathematical analysis of ODEs but have little or no
experience with statistics and parameter estimation.  Unfortunately,
we have lost the opportunity to present our work to Fred, but it seems
fitting (!) to highlight Fred's role in the history of this work, and
to dedicate this tutorial to his memory.\footnote{We had originally
intended to submit this paper to a collection in honour of Fred's memory
\cite{Kribvand23}.}

\section{Kermack and McKendrick's fit}

We begin by revisiting \KM's \cite{KermMcKe27}
application of their SIR model \eqref{eq:SIR} to the epidemic of
plague in Bombay in 1905--1906.  The observed data (dots in
\cref{fig:Bombay}) were weekly numbers of deaths from plague.

Referring to their version of \cref{fig:Bombay}, \KM
\cite[p.\,714]{KermMcKe27} argued that ``As at least 80 to 90 per
cent.\ of the cases reported terminate fatally, the ordinate may be
taken as approximately representing [$\dbydt{R}$] as a function of
$t$.''  Since (non-human) computers did not yet exist \cite{Camp09},
and an exact analytical form for this function could not be found,
they proceeded to assume \cite[p.\,713]{KermMcKe27} that
$\frac{\beta}{\gamma}R(t)\ll1$, which yields the approximate
analytical form,
\begin{linenomath*}
\begin{equation}\label{eq:sech}
\ddt{R} \approx a \sech^2{(\omega\,t - \phi)}\,.
\end{equation}
\end{linenomath*}
Noting that the \term{basic reproduction number} is\footnote{$\R_0$ is
the expected number of secondary cases resulting from a primary case
in a wholly susceptible population \cite{AndeMay91}.}
\begin{linenomath*}
\begin{equation}\label{eq:R0def}
\R_0 = \frac{N\beta}{\gamma} \,,
\end{equation}
\end{linenomath*}
where $N$ is the total population size,
the assumption that yields \KM's approximation \eqref{eq:sech}
can be written
\begin{linenomath*}
\begin{equation}\label{eq:KMassumption}
\frac{R(t)-R(0)}{N} \ll \frac{1}{\R_0} \,,
\end{equation}
\end{linenomath*}
i.e., \cref{eq:sech} is a good approximation as long as
the proportion of the population that has been infected and
removed since the initial time is much less than $1/\R_0$.
(\KM assumed $R(0)=0$.)
\djde{We could add that this will be true initially for any $\R_0$,
  and true for all time if $\R_0$ is sufficiently close to 1, which
  can be quantified using the final size relation that \KM actually
  derived.}

Given \cref{eq:R0def}, the \term{effective reproduction number} at
time $t=0$ is
\begin{linenomath*}
\begin{equation}\label{eq:Reffdef}
\Reff = \frac{S_0\beta}{\gamma} \,.
\end{equation}
\end{linenomath*}
In terms of $\Reff$, $\gamma$, $S_0$ and $I_0$,
the parameters in \cref{eq:sech} can be
written\footnote{There is a typographical error in equation (31) of
  \KM \cite{KermMcKe27}: their factor $\sqrt{-q}$ should be $(-q)$ in
  their equivalent of the parameter we call $a$.  Baca\"er
  \cite[\S3]{bacaermodel2012} corrected this error without comment.}
\begin{linenomath*}
\begin{subequations}\label{eq:sechparams}
\begin{align}
  \omega &= \frac{\gamma}{2} \sqrt{(\Reff-1)^2 +
           \frac{2I_0}{S_0}\Reff^2}\,, \label{eq:omega} \\
  \phi &= \textrm{arctanh}\left(\frac{\Reff - 1}{2\,\omega/\gamma}
         \right)\,, \label{eq:phi} \\
  \text{and}\qquad
  a &= \frac{2\,\omega^2 S_0}{\gamma\,\Reff^2} \label{eq:a} \,.
\end{align}
\end{subequations}
\end{linenomath*}
The values of these parameters that \KM estimated for the Bombay
plague epidemic are listed in the \KM column of \cref{tab:Bombay}.
Using these values, \KM plotted their ``calculated'' curve, which we
have reproduced in \KMcol in \cref{fig:Bombay}.

<<functions to compute KM parameters, echo=FALSE>>=
omega_fun <- function(Reff, gamma, S0, I0)
    {(gamma/2)*sqrt((Reff-1)^2+(2*I0/S0)*Reff^2)}
phi_fun <- function(Reff, gamma, S0, I0)
    {atanh((Reff-1)/(2*omega_fun(Reff,gamma,S0,I0)/gamma))}
a_fun <- function(Reff, gamma, S0, I0)
    {(2 * S0/(gamma * Reff^2))*omega_fun(Reff,gamma,S0,I0)^2}
@

\section{How to fit the model to the data}

The \KMcol curve in \cref{fig:Bombay} does appear to provide a
reasonable fit to the data, but \KM \cite{KermMcKe27} gave no
indication of how their parameter estimates were obtained.
Whatever their process, they must have engaged in some
sort of \term{trajectory matching}, i.e., adjusting parameter values
until the model---\cref{eq:sech} in their case---is, by some
measure, close to the observed data points.  The most obvious metric
for this purpose is the Euclidean distance between the model curve and
the data.  Thus, a natural \term{objective function} to
minimize is
\begin{linenomath*}
\begin{equation}\label{eq:leastsquares}
\sum_{\tindex=1}^{\tindexmax} \big(\traj(\ti;\thetavec) - \traj[\ti]\big)^2 \,,
\end{equation}
\end{linenomath*}
where the observed data are the points
$\{(\ti,\traj[\ti]):\tindex=1,\ldots,\tindexmax\}$, $\thetavec$ is the
vector of parameters, and $\traj(t;\thetavec)$ is the model; for \KM's
problem, the parameter vector is $\thetavec=(a,\omega,\phi)$ and the
model is given by \cref{eq:sech}.  (Note that we write $\traj[\cdot]$
when referring to observations of the variable $\traj$ and
$\traj(\cdot\,;\cdot)$ when referring to the model.)
Choosing this objective function is equivalent to assuming that the $\traj[\ti]$ values are direct (but noisy) observations of the state variable $\traj(t)$. When the connection between the dynamical system and our observations is more complicated, we need to define an explicit \term{observation process}; see Section~\ref{sec:Uncertainty}.
Minimizing
\eqref{eq:leastsquares} with respect to $\thetavec$ would have
required some heroic arithmetic with a pencil and paper in 1927, but
it is a simple task with the aid of a modern computer.

In the following segment of R code, we fit equation \eqref{eq:sech} to
the Bombay plague data (which are included in the \code{fitode}
package that we describe below, as a data frame with columns
\code{week} and \code{mort}).  We exploit R's nonlinear least squares
function (\code{nls}), which attempts to minimize the distance
\eqref{eq:leastsquares} to the data, starting from an initial guess
(\code{start}).

<<nls-bombay>>=
sech <- function(x) {1/cosh(x)}
KM_approx <- function(t, a, omega, phi) {a * sech(omega*t - phi)^2}
KM.parameters <- c(a = 890, omega = 0.2, phi = 3.4)
nlsfit <- nls(mort ~ KM_approx(week, a, omega, phi),
              data = fitode::bombay,
              start = KM.parameters)
nls.parameters <- coef(nlsfit)
print(nls.parameters)
@

\noindent
Above, we chose as our starting value the fitted parameter values of
\KM.  Our least squares parameter values differ from \KM's by a few
percent (see \cref{tab:Bombay}).  The least squares fitted
function is shown in \nlscol in \cref{fig:Bombay}.

Starting from someone else's fit is not a great way to test the
method, but fortunately the least squares fit for this problem is not
very sensitive to the starting value.  
%%
\djde{We could reference \cref{foot:startingvalue}, or move it here
  and reference it where it currently is.}  \djde{Actually, moving
  \cref{foot:startingvalue} here might be complicated because it
  refers to unidentifiability, which we haven't mentioned yet.}
%%
To pick reasonable starting values, it often helps to think about the
meaning of parameters.  For example, in the case of \cref{eq:sech}, it
is useful to note that $a$ is the maximum of the function, and if
write $\omega t - \phi$ as $\omega(t-\tpeak)$ then
\begin{linenomath*}
  \begin{equation}\label{eq:tpeak}
    \tpeak = \frac{\phi}{\omega}
  \end{equation}
\end{linenomath*}
is the \term{peak time} (at which the maximum occurs); both $a$ and
$\tpeak$ can be approximated by looking at the plotted data.  Assuming
$I_0/S_0\ll1$, $\omega$ is half the initial exponential growth
rate\footnote{From \cref{eq:SIR;I}, the initial exponential growth
rate is $\beta S_0 - \gamma = \gamma(\Reff-1)$.}, so it can be
approximated easily by plotting the data on a log scale, estimating
the initial slope, and dividing by 2.  Very rough guesses for $a$,
$\tpeak$ and $\omega$ are sufficient to converge on the same fit:

<<nls-bombay-2>>=
a.guess <- 1000    # crude "by eye" estimate of peak value,
tpeak.guess <- 15  # peak time,
omega.guess <- 1   # and half the initial growth rate
phi.guess <- omega.guess * tpeak.guess
nlsfit <- nls(mort ~ KM_approx(week, a, omega, phi),
              data = fitode::bombay,
              start = c(a = a.guess, omega = omega.guess,
                        phi = phi.guess))
print(nls.parameters <- coef(nlsfit))
@

\noindent
However, if you experiment with starting values, you will find that if
you pick sufficiently \emph{bad} starting values, then \code{nls} will
fail.  For example, starting from $a=2000$, $\tpeak=5$, and
$\omega=0.1$ yields a ``singular gradient'' error.  More
interestingly, starting from $a=500$, $\tpeak=5$, and $\omega=0.1$
yields $a=869$, $\omega=-0.19$, $\phi=-3.48$, which is far from our
fitted values and illustrates an important fact: there is
not necessarily a unique best fit set of parameters!  In this
case, the alternative solution exists because $\sech^2(x)$ is
symmetric about the $y$ axis, but in general, there can be multiple
local minima that cause nonlinear optimizers to converge to points
that may or may not represent equally good fits to the data.  The
potential existence of multiple local optima makes fitting to data
hard; you need to be cautious, and use common sense, in interpreting
the solutions found by your software (always plot the
solutions!). Raue \emph{et al.}\/ \cite{raue2013lessons} give
suggestions for how to diagnose and handle multiple optima.

If you know that your parameters should be in a certain range, then
you can exclude values outside that range.  For example, to ensure
that all the parameters are non-negative (and exclude the alternative
fit above), you would add the \code{nls} option

<<eval=FALSE>>=
lower = c(a = 0, omega = 0, phi = 0)
@

\noindent
which would prevent convergence to negative $\omega$ and $\phi$.
Alternatively, you could write
\begin{linenomath*}
  \begin{equation}\label{eq:loglink}
    a=e^A,\quad \omega=e^\Omega,\quad \phi=e^\Phi \,,
  \end{equation}
\end{linenomath*}
and fit $A$, $\Omega$, and $\Phi$, which would guarantee positive $a$,
$\omega$, and $\phi$ without having to constrain the values of the
fitted parameters.  While this last suggestion may just seem like a cute
trick, there is more to it than that.  Many more optimization algorithms
are available for unconstrained fitting;
numerical parameter values of very small magnitude can also lead to
numerical instability, so it is advantageous to link parameters that
must lie in a given range to unconstrained parameters that can be fit
more easily.  In \cref{eq:loglink}, the \term{link function}
that converts the parameters to the unconstrained scale is $\log(x)$.
Another common link function is $\textrm{logit}(x) = \log(x/(1-x))$ (the log-odds function,
or the inverse of the logistic function), which converts the unit interval $(0,1)$ to
$(-\infty,\infty)$, and is convenient when parameters
represent proportions or probabilities.  (Requiring positivity is so
common that \code{fitode} uses a log link for all parameters by default.)

<<KM-fitted-values, echo=FALSE>>=
dig <- 3 # number of significant digits for printing
assignfun <- function(par,
                      text=".KM") {
    parnames <- names(par)

    for (p in parnames) {
        assign(paste0(p, text), signif(par[[p]], dig),
               envir=globalenv())
    }
}

assignfun(KM.parameters)
tpeak.KM <- with(as.list(KM.parameters), signif(phi / omega,dig))
@

<<nls-fitted-values, echo=FALSE>>=
assignfun(nls.parameters, text=".nls")
tpeak.nls <- with(as.list(nls.parameters), signif(phi / omega,dig))
cc <- suppressMessages(confint(nlsfit))
## would like to use built-in knitr/Sexpr{} magic but ...
## from https://stackoverflow.com/questions/8366324/r-sweave-formatting-numbers-with-sexpr-in-scientific-notation
format_sn0 <- function(x, digits = dig,
                      max_ord = 2,
                      ensuremath = TRUE) {

    if (x==0) return("0")
    ord <- floor(log(abs(x), 10))
    if (abs(ord) <= abs(max_ord)) return(as.character(signif(x, dig)))
    x <- x / 10^ord
    x <- signif(x, digits=dig)
    r <- sprintf("%s\\times 10^{%s}", x, ord)
    if (ensuremath) r <- sprintf("\\ensuremath{%s}", r)
    return(r)
}
format_sn <- Vectorize(format_sn0,
                       vectorize.args = "x")
## format_sn <- knitr::knit_hooks$get("inline")
ci_fmt  <- function(p, cc, sn = TRUE, ...) {
    ffun <- if (sn) format_sn else format
    ( ffun(cc[p,], ...)
    |> paste(collapse = ", ")
    |> sprintf(fmt = "(%s)")
    )
}
assignfun(cc[,"2.5%"], text=".lwr")
assignfun(cc[,"97.5%"], text=".upr")
@

If we accept our fit as satisfactory, what can we infer about the
dynamics of plague that \KM were attempting to capture with the SIR
model \eqref{eq:SIR}?  We need to convert the parameters of \KM's
approximation \eqref{eq:sechparams} back to the original parameters
that are directly related to the mechanism of disease spread formalized by the model (i.e., $\beta$ and $\gamma$, and initial conditions
$S_0$ and $I_0$).

\FloatBarrier

The nonlinear algebraic relationships specified by
\cref{eq:sechparams} can be inverted\footnote{Our expressions are
  slightly different from those of Baca\"er
  \cite[eq.~(3)]{bacaermodel2012} because we have corrected a minor
  error.  At the start of \S3 of \cite{bacaermodel2012}, in the
  expression for $Q$, the term $2Ry_0/x_0$ should be $2R^2y_0/x_0$ and
  this missing square is propagated through to the inversion formulae.}
analytically\footnote{In (common) situations in which nonlinear
  algebraic equations cannot be solved analytically, they can still be
  solved numerically, for example with the \code{nleqslv} package in
  R.} \cite[\S3]{bacaermodel2012}, to obtain
\begin{linenomath*}
\begin{subequations}\label{eq:invertparams}
\begin{align}
  \Reff &= 1 + \frac{2\, \omega I_0 \sinh(\phi) \cosh(\phi)}{a} \,,
          \label{eq:Reff} \\
  \gamma &= \frac{2\,\omega\tanh{\phi}}{\Reff-1} \,, \label{eq:gamma} \\
  S_0 &= \frac{2\,\Reff^2\,I_0 \sinh^2\!{\phi}}{(\Reff-1)^2} \,. % Bacaer eq. (3)
      \label{eq:S0}
\end{align}
\end{subequations}
\end{linenomath*}
Since there are four original parameters ($\beta$, $\gamma$, $S_0$,
$I_0$) and only three parameters in \KM's approximation
\eqref{eq:sech} ($a$, $\omega$, $\phi$), one of the four original
parameters needs to be specified separately; in
\cref{eq:invertparams} above we have taken this to be the initial
prevalence $I_0$.  From \cref{eq:invertparams}, we can compute the
transmission rate,
\begin{linenomath*}
\begin{equation}\label{eq:beta}
  \beta = \frac{\Reff\gamma}{S_0} \,,
\end{equation}
\end{linenomath*}
and the mean intrinsic generation interval \cite{ChamDush15},
\begin{linenomath*}
\begin{equation}\label{eq:Tg}
  \Tg = \frac{1}{\gamma} \,,
\end{equation}
\end{linenomath*}
which is the same as the mean infectious period in this simple model
\cite{KrylEarn13,Cham+18}.  \cref{tab:Bombay} lists the values of
the parameters as estimated by \KM and by us using \code{nls}.

<<invert-KM-analytically, echo=FALSE>>=
invert_params <- function(I0, params) {
    with(as.list(params), {
        tpeak <- phi / omega
        Reff <- 1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a
	q <- (Reff-1)/tanh(phi)
        gamma <- 2 * omega/q ## gamma has units of 1/week
	S0 <- 2 * Reff^2 * I0 * sinh(phi)^2/(Reff - 1)^2
	beta <- Reff * gamma/S0
	c(tpeak=tpeak, Reff=Reff, S0=S0, gamma=gamma, Tg=1/gamma, beta=beta)
    })
}
N.KM <- 10^6
I0.KM <- 1
orig.params.KM <- invert_params(I0 = I0.KM, params = KM.parameters)
orig.params.KM <- signif(orig.params.KM,dig)
assignfun(orig.params.KM)
R0.KM <- Reff.KM/S0.KM*N.KM
I0.nls <- 1
orig.params.nls <- invert_params(I0 = I0.nls, params = coef(nlsfit))
orig.params.nls <- signif(orig.params.nls,dig)
assignfun(orig.params.nls, ".nls")
R0.nls <- Reff.nls/S0.nls*N.KM
@

<<delta-method-ci, echo=FALSE>>==
convert_express <- list(
    tpeak=expression(phi/omega),
    Reff=expression((1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a)),
    gamma=expression(2 * omega/((1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a)-1)/tanh(phi)),
    S0=expression(2 * ((1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a))^2 * I0 * sinh(phi)^2/(((1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a)) - 1)^2),
    beta=expression((2 * omega*tanh(phi))*((1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a) - 1)/(2 * (1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a) * I0 * sinh(phi)^2)),
    Tg=expression(1/(2 * omega/((1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a)-1)/tanh(phi))),
    R0=expression(((1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a) - 1)^2/(2 * (1 + 2 * omega * I0 * sinh(phi) * cosh(phi)/a) * I0 * sinh(phi)^2)*N)
)

KMcifun <- function(fit,
                    I0,
                    N) {
    param <- coef(fit)
    ##dg/dx
    dgdx <- sapply(convert_express, function(x) {
        dd <- deriv(x, c("a", "omega", "phi"))
        ee <- eval(dd, c(as.list(param), I0=I0, N=N))
        attr(ee, "gradient")
    })

    vcov <- vcov(fit)
    est_vcov <- t(dgdx) %*% vcov %*% dgdx
    est_err <- sqrt(diag(est_vcov))
    est_par <- sapply(convert_express, eval, c(as.list(param), I0=I0, N=N))
    z <- -qnorm((1-0.95)/2)

    matrix(
        c(est_par - z * est_err, est_par + z * est_err),
        ncol=2,
        dimnames=list(
            c("tpeak", "Reff", "gamma", "S0", "beta", "Tg", "R0"),
            c("2.5%", "97.5%")
        )
    )
}

cc.delta <- KMcifun(nlsfit, I0=I0.KM, N=N.KM)
assignfun(cc.delta[,"2.5%"], text=".lwr")
assignfun(cc.delta[,"97.5%"], text=".upr")
@

<<delta-method-band, echo=FALSE>>==
traj_express <- expression(a * 1/cosh(omega*t - phi)^2)

traj_ci <- function(t,
                    parameters,
                    vcov,
                    level=0.95) {
    traj <- eval(deriv(traj_express, c("a", "omega", "phi")), c(as.list(parameters), t=t))

    traj_dgdx <- attr(traj, "gradient")

    est_vcov <- traj_dgdx %*% vcov %*% t(traj_dgdx)
    est_err <- sqrt(diag(est_vcov))
    ll <- (1-level)/2
    z <- -qnorm(ll)

    dd <- data.frame(
        times=t,
        estimate=traj,
        lwr=traj-est_err*z,
        upr=traj+est_err*z
    )
    names(dd)[3:4] <- c(paste(100*ll, "%"), paste(100*(1-ll), "%"))

    dd
}
@

\paragraph*{Correctly handling weekly mortality.}

We have glossed over the fact that we have fitted observed weekly
mortality to the \emph{instantaneous} rate, $\dbydt{R}$
\eqref{eq:sech}, which is not observed.  We did this because it is
what \KM did, and we wanted to be able to compare formal nonlinear
least squares fits to \KM's results\footnote{In his reanalysis of
  \KM's results, Baca\"er \cite{bacaermodel2012} also retained this
  conceptual error.}.  Weekly mortality reported at time $\ti$ should
really be modelled as the aggregation of $\dbydt{R}$ over the
preceding week, i.e., it would be better to define
\begin{linenomath*}
  \begin{subequations}\label{eq:sechcorrect}
    \begin{align}
      \traj(\ti;\thetavec) &=
                         \int_{\timo}^{\ti} \ddt{R}\,\dt
\label{eq:correct.mortality} \\
                       &= \int_{\timo}^{\ti}  a \sech^2{(\omega\,t - \phi)}\,\dt \\
                       &= \frac{a}{\omega}\Big(\tanh{(\omega\,\ti - \phi)}
                         - \tanh{(\omega\,\timo - \phi)}\Big) \,.
    \end{align}
  \end{subequations}
\end{linenomath*}
Indeed, whether we are fitting to mortality or incidence or another
instantaneous rate, we should be integrating over the observation
interval, which is precisely what we do below when fitting to the ODEs
directly.  In addition, we really ought to consider the fact that not
all infections end in death---we have followed \KM in assuming that
the \term{infection fatality proportion} is 100\%.  Similarly, when
analyzing incidence data, the \term{reporting proportion} ought to be
taken into account.

\section{Uncertainty}\label{sec:Uncertainty}

To this point, we have addressed only an optimization problem.  We
solved it using the method of nonlinear least squares, which yields
estimates of the values of the parameters of the model
\eqref{eq:sech}.  But our best estimates are just that:
\emph{estimates}, not known values of the parameters.

To quantify uncertainty in our estimates, we need a statistical
framework.  The typical output of such a framework is a
\term{confidence interval} (CI) within which our best estimate lies.
For example, the final column of \cref{tab:Bombay} lists 95\% CIs
on our \code{nls} parameter estimates, and the pink shaded region in
the top panel of \cref{fig:Bombay} is a 95\% \term{confidence
  band}, which shows CIs for each point of the fitted model curve.

To understand how to estimate CIs, we will start by thinking about our observation model, the
probability of observing the data $\{\traj[\ti]\}$ given $\{\traj(ti)\}$.
We imagine that the
model \eqref{eq:sech} is a perfect representation of reality, and we
consider the deviations from the model curve in
\cref{fig:Bombay} to be observation errors.
A simple observation model assumes that the observation error for each data point is independent and
identically distributed (iid), and drawn from a Normal distribution
with zero mean and standard deviation $\sigma$ equal to the standard
deviation of the residuals (the differences between the model curve
and the observed data).  Then the probability density of the data given the
model is
%%
\begin{linenomath*}
  \begin{equation}\label{eq:prob.data.given.model}
    \begin{split}
      \Pop(\text{data} \mid \text{model})
      & = \lim_{dx \to 0} \left(\frac{1}{dx} \right) \textbf{Prob}\left(\traj(\ti;\theta) < \traj[\ti] < \traj(\ti;\theta)\right) \\
      & = \prod_{\tindex=1}^n
\left[\frac{1}{\sqrt{2\pi\sigma^2}}
\exp\left(-\frac{\big(\traj(\ti;\thetavec) - \traj[\ti]\big)^2}{2\sigma^2} \right) d{\traj[\ti]}
\right]
\,.
\end{split}
\end{equation}
\end{linenomath*}
%%
We use a probability density function here because the Normal is a continuous distribution; we would use a probability mass function for a discrete response distribution such as the Poisson. In practice, we don't have to worry about this distinction much in practice when we are estimating the parameters of an epidemic model, and we will refer to ``probability'' rather than ``probability density'' from now on.

Using these assumptions we can adopt a \emph{maximum likelihood}
framework, where we consider parameter values that maximize the
probability of observing the data \eqref{eq:prob.data.given.model} to
be the best \cite{Bolk08}.  We define the \term{likelihood} $\lik$ of
a set of parameter values $\thetavec$ as
\begin{linenomath*}
\begin{equation}\label{eq:lik}
\lik(\thetavec)
= \Pop (\{\traj[\ti]\} \mid \thetavec) \,.
\end{equation}
\end{linenomath*}
Maximizing $\lik$ with respect to $\thetavec$ or, equivalently,
minimizing the negative log-likelihood,
yields an estimate,
\begin{linenomath*}
\begin{subequations}\label{eq:negloglik}
\begin{align}
\thetavechat
&= \argmax_\thetavec \lik(\thetavec) \\
&= \argmin_\thetavec \big(\!\!-\log{\lik(\thetavec)}\big) \\
&= \argmin_\thetavec\Big(
\sum_{\tindex=1}^{\tindexmax} \big(\traj(\ti;\thetavec) - \traj[\ti]\big)^2
\;+\; \text{constant}
\Big) \\
&= \argmin_\thetavec\sum_{\tindex=1}^{\tindexmax} \big(\traj(\ti;\thetavec) - \traj[\ti]\big)^2
 \,,
\end{align}
\end{subequations}
\end{linenomath*}
which---lo and behold---agrees exactly with \eqref{eq:leastsquares},
the least squares solution!  The standard way of expressing this is to
say that the least squares solution $\thetavechat$ is the
\term{maximum likelihood estimate} (MLE) of $\thetavec$, under the
assumption of independent, identically distributed
(i.e., mean-zero, constant-variance) Normal observation errors in the time series.

Having introduced the idea of maximum likelihood, we can do better by
making a more realistic assumption about the error distribution.  We
will then end up with a different likelihood function to maximize, and
obtain a different $\thetavechat$, but the basic idea is the same.

So what is a better assumption about the observation error
distribution, and how can we use the likelihood function to estimate
uncertainty in $\thetavechat$ and on the fitted trajectory?

Our data are actually non-negative, discrete counts of deaths (or
cases in other epidemiological contexts),
so a continuous, real-valued Normal distribution is
somewhat unrealistic. More importantly, we expect (and can see
in the plots of our fitted curves) that the
magnitude of error in the observations will vary over the course of
the epidemic; the error might be $\pm 2$ at the beginning of the
epidemic when mortality is low and $\pm 50$ at the peak.

We could address both of these problems by using a
Poisson distribution of observations with mean equal to the fitted
model trajectory [\cref{eq:SIR;R} or \cref{eq:sech}]. This approach handles
discrete observations and allows the variance to change as a function
of the mean. However, the Poisson distribution assumes
\term{equidispersion}---the variance is equal to the mean---while
typical observation errors are \term{overdispersed}, meaning that
the variance is greater than the mean. Ignoring overdispersion
will underestimate the uncertainty in the parameters and lead
to overly narrow confidence intervals on parameters and predictions.
The negative binomial distribution is the most common way to generalize
the Poisson to allow for overdispersion \citep{linden2011using}, although
other distributions such as the generalized Poisson are occasionally used
\citep{Broo+2019,Kim+2022}.

The probability mass function for the \term{negative binomial
  distribution} (for counts $x=0,1,2,\ldots$) is
\begin{linenomath*}
\begin{equation}\label{eq:NB}
  \texttt{NB}(x;\mu,k) =
  \frac{\Gamma(k+x)}{\Gamma(k) x!}
  \left(\frac{k}{k+\mu}\right)^k
  \left(\frac{\mu}{k+\mu}\right)^x \,.
\end{equation}
\end{linenomath*}
The predicted variance of a particular observation $\traj[\ti]$ is
given by $\mu_\tindex (1+\mu_\tindex/k)$, where
$\mu_\tindex(\thetavec) = \traj(\ti;\thetavec)$ is the model evaluated
at the $\tindex$\textsuperscript{th} observed data point
[\emph{cf.}\ \eqref{eq:leastsquares} and \eqref{eq:sechcorrect}].  The
maximum likelihood estimate is, therefore,
\begin{linenomath*}
  \begin{equation}
\label{eq:negbinomMLE}
\begin{split}
 \thetavechat
 &= \argmin_\thetavec\sum_{\tindex=1}^{\tindexmax} \Big(\!\!
                -\log\Gamma\big(\traj[\ti]+k\big)
                +\log\Gamma(k) + \log(\traj[\ti]!) \\
                &\hspace{3cm}
                -k \log\left(\frac{k}{k+\mu_\tindex(\thetavec)}\right)
                -\traj[\ti] \log\left(\frac{\mu_\tindex(\thetavec)}{k+\mu_\tindex(\thetavec)}\right)
                \Big)
  \,.
\end{split}
  \end{equation}
\end{linenomath*}
Here, the overdispersion parameter $k$ also needs to be estimated
alongside $\thetavechat$ to maximize the likelihood.
This is different from the likelihood
 associated with Normal errors, where $\sigma^2$ can be either
 computed as the variance of the residuals across the full time
 series or estimated jointly with model parameters.

Regardless of the form of the likelihood function, we can use it to
obtain CIs on the MLE $\thetavechat$.  A relatively simple approach is
to use the the curvature of $-\log{\lik(\thetavec)}$ at $\thetavechat$
to infer parameter values of a multivariate Normal distribution for
$\thetavec$.  At $\thetavechat$, the shape of $-\log{\lik}$ is
described by its \term{Hessian matrix} (the matrix of second order
partial derivatives of $-\log{\lik}$, also known as the \term{Fisher
  information matrix}), and the inverse of the Hessian is the
\term{variance-covariance matrix} $\mathrm{Cov}(\thetavec)$ that
specifies the desired multivariate Normal with mean $\thetavechat$.
This relationship between $\mathrm{Cov}(\thetavec)$ and the Hessian of
$-\log{\lik}$ is, admittedly, not obvious!  See \citep[\S6.5]{Bolk08}
for a heuristic explanation or \cite[][\S\S9.7, 9.10]{Wassermanall2010} for a rigorous (if terse) explanation.

\hypertarget{DeltaMethod}{} The diagonal elements of
$\mathrm{Cov}(\thetavec)$ are the (estimated) variances of the
parameter estimates, so we can take their (positive) square roots to
get the standard error (SE) and compute approximate 95\% confidence
intervals by adding $\pm1.96\,\mathrm{SE}$ to $\thetavechat$
($\pm 1.96$ represents a range containing 95\% of the probability of a
standard Normal distribution).  To obtain CIs on \emph{functions of the
fitted parameters} (e.g., $\R_0$ or $\gamma$ if our model is \KM's
approximation \eqref{eq:sech}), we build on the idea that if the error
in a parameter $a$ is $\Delta a$, then the associated error in a
(differentiable) function $g(a)$ is $\Delta{g}\approx g'(a)\Delta{a}$.
Given a (smooth) nonlinear function $g(\thetavec)$ of the parameters,
the \term{Delta Method} \cite{Dorf38,VerH12} expands
$\textrm{Var}(g(\thetavec))$ to first order about $\thetavechat$,
which gives us the variance-covariance matrix of $g(\thetavec)$
\cite[][\S7.5.2]{Bolk08} \cite[][\S9.9]{Wassermanall2010}.  In
particular, the variance of $g(\thetavec)$ is
\begin{linenomath*}
\begin{subequations}\label{eq:DeltaMethod}
\begin{align}
\mathrm{Var}(g(\thetavec))
&\approx
\mathrm{Var}\big[g(\thetavechat)
   + (\gradtheta g)(\thetavechat)\cdot(\thetavec-\thetavechat)\big] \\
&=
\mathrm{Var}\big[(\gradtheta g)(\thetavechat)\cdot(\thetavec-\thetavechat)\big] \\
&=
\mathbb{E}\big[\big((\gradtheta g)(\thetavechat)\cdot(\thetavec-\thetavechat)\big)^2\big] \\
&=
\,\mathbb{E}\big[
(\gradtheta g)(\thetavechat)^\transpose
  (\thetavec-\thetavechat)
  (\thetavec-\thetavechat)^\transpose
  (\gradtheta g)(\thetavechat)
  \big]\,
\\
&=
(\gradtheta g)(\thetavechat)^\transpose
\,\mathbb{E}\big[
  (\thetavec-\thetavechat)
  (\thetavec-\thetavechat)^\transpose
  \big]\,
(\gradtheta g)(\thetavechat) \\
&=
(\gradtheta g)(\thetavechat)^\transpose
\,\textrm{Cov}(\thetavec)\,
(\gradtheta g)(\thetavechat)
\end{align}
\end{subequations}
\end{linenomath*}
We can again get the 95\% CIs by taking square roots and computing
$g(\thetavechat) \pm1.96\,\mathrm{SE}$.

Given a fit of \KM's approximation \eqref{eq:sech} to the time series
data, which yields $\thetavechat=(\hat{a},\hat{\omega},\hat{\phi})$, we
can apply the Delta method \eqref{eq:DeltaMethod} to the nonlinear
relationships \eqref{eq:invertparams} to obtain CIs on
$g(\thetavechat)=(\hat{\Reff}, \hat{\gamma},\hat{S_0})$.  This is
precisely how we obtained the CIs on the derived parameters listed in
\cref{tab:Bombay}.  Perhaps less obviously, we can also use the
Delta method to obtain CIs on the fitted trajectory at each
observation time $\ti$ (and hence obtain a confidence band) by
considering $g(\thetavec)=\traj(\ti;\thetavec)$.  This is how we obtained
the pink confidence band shown in \cref{fig:Bombay}.

Better confidence intervals can be obtained using the \term{profile
  likelihood}, which is calculated by fixing a set of model parameters
to specific values and fitting the remaining parameters to maximize
the likelihood \citep[][\S 7.5.1]{Bolk08}.  By calculating the profile
likelihood across a range of parameter values, we obtain the profile
likelihood surface, from which confidence intervals can be estimated
using the likelihood ratio test \citep[][\S 6.4.1.1]{Bolk08}.  While
profile likelihoods generally give more accurate estimates of
confidence intervals, calculating the profile likelihood can be
challenging, if not practically impossible, for derived parameters or
epidemic trajectories \cite[][\S 7.5.1.2]{Bolk08}.  Consequently, we
rely on the Delta Method here.

\section{Fitting the ODE}\label{sec:fitode}

Until now, we have focused on fitting \KM's approximation
\eqref{eq:sech} rather than actual solutions of the SIR model
\eqref{eq:SIR}.  If we had an exact analytical solution of the SIR
ODE~\eqref{eq:SIR} then we could proceed as above, replacing the
approximate analytical expression \eqref{eq:sech} with the exact
formula.  Since we do not have an exact solution, we must instead rely
on numerical solutions of the ODE.  Fitting numerical solutions of
ODEs to data introduces significant coding/computational challenges,
but conceptually the problem is the same as if we did have an
analytical formula.  We can still use the Delta method
\eqref{eq:DeltaMethod} to estimate uncertainty, but calculating the
gradient $(\gradtheta g)(\thetavec)$ is not straightforward if $g$ is
a numerical solution of an ODE; we must simultaneously solve a set of
\term{sensitivity equations} \cite[Eq.\,(6)]{raue2013lessons}
alongside the main differential equations. Sensitivity equations
define the time derivatives of the gradients of trajectories with
respect to the parameters.  They can easily be derived using the chain
rule; if we write a generic, autonomous ODE for $\xvec(t;\thetavec)$
as
\begin{equation}\label{eq:genericODE}
  \ddt{\xvec} = \fvec(\xvec,\thetavec)\,,
  \qquad \xvec(0,\thetavec) = \xvec_0(\thetavec),
\end{equation}
then the sensitivity equations are
\begin{linenomath*}
\begin{subequations}\label{eq:senseqns}
\begin{align}
\ddt{}\Big(\gradtheta\xvec(t;\thetavec)\Big)
&= \gradtheta\Big( \ddt{\xvec(t;\thetavec)} \Big)
= \gradtheta\Big( \fvec(\xvec,\thetavec) \Big) \\
\noalign{\vspace{10pt}}
&=  \gradx\fvec(\xvec,\thetavec)
    \, \gradtheta\xvec(t;\thetavec)
   +  \gradtheta\fvec(\xvec,\thetavec) \,. \label{eq:senseqns;b}
\end{align}
\end{subequations}
\end{linenomath*}
If $\xvec$ and $\thetavec$ are $\xindexmax$- and $\thetaindexmax$-dimensional,
respectively, then the $\xindexmax\thetaindexmax$ \term{sensitivities} $\sens_{ij}(t)$
are given by the $\xindexmax\times \thetaindexmax$ \term{sensitivity matrix},
\begin{equation}
\sensmat(t) = \gradtheta\xvec(t;\thetavec) \,.
\end{equation}
\cref{eq:senseqns} defines a set of $\xindexmax\thetaindexmax$ differential
equations for $\sens_{ij}$,
\begin{subequations}\label{eq:senseqns.S}
\begin{equation}
  \ddt{\sensmat} = \big[\gradx\fvec(\xvec,\thetavec)\big]
    \, \sensmat
   +  \big[\gradtheta\fvec(\xvec,\thetavec)\big] \,,
\end{equation}
which can be solved jointly with the original ODEs
\eqref{eq:genericODE} for the state variables ($\xvec$) by specifying
initial conditions
\begin{equation}\label{eq:sensic}
  \sensmat(0) = \gradtheta\big(\xvec_0(\thetavec)\big) \,.
\end{equation}
\end{subequations}
%%
We can then use a further chain-rule step to compute the (total)
derivative of the log-likelihood of the observations with respect to
the parameters.  To get this right, it helps to make explicit the
dependence on the trajectory ($\xvec$) versus dependence on the
parameters ($\thetavec$, by which we will now mean all parameters,
including parameters of the trajectory model and of the observation
process model).  For a general function $\Phivec(\xvec,\thetavec)$,
the total derivative with respect to $\thetavec$ is
\begin{equation}
  \dd{\Phivec}{\thetavec}
  = \gradx\Phivec \; \gradtheta\xvec + \gradtheta\Phivec \,.
\end{equation}
To apply this to the log-likelihood, it is helpful to make dependence
on the trajectory $\xvec$ explicit.
Consistent with our notation above [e.g., \cref{eq:leastsquares}], we
write $\xvec[\ti]$ for the observations at times
$\ti\in\{t_1,t_2,\ldots,t_{\tindexmax}\}$, making it easier to
distinguish them from the fitted model trajectory evaluated at these
times, $\xvec(\ti;\thetavec)$.  Then
%%
\begin{linenomath*}
\begin{subequations}\label{eq:2nd.chain}
\begin{align}
  \dd{\,\log\lik(\thetavec)}{\thetavec}
  &= \dd{}{\thetavec}\Big(\log\Pop\big(\{\xvec[\ti]:\tindex=1,\dots,\tindexmax\} \mid \xvec(\ti;\thetavec),\,\thetavec\big)\Big) \\
  &= \dd{}{\thetavec}\Big(\log\prod_{\tindex=1}^{\tindexmax} \Pop\big(\xvec[\ti] \mid \xvec(\ti;\thetavec),\,\thetavec\big)\Big) \\
  &= \dd{}{\thetavec}\sum_{\tindex=1}^{\tindexmax} \Big(\log \Pop\big(\xvec[\ti] \mid \xvec(\ti;\thetavec),\,\thetavec\big)\Big) \\
  &= \sum_{\tindex=1}^{\tindexmax} \dd{}{\thetavec}\Big(\log \Pop_\tindex(\xvec,\thetavec)\Big)
     \qquad [\text{\footnotesize abbreviating $\Pop_\tindex(\xvec,\thetavec) \equiv
       \Pop\big(\xvec[\ti] \mid \xvec(\ti;\thetavec),\,\thetavec\big)$}]
       \\
  &= \sum_{\tindex=1}^{\tindexmax} \frac{1}{\Pop_\tindex(\xvec,\thetavec)}
  \Big( \gradx\Pop_\tindex(\xvec,\thetavec) \, \gradtheta\xvec
  + \gradtheta\Pop_\tindex(\xvec,\thetavec) \Big)\Big|_{\xvec=\xvec(\ti;\thetavec)} \\
  &= \sum_{\tindex=1}^{\tindexmax} \frac{1}{\Pop_\tindex(\xvec,\thetavec)}
  \Big( \gradx\Pop_\tindex(\xvec,\thetavec) \, \sensmat(\ti)
  + \gradtheta\Pop_\tindex(\xvec,\thetavec) \Big)\Big|_{\xvec=\xvec(\ti;\thetavec)}
 \,,
\end{align}
\end{subequations}
\end{linenomath*}
where we typically assume the probability distribution
\begin{equation}\label{eq:PopNB}
\Pop\big(\xvec[\ti] \mid \xvec(\ti;\thetavec),\,\thetavec\big) =
\prod_{\xindex=1}^\xindexmax\texttt{NB}(x_\xindex[\ti];\, x_\xindex(\ti,\thetavec),\,\thetavec) \,.
\end{equation}
We have slightly abused notation here, compared with \cref{eq:NB}; we
have written $\thetavec$ rather than $k$ as the final argument of the
negative binomial distribution, since there might be a different $k$
for each observed variable $x_\xindex$, and we collect all parameters
into the single vector $\thetavec$.  (The examples we discuss in this
paper involve only a single observed time series, so $\xindexmax=1$.)

Integrating the sensitivity equations \eqref{eq:senseqns.S} in
parallel with the ODEs \eqref{eq:genericODE} is a computationally
efficient and numerically stable way to calculate the overall
gradients of the log-likelihood with respect to the parameters, which
makes nonlinear estimation more robust and efficient. We can also use
these gradients to calculate CIs using the Delta method.  Raue
\emph{et al.} \cite{raue2013lessons} give a detailed comparison
between using the sensitivity equations and computing gradients by
finite-difference approximations.  (Bj{\o}rnstad
\cite[Chapter~9]{bjorn2018} also gives an introduction to trajectory
matching.)

The \code{fitode} package\footnote{\code{fitode} is available on
\href{https://cran.r-project.org/}{CRAN}, and can be installed via
\code{install.packages("fitode")}.} does all of this computational
work under the hood, and makes it as easy for a user to fit an ODE to
data as it was for us to use \code{nls} above to fit a curve based on
an analytical formula.  We begin illustrating the use of the package
by fitting the SIR model~\eqref{eq:SIR} to the Bombay plague epidemic.

We first load the package
<<load fitode, message=FALSE>>=
library(fitode)
@
\noindent
and define a model object:

\vbox{
<<sirmortmodel_correct>>=
SIR_model <- odemodel(
    name="SIR model",
    model=list(
        S ~ - beta * S * I,
        I ~ beta * S * I - gamma * I,
        R ~ gamma * I
    ),
    observation = list(
        mort ~ dnbinom(mu = R, size = k)
    ),
    diffnames="R",
    initial=list(
        S ~ S0,
        I ~ I0,
        R ~ 0
    ),
    par=c("beta", "gamma", "S0", "I0", "k")
)
@
}% end vbox
\noindent
In the model definition above:
\begin{description}
\item[\code{model}]specifies the vector field given by the ODE \eqref{eq:SIR}.
\item[\code{observation}]specifies the observation model: the observed data
  (\code{mort}) are assumed to arise from sampling from the negative
  binomial distribution [\code{dnbinom}, \cref{eq:NB}] with
  overdispersion parameter $k$. Ordinary least squares
  (with normally distributed observation errors)
  can be implemented by changing the \code{observation} argument to
  {\tt
    \hlstd{mort}
    \!\!{\color{black}\~{}}\!\!
    \hlkwd{ols}\hlstd{(}\hlkwc{mean} \hlstd{= R)}}.
  The mean of the distribution is given by the incidence derived from
  the fitted model trajectory [\cref{eq:correct.mortality}],
\begin{linenomath*}
\begin{equation}
  \mu(\ti) = \int_{\timo}^{\ti} \ddt{R}\,\dt
  = R(\ti) - R(\timo)
  \label{eq:correct.mortality2}
 \,,
\end{equation}
\end{linenomath*}
Fitting to such differences, useful whenever the observations represent
accumulated values of processes (such as infections, recoveries, or deaths) between observation times, is implemented by using the
\code{diffnames} argument to specify the state variable for which
consecutive differences are to be used (so, if the focal variable is
$R$ then \code{fitode} fits to $R(\ti) - R(\timo)$ rather than
$R(\ti)$).
\item[\code{initial}]conditions are expressed as numbers of individuals.
\item[\code{par}]refers to the parameters to be fitted:
  $\beta$, $\gamma$, initial conditions $S(0)$ and $I(0)$, and the overdispersion parameter $k$.
\end{description}
Since we are taking the difference $\mu(\ti) = R(\ti) - R(\timo)$ to
calculate the mortality trajectory,\footnote{Modelers often
  fit trajectories to cumulative curves.  However, doing so is ill-advised because
  points in a cumulative time series are not independent, making it
  difficult to define CIs \cite{King+15}.}  we have to add an extra
row representing $t_0$ to the data set in order to compute
$\mu(t_1) = R(t_1) - R(t_0)$:
<<sirmortfit_data, cache=TRUE, warning=FALSE>>=
bombay2 <- rbind(
    c(times=bombay$week[1] -
          diff(bombay$week)[1], mort=NA),
    bombay
)
@

\noindent
Taking our previous parameter estimates from \code{nls} as starting
values (and choosing a starting value for $k$), we can fit the model
by calling the \code{fitode} function:

<<sirmortfit,cache=TRUE,warning=FALSE,message=FALSE, tidy = FALSE>>=
SIR_start <- c(beta=beta.nls, gamma=gamma.nls,
               I0=I0.KM, S0=S0.nls, k=50)
SIR_fit <- fitode( model = SIR_model, data = bombay2,
                  fixed = list(gamma=gamma.nls),
                  start = SIR_start, tcol = "week" )
@ 

\noindent In the fitting function above:
\begin{description}
\item[\code{model}]specifies the ODE model to be fitted.
\item[\code{data}]specifies the data.
\item[\code{fixed}]specifies parameter values to be fixed (and
  therefore not estimated); above, we chose to assume that the
  recovery rate $\gamma$ is known (due to parameter
  unidentifiability\footnote{In short, unidentifiability of $\gamma$
  means that we can obtain nearly identical fits across a wide range
  of $\gamma$.  While it is possible to fit the model without fixing
  $\gamma$, the resulting estimates are sensitive to starting
  conditions and numerically unstable, preventing a reliable
  calculation of the Hessian matrix and therefore precluding
  estimation of confidence intervals. These issues could be addressed
  alternatively by fixing a different parameter instead and estimating
  $\gamma$.  We typically choose to fix $\gamma$ because the mean
  duration of infection ($1/\gamma$) can often be estimated from
  independent data sources; here, to make comparisons of fits easier
  to interpret, we have fixed $\gamma$ to the value we estimated via
  \code{nls} fits of the \KM approximation
  \eqref{eq:sech}.\label{foot:fixedgamma}}).
\item[\code{start}]specifies the starting parameter set for the
  optimization\footnote{In general, worse models (providing a poorer
  or less identifiable fit to the data) and worse data (fewer data
  points and more noise) will increase the sensitivity of fits to the
  starting values.\label{foot:startingvalue}}.
\item[\code{tcol}]specifies the name of the time column of the data
  frame.
\end{description}
The resulting fits are plotted in \cref{fig:Bombay} and summarized in
\cref{tab:bombay.fitode}.  The estimated parameter values (the
\term{coefficients} of the model) can be obtained
via \code{coef(SIR_fit)}.  The coefficients together with associated
confidence intervals are obtained via \code{confint(SIR_fit)}, which
can also provide confidence intervals for derived
parameters using the Delta method.  Note that \code{fitode} gives
discrete predictions (rather than smooth curves) because we are
calculating mortality at discrete (weekly) time intervals using
\cref{eq:correct.mortality2}.

<<sirmortfitcoef, include=FALSE>>==
coef.bombay.fitode.nb <- coef(SIR_fit)
ci.bombay.fitode.nb <- confint(SIR_fit)[,-1]
assignfun(coef.bombay.fitode.nb, ".bombay.fitode.nb")
@

<<sirmortconfint, include=FALSE>>==
ci.derived.bombay.fitode.nb <- confint(SIR_fit,
        parm=list(
            Reff~beta*S0/gamma.nls,
            R0~beta*N.KM/gamma.nls,
            Tg~1/gamma.nls
        ))
assignfun(ci.derived.bombay.fitode.nb[,1], ".bombay.fitode.nb")
@

<<confband, include=FALSE>>=
tmax <- 33
tvals <- 0:tmax
SIR_confband <- predict(SIR_fit, times=tvals, level=0.95)$mort
@

% OLS:
<<sirmortmodel, include=FALSE>>=
SIR_model_ols <- odemodel(
    name="SIR model",
    model=list(
        S ~ - beta * S * I,
        I ~ beta * S * I - gamma * I,
        R ~ gamma * I
    ),
    observation = list(
        mort ~ ols(mean = R)
    ),
    initial=list(
        S ~ S0,
        I ~ I0,
        R ~ 0
    ),
    par=c("beta", "gamma", "S0", "I0"),
    diffnames="R"
)
@

% Also fixing gamma for now:
<<sirmortfit_correct, cache=TRUE, warning=FALSE, include=FALSE>>=
SIR_start_ols <- coef(SIR_fit)[-5]

SIR_fit_ols <- fitode(
    SIR_model_ols,
    data = bombay2,
    start = SIR_start_ols,
    fixed = list(gamma=gamma.nls),
    tcol = "week"
)
@

<<sirmortfitcoefols, include=FALSE>>==
coef.bombay.fitode.ols <- coef(SIR_fit_ols)
ci.bombay.fitode.ols <- confint(SIR_fit_ols)[,-1]
assignfun(coef.bombay.fitode.ols, ".bombay.fitode.ols")

ci.derived.bombay.fitode.ols <- confint(SIR_fit_ols,
        parm=list(
            Reff~beta*S0/gamma.nls,
            R0~beta*N.KM/gamma.nls,
            Tg~1/gamma.nls
        ))

assignfun(ci.derived.bombay.fitode.ols[,1], ".bombay.fitode.ols")
@

<<sirmortfit_ols_confband, include=FALSE>>==
SIR_ols_confband <- predict(SIR_fit_ols, times=tvals, level=0.95)$mort
@

<<bombay_likelihood_fit, cache=TRUE, warning=FALSE, message=FALSE, include=FALSE>>==
if (file.exists("bombay-likelihood.rda")) {
    load("bombay-likelihood.rda")
} else {
    nfits <- 100
    gammavec <- seq(1, 28, length.out=nfits)
    loglikvec <- rep(NA, nfits)
    fitlist <- vector('list', length(gammavec))

    for (i in 1:nfits) {
        if (i==1) {
            start_ll <- coef(SIR_fit2)
        } else {
            start_ll <- coef(fitlist[[which.max(loglikvec[1:(i-1)])]])
        }

        fitlist[[i]] <- try(fitode(
            SIR_model,
            data = bombay2,
            start = start_ll,
            fixed = list(gamma=gammavec[i]),
            tcol = "week"
        ))

        if (!inherits(fitlist[[i]], "try-error")) {
            loglikvec[i] <- logLik(fitlist[[i]])
        }
    }
    save("gammavec", "loglikvec", "fitlist", file="bombay-likelihood.rda")
}
@

<<bombay_likelihood_plot, cache=TRUE, include=FALSE>>==
plot(gammavec, loglikvec,
     xlab="gamma (1/week)",
     ylab="log likelihood",
     ylim=c(-140, -135.5))
@

\section{Cautionary remarks concerning fits to Bombay plague}

We have highlighted the Bombay plague data because of their prominent
role in \KM's paper \cite{KermMcKe27} and, consequently, for the
history of mathematical epidemiology.  However, while they provide an
interesting example with which to illustrate the process of fitting an
epidemiological model to data, modelling plague dynamics
with the simple SIR model is, at best,
difficult to justify: Baca\"er \cite{bacaermodel2012} argues that
the trajectory of the Bombay plague epidemic was primarily governed
by seasonality rather than SIR dynamics.
Indeed, \KM\ themselves recognized that their model involves a
sequence of optimistic assumptions, which they
admitted were not ``strictly'' satisfied:

\begin{quote}
  ``We are, in fact, assuming that plague in [humans] is a reflection
  of plague in rats, and that with respect to the rat (1) the
  uninfected population was uniformly susceptible; (2) that all
  susceptible rats in the island had an equal chance of being
  infected; (3) that the infectivity, recovery, and death rates were
  of constant value throughout the course of sickness of each rat; (4)
  that all cases ended fatally or became immune; and (5) that the flea
  population was so large that the condition approximated to one of
  contact infection.  None of these assumptions are strictly fulfilled
  and consequently the numerical equation can only be a very rough
  approximation. A close fit is not to be expected, and deductions as
  to the actual values of the various constants should not be
  drawn.''

  %%It may be said, however, that the calculated curve, which implies
  %%that the rates did not vary during the period of the epidemic,
  %%conforms roughly to the observed figures.

  \rightline{--- \KM \cite[p.\,715]{KermMcKe27}}
\end{quote}

\noindent
Given the mental gymnastics required to motivate applying the SIR
model to plague transmission, it is surprising that \KM did not choose
to examine a more obviously suitable disease.  The surprise is
especially extreme given that the most salient infectious disease
epidemic in the 1920s would have been the 1918 influenza pandemic,
which did involve direct human-to-human transmission, and for which
much more detailed data were available at the time
\cite{Roge20,Fros20,Eich23}.

\section{Influenza in Philadelphia, October 1918}\label{sec:phila}

Deaths caused ultimately by influenza are often attributed to
pneumonia \cite{Earn+02}, so influenza mortality studies typically
combine pneumonia and influenza (P\&I).  Among published tables
summarizing P\&I mortality during the 1918 pandemic, a particularly
valuable example concerns the main wave in the city of Philadelphia
\cite{Roge20}.  These data are exceptional because they are restricted
to a single, large city, and because they provide \emph{daily} counts
that capture the detailed temporal pattern (dots in \cref{fig:phila}).
<<philapop, echo=TRUE, include=FALSE>>=
## https://en.wikipedia.org/wiki/Demographics_of_Philadelphia
philapop <- data.frame(year = c(1910,1920), population = c(1549008, 1823779))
##' @param pop data frame
##' @param year year to interpolate to
pop_interp <- function( pop, year ) {
    slope <- (pop[2,2] - pop[1,2])/(pop[2,1] - pop[1,1])
    out <- round( pop[1,2] + (year-pop[1,1])*slope )
    return(out)
}
philapop1918 <- pop_interp( philapop, 1918 )
print(philapop1918)
## reduce this by case fatality proportion so we have
## the initial susceptible population who will be recorded
## as a death if they get infected:
philacfp <- 0.025
philaS0 <- round(philapop1918 * philacfp)
print(philaS0)
@

<<read.phila.data, include=FALSE>>=
## we have to cut the data when we're using nbinom
## because there are long trailing zeroes..
## same with the beginning
phila1918a <- subset(phila1918, as.Date("1918-09-10") < date &
                                date < as.Date("1918-11-18"))
phila1918b <- data.frame(
    date=c(as.Date("1918-09-10"), phila1918a$date),
    mort=c(NA, phila1918a$mort)
)
## avoids dependence on lubridate
## not reliable for leap years!
mk_dec_date <- function(x) {
    p <- as.POSIXlt(x)
    1900 + p$year + p$yday/365
}
phila1918b$time <- mk_dec_date(phila1918b$date)
if (requireNamespace("lubridate")) {
    stopifnot(all.equal(
        phila1918b$time, lubridate::decimal_date(phila1918b$date)))
}
    
@

<<phila.fit,cache=TRUE,warning=FALSE, include=FALSE>>=
## time is in the unit of years
## so gamma=52 corresponds to a mean of 1 week
## not-so-random starting values chosen by hand
start.phila <- c(beta=4e-3, gamma=52*2,
                 S0=philaS0*0.9,
                 I0=3, k=50)
names(start.phila) <- c("beta","gamma","S0","I0", "k")
print(start.phila)

phila_fit <- fitode(
    SIR_model,
    data = phila1918b,
    start = start.phila,
    tcol = "time"
)
ppp <- predict(phila_fit, level=0.95)[[1]]
@

<<philaassign, include=FALSE>>==
coef.phila.fitode <- coef(phila_fit)
ci.phila.fitode <- confint(phila_fit)[,-1]
assignfun(coef.phila.fitode, ".phila.fitode")
@

% Confint on derived parameers:
<<sirphilaconfint, include=FALSE>>==
ci.derived.phila.fitode <- confint(phila_fit,
        parm=list(
            Reff~beta*S0/gamma,
            R0~beta*philaS0/gamma,
            Tg~1/gamma
        ))
assignfun(ci.derived.phila.fitode[,1], ".phila.fitode")
philaparms <- as.list(c(coef.phila.fitode, ci.derived.phila.fitode[,1]))
@

<<philaKMparms, include=FALSE>>=
## get KM approx parms associated with fitode fit:
a.phila <- with(philaparms, a_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
omega.phila <- with(philaparms, omega_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
phi.phila <- with(philaparms, phi_fun(Reff=Reff, gamma=gamma, S0=S0, I0=I0))
KM.approx.phila <- c(a=a.phila, omega=omega.phila, phi=phi.phila)
print(KM.approx.phila)
@

<<adjust philaKMparms, include=FALSE>>=
## adjust because above is giving nls problems:
KM.start.phila <- c(a=a.phila, omega=omega.phila, phi=phi.phila)
print(KM.start.phila)
@

<<phila.nlsfit, include=FALSE>>=
## shift time to start at 0 before passing to nls
df <- phila1918b[,c("time","mort")]
df[,"time"] <- df[,"time"] - df[1,"time"]
phila.nlsfit <- nls(mort ~ KM_approx(time, a, omega, phi),
              data = df,
              start = KM.start.phila)
phila.nls.parameters <- coef(phila.nlsfit)
ci.phila.nls <- confint(phila.nlsfit)
assignfun(phila.nls.parameters, ".phila.nls")
print(phila.nls.parameters)
@

<<philanlsR0, include=FALSE>>==
cc.phila.delta <- KMcifun(phila.nlsfit, I0=I0.phila.fitode, N=philaS0)
orig.params.phila <- invert_params(I0=I0.phila.fitode, coef(phila.nlsfit))
orig.params.phila <- signif(orig.params.phila,dig)
assignfun(orig.params.phila, text = ".phila.nls")
R0.phila.nls <- Reff.phila.nls/S0.phila.nls*philaS0
@


As for Bombay plague, we can fit \KM's approximation \eqref{eq:sech}
to the Philadelphia influenza epidemic using nonlinear least squares,
which yields the red curve in \cref{fig:phila}.  While this \code{nls}
fit does not look unreasonable at a glance, the fitted parameter
values (\cref{tab:philanls}) are absurd, including a basic
reproduction number $\R_0\approx \Sexpr{signif(R0.phila.nls,2)}$ and a
mean generation interval $\Tg\approx \Sexpr{signif(Tg.phila.nls,2)}$
years.

Matching trajectories of the exact SIR model using \code{fitode} gives
a fit---the solid yellow curve in \cref{fig:phila}---that is visually
similar to the (red) fit of \KM's approximation, but provides much
more realistic parameter estimates (\cref{tab:phila.fitode}); in
particular, $\R_0\approx \Sexpr{signif(R0.phila.fitode,2)}$ and $\Tg\approx
\Sexpr{signif(Tg.phila.fitode*365,2)}$ days.

If we convert the \code{fitode} estimates of the SIR parameters to the
parameters of \KM's approximation, we obtain the dotted yellow curve
in \cref{fig:phila}, which grossly underestimates the magnitude of the
epidemic (the epidemic peak occurs much too soon).  The \KM
approximation \eqref{eq:sech} is good initially, but becomes poorer
and poorer over time as the underlying assumption on which it is based
\eqref{eq:KMassumption} becomes less and less valid.

%%\section{Example for which \KM approximation is poor (via stochastic  SIR)}
\section{Fitting the deterministic SIR model to stochastic simulations}\label{sec:stoch}

<<compute-stochastic-SIR-via-sirr, include=FALSE>>=
sirstoch_fn <- system.file("vignette_data", "sirstoch.RData", package = "fitode")
if (file.exists(sirstoch_fn)) {
    load(sirstoch_fn)
} else {
    library(sirr)
    R0 <- 5
    N <- 2000
    mm <- create_SIRmodel(R0=R0, N=N)
    ii <- set_inits(mm)
    iii <- ii
    iii[2] <- exp(iii[2])
    names(iii) <- c("S","I","R")
    ## FIX: this should not be necessary, but it is a bug in sirr
    ##      associated with N != 1.
    mm1 <- create_SIRmodel(R0=R0, N=1)
    sir.det <- compute_SIRts(mm1)
    ## get associated params for KM approx:
    a.det <- a_fun(Reff=R0*iii["S"]/2000, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    omega.det <- omega_fun(Reff=R0*iii["S"]/2000, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    phi.det <- phi_fun(Reff=R0*iii["S"]/2000, gamma=mm$gamma, S0=iii["S"], I0=iii["I"])
    ##
    tt <- 0:10
    sir.stoch <- get_ssa_soln(mm, stopcrit = NULL, inits=ii, times=tt)
    save(R0, N, mm, ii, iii, mm1, sir.det, a.det, omega.det, phi.det, tt,
         sir.stoch, file="sirstoch.RData")
}
@

The most compelling tests of estimation methods involve fitting models
to data that have been generated from a known model, so we know
the true underlying values of the parameters we are trying to estimate.

The most basic test is essentially a consistency check: in the context
of the SIR model, we choose initial conditions $(S_0,I_0)$ and
parameter values ($\R_0,\Tg$), compute the associated trajectory by
solving \cref{eq:SIR} numerically, and then use \code{fitode} to
estimate the parameters.  At least if we choose starting values
reasonably close to the correct underlying values, \code{fitode} should converge
to those values.

The next level of testing is to take our numerically computed
solution and artificially ``observe'' it with error, i.e., using a
noise distribution that we specify.  For example, we could take
observation errors to be negative binomially distributed with
overdispersion parameter $k$, and then use \code{fitode} to estimate
$k$ together with the other parameters ($S_0,I_0,\R_0,\Tg$).

A still more stringent test is to simulate data from a model
that is more complex and realistic than the idealized model that we
want to fit, and then see if we can nevertheless recover parameters
that correspond to those of our idealized
model (e.g., $\R_0$ and $\Tg$ for the SIR model).  We will take a step
in this direction in this section by fitting the deterministic SIR
model \eqref{eq:SIR} to data generated by a fully stochastic version
of the model.

The standard stochastic SIR model \cite{AndeBrit00b} can be defined by
interpreting the individual terms in \cref{eq:SIR} as event rates for
stochastic processes in a population of $N$ individuals (in the limit
$N\to\infty$ the stochastic model approaches the ODEs \eqref{eq:SIR};
see \cite{EthiKurt86}).  Realizations of this discrete-state model can
be generated exactly using the Gillespie algorithm \cite{Gill76}, or
approximately (as we do here) using the ``$\tau$-leaping'' approach
\cite{Gill01}, which is implemented in the \code{adaptivetau} R
package \cite{adaptivetau}.  The demographic stochasticity that these
algorithms simulate is essential to capture real effects that occur
when the number of infected individuals is small (especially the
possibility that an epidemic can burn out \cite{Pars+24}).

In \cref{fig:stoch}, the dots show a single realization of the
stochastic SIR model with initial state $(S_0,I_0,R_0)=(\Sexpr{iii})$,
basic reproduction number $\R_0=\Sexpr{R0}$, and mean generation
interval $\Tg=\gamma^{-1}=\Sexpr{1/mm[["gamma"]]}$ week.  In the top
panel, $\dbydt{R}$ [\cref{eq:SIR;R}] with the correct initial
conditions and parameter values is shown with solid green, and the \KM
approximation \eqref{eq:sech} based on those parameter values is shown
with dotted green.  The \code{fitode} fit [based on $\int (\dbydt{R})
\dt$] and confidence band are shown in yellow.  The time shift between
the deterministic solution and the stochastic realization arises
because the stochastic model captures the demographic noise (which
causes a randomly distributed delay until the tipping point is
reached, i.e., until the epidemic takes off in a roughly deterministic
fashion).

As expected, with the correct parameter values, \KM's approximation
\eqref{eq:sech} fails once the requirement \eqref{eq:KMassumption}
that $R(t)/N\ll1/\R_0$ is violated.  We can, of course, find values of
the parameters $(a,\omega,\phi)$ such that the function
$a\sech^2{(\omega\,t - \phi)}$ [\cref{eq:sech}] more closely matches
the shape of the full simulated epidemic.  Using nonlinear least
squares (\code{nls}) as in previous sections, we obtain visually
reasonable agreement (\cref{fig:stoch}, bottom panel, red curve;
\cref{tab:stoch.nls}). This \code{nls} fit cannot be improved further
because the function we are fitting \eqref{eq:sech} is symmetric about
its peak, whereas the rise is steeper than the fall in the simulated
epidemic.  It is also worth emphasizing that the parameter values that
yield the red curve in \cref{fig:stoch} are far from the true
parameters that were used in the simulation (\cref{tab:stoch.nls}).

The excellent fit of the deterministic trajectory that \code{fitode}
finds (yellow) is achieved by estimating an initial prevalence that is
only a third of the true initial prevalence, thereby mimicking the
stochastic delay with the deterministic model; all other parameter
estimates are nearly identical to the true parameter values used to
generate the stochastic trajectory (\cref{tab:stoch.fitode}).

<<colours, include=FALSE>>=
my.red <- "#E41A1C"     # colour-blind friendly red
my.blue <- "#377EB8"    # colour-blind friendly blue
my.green <- "#4DAF4A" # colour-blind friendly green
my.yellow <- "#FFD92F" # colour-blind friendly yellow
my.orange <- "#D55E00" # colour-blind friendly orange
@

<<sirstochdata, include=FALSE>>==
## convert to daily mortality
mort.stoch <- diff(sir.stoch$R[!duplicated(ceiling(sir.stoch$time*7),fromLast = TRUE)])
mort.time <- ceiling(sir.stoch$time[!duplicated(ceiling(sir.stoch$time*7),fromLast = TRUE)]*7)
mort.which <- which(diff(mort.time)==1)

sir.stoch.data <- data.frame(
    time=mort.time[mort.which]/7,
    mort=c(NA, mort.stoch[head(mort.which, -1)])
)
@

<<sirnlsfit, cache=TRUE, warning=FALSE, include=FALSE>>==
nlsfit.stoch <- nls(mort ~ KM_approx(time, a, omega, phi),
              data = sir.stoch.data,
              start = c(a = unname(a.det), omega = unname(omega.det),
                        phi = unname(phi.det)))

nls.stoch.parameters <- coef(nlsfit.stoch)
assignfun(nls.stoch.parameters, text=".nls.stoch")

orig.params.KM.stoch <- invert_params(I0 = exp(ii[[2]]), params = c(a=unname(a.det), phi=unname(phi.det), omega=unname(omega.det)))
orig.params.KM.stoch <- signif(orig.params.KM.stoch)
assignfun(orig.params.KM.stoch, ".KM.stoch")

orig.params.nls.stoch <- invert_params(I0 = exp(ii[[2]]), params = coef(nlsfit.stoch))
orig.params.nls.stoch <- signif(orig.params.nls.stoch)
assignfun(orig.params.nls.stoch, ".nls.stoch")

cc.nls.stoch <- suppressMessages(confint(nlsfit.stoch))

cc.delta.stoch <- KMcifun(nlsfit.stoch, I0 = exp(ii[[2]]), N=N)
assignfun(cc.delta.stoch[,"2.5%"], text=".lwr.nls.stoch")
assignfun(cc.delta.stoch[,"97.5%"], text=".upr.nls.stoch")

@

<<sirstochfit, cache=TRUE, warning=FALSE, include=FALSE>>=
start.stoch <- c(beta=mm$beta, gamma=mm$gamma,
                 S0=iii["S"], I0=iii["I"])
names(start.stoch) <- c("beta","gamma","S0","I0")
print(start.stoch)

SIR_stoch_fit <- fitode(
    SIR_model,
    data = sir.stoch.data,
    start = c(start.stoch, k=10),
    # fixed = c(gamma = 1), # Tg = 1/gamma = 1 week
    tcol = "time"
)
sss <- predict(SIR_stoch_fit, level=0.95)[[1]]
summary(SIR_stoch_fit)
@

<<stochestassign, include=FALSE>>==
coef.stoch.fitode <- coef(SIR_stoch_fit)
ci.stoch.fitode <- confint(SIR_stoch_fit)[,-1]
assignfun(coef.stoch.fitode, ".stoch.fitode")
@

% Confint on derived parameers:
<<sirstochconfint, include=FALSE>>==
ci.derived.stoch.fitode <- confint(SIR_stoch_fit,
        parm=list(
            Reff~beta*S0/gamma,
            R0~beta*2000/gamma
        ))
assignfun(ci.derived.stoch.fitode[,1], ".stoch.fitode")
@

\section{Discussion}

We have presented a basic theoretical and practical introduction to
standard methods for fitting dynamical models to time series, in the
context of infectious disease epidemiology.  We explained how to use
nonlinear least squares (\code{nls}) to fit a given function to a time
series, and illustrated the approach using the Kermack and McKendrick
(\KM, \cite{KermMcKe27}) analytical approximation \eqref{eq:sech} to
the solution of the standard SIR model \eqref{eq:SIR}.  We also
explained how to fit solutions of ordinary differential equations
(ODEs) to a time series---using our R package \code{fitode}---and
obtain parameter estimates and confidence intervals, regardless of
whether analytical solutions of the ODEs are available.

\code{fitode} is flexible enough to handle most compartmental
epidemiological and ecological models
\cite{BrauCast01,BrauKrib16,Brau+19}, including non-autonomous models,
such as seasonally forced epidemic models
\cite{LondYork73,Earn+00,HeEarn07,HeEarn16,PapsEarn19}.  We hope the
package will be useful for many readers, not only as a pedagogical
tool but also to fit models to novel data.  Potential applications
abound (we have ourselves used \code{fitode}'s predecessor,
\code{fitsir}, to study music popularity \cite{Rosa+21}).

We focused here on three illustrative examples of epidemic time
series.  The first was the reported weekly mortality from plague in
Bombay in 1906 (\cref{fig:Bombay}), which was examined by \KM in their
original paper \cite{KermMcKe27}.  Although historically important, it
is certainly debatable whether any inferences we might draw from
fitting the simple SIR model \eqref{eq:SIR} to these plague data can
be trusted.  As we quoted at the end of \cref{sec:fitode}, to justify
the application of their SIR model to these data, \KM highlighted five
implicit assumptions, any or all of which might be violated.  In
addition, Baca\"er investigated the longer term pattern of plague
mortality in Bombay and found that there were seasonal epidemics every
year from 1897 to 1911 \cite[Fig.\,2]{bacaermodel2012}, suggesting
that the 1906 epidemic was just one in a long sequence of epidemics
that were ``driven by seasonality'' \cite[p.\,403]{bacaermodel2012}.
Of course, other mechanisms (e.g., heterogeneity in contact patterns)
might play a role as well.

%\djde{The strict seasonality in Baca\"er's Figure 2 is very
%%   compelling.  This could happen from demographic stochasticity
%%   sustaining transients, but only if the damping period happens to be
%%   exactly one year.  I don't know if we can constrain the parameters
%%   sufficiently to rule that out (and we would need vital dynamics
%%   and/or decay of immunity to even try), but an exactly annual period
%%   is much more likely to arise from seasonal forcing.  Digging into
%%   this seems like a rabbit hole that is well outside the intended
%%   scope of this paper; it would certainly have to involve fitting all
%%   15 annual epidemics that Baca\"er shows and figuring out what other
%%   useful information is in all those reports that he cites.  But it
%%   would be fun to investigate, perhaps in another paper.}
%% \swp{Definitely another paper...}

To obtain a deeper understanding of the Bombay plague epidemic, we
could formulate a variety of models, fit them to the data using
\code{fitode} or other software, and use a statistical framework for
model selection \cite{BurnAnde02} to rank the relative importance
of the various mechanisms included in the sequence of models (see,
e.g., He \emph{et al.} \cite{He+13a} for an example of using this
approach to understand the occurrence of three distinct waves in the
1918 influenza pandemic. (Alternatively, we could formulate one model
that included \emph{all} of the processes and attempt to measure
their relative importance by comparing the magnitudes of parameters
\cite{Bolk23}.)
We have not attempted such a study here, since our goal was simply to
explain and illustrate the fitting methodology.  However, it is worth
highlighting that our analysis using the SIR model did reveal a
computational challenge that---in the absence of additional
information about the Bombay plague outbreak---would likely limit how
much can be learned from a model selection exercise: the mean
generation interval ($\Tg$) appears to be \term{unidentifiable}, i.e.,
difficult to estimate reliably from the reported weekly plague deaths
alone (see \cref{fig:proflikgamma}).

Our second example was the main wave of the 1918 influenza pandemic in
the city of Philadelphia, for which daily mortality from pneumonia and
influenza (P\&I) was reported (\cref{fig:phila}).  Again we fitted the
exact solution of the SIR model \eqref{eq:SIR} using \code{fitode},
and \KM's analytical approximation \eqref{eq:sech}, but found---unlike
the situation for Bombay plague---that only the \code{fitode} fit
yielded plausible parameter estimates (see
\cref{tab:phila.fitode,tab:philanls}).

Finally, we conducted a kind of test that truly makes most sense to
perform \emph{before} fitting to a real, empirically observed time
series: we fit models to a simulation that we ran, so we knew
the parameter values used to generate the simulated ``observations''.
The simulation was a realization of the stochastic SIR model, to
which, again, we fit both the deterministic SIR model \eqref{eq:SIR}
using \code{fitode} and \KM's analytical approximation \eqref{eq:sech}
using \code{nls}.  Both provide visually reasonable fits
(\cref{fig:stoch}) but \KM's approximation yields absurd parameter
values, whereas \code{fitode} estimates the correct values of the
underlying disease-related parameters
(\cref{tab:stoch.fitode,tab:stoch.nls}).
(We did find a discrepancy in the estimates of
initial conditions; this was driven by the failure of
the stochastic outbreak simulation to take off immediately.
A lower initial prevalence is the only mechanism by which
the deterministic model can capture the delayed onset of the epidemic.
In practice, modelers fitting to epidemic time series by trajectory
matching usually pick an ``epidemic window'' that corresponds to
the part of the epidemic that can be reasonably captured by
a deterministic model \cite{Earn+20}.)

\KM's approximation \eqref{eq:sech} estimates the simulation
parameters badly because the assumption on which
it is based \eqref{eq:KMassumption} is strongly
violated in the simulation (\cref{fig:stoch}).  Consequently, the
parameters of the \KM approximation cannot be interpreted biologically
or mechanistically.  More generally, a purely
phenomenological model with the same number of parameters can
sometimes fit a stochastic simulation just as closely or even closer
than the deterministic limit of the model that generated the data
\cite{Rosa+21}; a good fit is not, on its own, sufficient to conclude
that a model matches the underlying processes of a dynamical system.

Beyond the basics that we have discussed here, \code{fitode} contains
a number of useful advanced features. In particular,
\code{fitode} can
\begin{description}
\item [fit to multiple data streams:] \code{fitode} is not
  limited to fitting a trajectory to a single state variable, such
  as incidence or prevalence of infected individuals. For example,
  during the later stages of the COVID-19 pandemic modelers often
  had access to time series of case reports, hospitalization reports, and
  wastewater sampling for the same geographic region. If we
  build a model that includes state variables for hospitalized individuals
  and for virus concentrations in wastewater, \code{fitode} can fit
  the model's parameters using all of the available data (as in \cite{Nour+2022}).
\item [compute confidence intervals via importance sampling:]
  While the Delta method can compute confidence intervals for derived
  quantities such as predicted trajectories, it rests on strong
  and sometimes unreliable assumptions. A more accurate
  but computationally expensive approach starts by
  sampling parameter sets randomly from a multivariate normal
  distribution with a mean and covariance matrix drawn from
  the maximum likelihood fit. For each set of parameters in
  the ensemble, \code{fitode} computes the likelihood and
  a predicted trajectory (or some quantity such as the total
  size of the epidemic); an average value and confidence intervals
  are derived from weighted moments (means) or quantiles (medians
  or extremes such as 10th and 90th percentiles).
\item [apply Bayesian inference:]
  Unlike maximum likelihood approaches, which seek to estimate the best-fitting
  parameter set, Bayesian methods aim to estimate a distribution of
  parameters (also known as the posterior distribution) that are consistent with our
  previous knowledge about the system (encapsulated in \emph{prior distributions}) as well
  as the observed data.
  These approaches are generally
  better at handling parameter uncertainties \citep{elderd2006uncertainty}
  but are usually much more computationally expensive.

  \code{fitode} allows the user to specify prior distributions on
  parameters; these priors
  can either reflect previous knowledge of a disease system, or can be used to
  \emph{regularize} a fitting procedure by downweighting extreme values of parameters
  \cite{Lemo2019}, which can help mitigate problems with identifiability (see below).

  Bayesian modelers typically use \emph{Markov chain Monte Carlo} algorithms
  to explore the parameter space and approximate the target distribution. \code{fitode} implements
  a simple \emph{Metropolis-Hasting} sampler \citep[\S7.3.1]{Bolk08}.
  (The Stan platform provides a much more powerful Bayesian sampling algorithm
  using sensitivity equations,
  built on top of a fully general system for specifying ODEs; however, this
  tool requires significantly more computational and statistical background to
  use effectively \cite{Grin+2021}.)

\end{description}

Even with these extensions, modelers may face many challenges when
fitting ODEs to data with the \code{fitode} package, as with
fitting any nonlinear model to data.  For example, it is often
difficult to ensure that the model has converged properly or reached
its true maximum.  More generally, naive and optimistic epidemic modelers often run into problems of \term{structural identifiability} (the impossibility of estimating particular sets of parameters from data, regardless of how much data is available \citep{TuncTrang2018,Chow+2023}) and \term{practical identifiability} (the impossibility of reliably estimating parameters from a particular small, noisy data set \cite{Gall+2022, Chow+2023}) when they first start attempting to fit models to data.
In addition to the rigorous methods described by \cite{Chow+2023}, using a \term{multistart method} (performing optimization from multiple starting conditions: \cite{raue2013lessons}), or plotting likelihood surfaces, can help diagnose these problems.  Using different optimization methods or reparameterizing the model can also help \cite{raue2013lessons,Bolk+2013}.  We encourage users of \code{fitode} who encounter these or other
fitting challenges to open issues via the \code{fitode}
GitHub repository (\url{https://github.com/parksw3/fitode}).

As its name suggests, \code{fitode} is limited to fitting ODEs to time
series.  Consequently, by design, \code{fitode} ignores \term{process
  error}, i.e., random variability that affects both current and
future steps of the trajectory---as opposed to \term{observation
  error}, which arises from imperfect measurements or reporting and is
usually assumed to be independent of the trajectory itself.  A key
component of process error is the demographic stochasticity that is
inherent to the discrete-state stochastic SIR model discussed above
(and to any real host-pathogen system).  Parameters of models can also
be subject to process error; for example, the transmission rate might
depend on random fluctuations in weather.  Properly accounting for
process error can be critical for estimating uncertainties in
parameter estimates and confidence bands on the projected dynamics of
a system \citep{King+15,taylor2016stochasticity,li2018fitting}.
Popular \code{R} packages that can fit models with process error
include \code{pomp} \citep{King+16} and \code{mcstate}
\citep{mcstate}.

\section{Closing remarks: from Fred Brauer to \code{fitode}}

The idea of digging into to data seemed like punishment to Fred
Brauer, but while he never---to our knowledge---did any data analysis
himself, he did develop a sincere appreciation for the value of data
in epidemiological research.  Fred's curiosity---about how dynamical
models can be fit to data, and why it is hard---convinced us that it
would be worth writing a paper (and accompanying software) that could
draw more dynamicists working on epidemic models into the world of
data.

We have provided two answers to Fred's question of ``how'' to fit
models to data (via \code{nls} or \code{fitode}), and through examples
we have hinted at some of the reasons ``why'' such fitting can be very
difficult.  A true understanding of ``why it is hard'' is something
that builds over time with experience, but the key points are that
finding optima of a complex multi-dimensional function is hard enough
on its own \citep{raue2013lessons}, and estimating statistically
meaningful uncertainty in those optima is extremely
challenging \citep{elderd2006uncertainty,li2018fitting}.

Fred would never have used \code{fitode}, but would have delighted in
seeing it demonstrated and in discussing the theoretical background on
model fitting that we have presented in this paper.  We hope that
others like him, as well as students and researchers who actually do
want to dig into data, will benefit from this exposition.

\bibliographystyle{tfs}
\bibliography{brauer-ms,fitode}

<<figfuns, echo=FALSE>>=
lwd <- 5
col.KM <- my.blue
col.nls <- my.red
col.fitode.nb <- my.yellow
col.fitode.ols <- "grey80" # "orange" # my.orange
transparent_colour <- function(col,alpha=150/255) {
    alpha <- round(alpha * 255)
    v <- col2rgb(col)[,1] # color as rgb vector
    tcol <- rgb(v["red"],v["green"],v["blue"],alpha=alpha,maxColorValue = 255)
    return(tcol)
}
col.fitodenbCI <- transparent_colour(col.fitode.nb, alpha=0.4)
col.fitodeCI <- transparent_colour(col.fitode.ols, alpha=0.4)
col.nlsCI <- transparent_colour(col.nls, alpha=0.4)
setup_plot <- function(xlab="", ylab="deaths",
                       at=100*(0:10), ...) {
    plot(NA, NA , bty="L", type="n",
         xaxs="i", yaxs="i", las=1,
         yaxt="n",
         xlab=xlab, ylab=ylab,
         ...
         )
    axis(side=2, at=at, las=1)
}
draw_confband <- function(confband, col = col.fitodenbCI) {
    with(confband,{
        polygon(
            x = c(times, rev(times)),
            y = c(`2.5 %`, rev(`97.5 %`)),
            col = col,
            border = NA,
            xpd = NA
        )
    })
}
draw_legend <- function(lwd=5, pt.bg="white") {
    legend("topleft", bty="n", lwd=c(2,lwd,lwd,lwd*0.60),
       lty=c(NA,"solid","solid","solid"),
       col=c("black",col.KM,col.nls,col.fitode.nb),
       pch=c(21,NA,NA,NA),
       pt.bg=c(pt.bg,NA,NA,NA),
       legend=c("observed data","KM","nls","fitode (nbinom)"))
}
@

<<testing, include=FALSE, results = "hide">>=
format_sn0(cc["a", 1])
ci_fmt("a", cc)
@

%%\begin{landscape}
\begin{table}
  \begin{center}
    \caption{\textbf{Fits of \KM's \cite{KermMcKe27} analytical SIR
      approximation \eqref{eq:sech} to Bombay plague} (see
      \cref{fig:Bombay}).  The \KM column lists the parameter values
      estimated by \KM \cite[p.\,714]{KermMcKe27}; the \code{nls}
      column lists the values estimated by us, using nonlinear least
      squares with confidence intervals obtained by the Delta method
      (see \cref{sec:Uncertainty}).  Values for the initial prevalence
      $I_0$ and population size $N$ are assumed in order to derive
      estimates of the standard SIR model parameters from the
      parameters of \KM's approximation (using the indicated
      equations).  Like Baca\"er \cite[p.\,408]{bacaermodel2012}, we
      assume the population of Bombay was $N=1$ million.
      \bmb{Could also play with formatting here to narrow the table
        slightly, as in Table 2, but
        this one is only slightly too wide \ldots (include symbol in
        parentheses at the end  of the parameter-name text here and in next fig?)}
    }\label{tab:Bombay}
\medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c | c | c c}
  \bfseries Estimated\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries equation} & {\footnotesize\bfseries units}
    & \thead{\footnotesize\bfseries KM \\ \bfseries estimate} & \thead{\footnotesize\bfseries \code{nls} \\ \bfseries estimate} & {\footnotesize\bfseries 95\% CI} \\\hline
    peak removal rate & $a$& \eqref{eq:a} & $\frac{1}{\textrm{weeks}}$ & \Sexpr{a.KM} &
      \Sexpr{signif(a.nls,dig)} & \Sexpr{ci_fmt("a", cc)} \\
    outbreak speed & $\omega$ & \eqref{eq:omega} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{omega.KM} & \Sexpr{signif(omega.nls,dig-1)} & \Sexpr{ci_fmt("omega", cc)} \\
    outbreak centre & $\phi$ & \eqref{eq:phi} & -- & \Sexpr{phi.KM} &
      \Sexpr{signif(phi.nls,dig)} & \Sexpr{ci_fmt("phi", cc)} \\
    \noalign{\vspace{10pt}}
    \bfseries Assumed\break parameter \\\hline
    initial prevalence & $I_0$ & -- & -- &
      \Sexpr{I0.KM} & \Sexpr{I0.nls} & -- \\
      population size & $N$ & -- & -- & $10^6$ & $10^6$ & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Derived\break parameter \\\hline
    peak time & $\tpeak$ & \eqref{eq:tpeak} & weeks &
      \Sexpr{tpeak.KM} & \Sexpr{tpeak.nls} & \Sexpr{ci_fmt("tpeak", cc.delta)} \\
    effective reproduction number & $\Reff$
      & \eqref{eq:Reffdef}, \eqref{eq:Reff} & -- &
      \Sexpr{Reff.KM} & \Sexpr{Reff.nls} & \Sexpr{ci_fmt("Reff", cc.delta)} \\
    removal rate & $\gamma$ & \eqref{eq:gamma} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{gamma.KM} & \Sexpr{gamma.nls} & \Sexpr{ci_fmt("gamma", cc.delta)} \\
    initial\break susceptibles & $S_0$ & \eqref{eq:S0} & -- &
      \Sexpr{as.integer(S0.KM)} & \Sexpr{as.integer(S0.nls)} & \Sexpr{ci_fmt("S0", cc.delta, max_ord = 6)} \\
    transmission rate & $\beta$ & \eqref{eq:beta} & $\frac{1}{\textrm{years}}$ &
      \Sexpr{signif(beta.KM*52,dig)} & \Sexpr{signif(beta.nls*52,dig)} & \Sexpr{ci_fmt("beta", cc.delta*52, sn=FALSE, digits=dig)} \\
    mean generation interval & $\Tg$ & \eqref{eq:Tg} & days &
      \Sexpr{signif(7*Tg.KM,dig)} & \Sexpr{signif(7*Tg.nls,dig)} & \Sexpr{ci_fmt("Tg", 7*cc.delta)} \\
    basic reproduction number & $\R_0$ & \eqref{eq:R0def} & -- &
      \Sexpr{signif(10^6*Reff.KM/S0.KM,dig)}
      & \Sexpr{signif(10^6*Reff.nls/S0.nls,dig)} & \Sexpr{ci_fmt("R0", cc.delta)}
  \end{tabular}
  \end{center}
\end{table}
%%\end{landscape}


%%\begin{landscape}
\begin{table}
  \begin{center}
    \caption{\textbf{Fits of numerical SIR model solutions to Bombay plague}
      (see \cref{fig:Bombay}).  Parameter values were estimated using
      \code{fitode} to fit trajectories of \cref{eq:SIR}, assuming
      observation errors were distributed negative binomially
      (\code{nbinom}) or normally (\code{ols}).  The recovery rate
      $\gamma$ was fixed rather than fitted due to parameter
      unidentifiability (see \cref{foot:fixedgamma}); we fixed
      $\gamma$ to the value obtained from our \code{nls} fit of the
      \KM approximation [\cref{tab:Bombay}] for the purpose of fair
      comparison of the fits.
      %%
      The \code{fitode}-fitted trajectories and confidence bands---for
      both \code{nbinom} and \code{ols}---are shown in the lower panel
      of \cref{fig:Bombay}.
      %%
      As in \cref{tab:Bombay}, a population size $N$ must be assumed
      to derive $\R_0$ estimates.
      \bmb{tweaked table to make it fit within margins.
      (See comment above at
      \texttt{\textbackslash usepackage\{makecell\}} for breaking lines
      in header columns).
      Would be nice to tweak 95\% CI columns slightly
      wider but we're probably at the limit there.
      Could add an
      appropriate (optional) \texttt{\textbackslash hspace} in
      \texttt{ci\_fmt()} to align line-broken CIs more nicely ... ?
      Maybe not worth fussing too much over table formatting since
      typesetters will be redoing it anyway?
      }
    }\label{tab:bombay.fitode}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c m{1.8cm} | c m{1.8cm}}
  \bfseries Fixed\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries units}
    & \thead{\footnotesize\bfseries \code{nbinom}\\ estimate} & {\footnotesize\bfseries 95\% CI} & \thead{\footnotesize\bfseries \code{ols}\\ estimate} & {\footnotesize\bfseries 95\% CI} \\\hline
    recovery rate & $\gamma$ & $\frac{1}{\textrm{weeks}}$ & \Sexpr{gamma.nls} &
      -- & \Sexpr{gamma.nls} & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Estimated\break parameter \\\hline
    transmission rate & $\beta$ & $\frac{1}{\textrm{years}}$ &
      \Sexpr{beta.bombay.fitode.nb*52} & \Sexpr{ci_fmt("beta", ci.bombay.fitode.nb*52, sn=FALSE, digits=dig)}  & \Sexpr{beta.bombay.fitode.ols*52} & \Sexpr{ci_fmt("beta", ci.bombay.fitode.ols*52, sn=FALSE, digits=dig)}  \\
    initial\break susceptibles & $S_0$& -- &
      \Sexpr{as.integer(S0.bombay.fitode.nb)} & \Sexpr{ci_fmt("S0", ci.bombay.fitode.nb, max_ord = 6)} & \Sexpr{as.integer(S0.bombay.fitode.ols)} & \Sexpr{ci_fmt("S0", ci.bombay.fitode.ols, max_ord = 6)}\\
      initial prevalence & $I_0$ & -- &
      \Sexpr{I0.bombay.fitode.nb} & \Sexpr{ci_fmt("I0", ci.bombay.fitode.nb)} & \Sexpr{I0.bombay.fitode.ols} & \Sexpr{ci_fmt("I0", ci.bombay.fitode.ols)} \\
      overdispersion\break parameter & $k$ & -- &
      \Sexpr{k.bombay.fitode.nb} & \Sexpr{ci_fmt("k", ci.bombay.fitode.nb)} & -- & -- \\
    \noalign{\vspace{10pt}}
    \bfseries Assumed\break parameter \\\hline
    population size & $N$ & -- & $10^6$ & -- & $10^6$ & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Derived\break parameter \\\hline
      effective reproduction number & $\Reff$ & -- &
      \Sexpr{Reff.bombay.fitode.nb} &  \Sexpr{ci_fmt("Reff", ci.derived.bombay.fitode.nb[,-1])} & \Sexpr{signif(beta.bombay.fitode.ols*as.integer(S0.bombay.fitode.ols)/gamma.nls,dig)}  &  \Sexpr{ci_fmt("Reff", ci.derived.bombay.fitode.ols[,-1])} \\
      mean generation interval & $\Tg$ & days &
      \Sexpr{signif(7*Tg.bombay.fitode.nb,dig)} & -- & \Sexpr{signif(7/gamma.nls,dig)} & -- \\
    basic reproduction number & $\R_0$ & -- &
      \Sexpr{signif(R0.bombay.fitode.nb,dig)}
      &  \Sexpr{ci_fmt("R0", ci.derived.bombay.fitode.nb[,-1])} & \Sexpr{signif(beta.bombay.fitode.ols*1e6/gamma.nls, dig)} &  \Sexpr{ci_fmt("R0", ci.derived.bombay.fitode.ols[,-1])}
  \end{tabular}
  \end{center}
\end{table}
%%\end{landscape}

\begin{figure}
\centering
<<Bombay-figure, echo=FALSE, fig.height=5, fig.show="hold", out.width="95%">>=
tvals <- seq(0, tmax, length.out=1000)
setup_plot(ylim = c(0, 1000), xlim=c(0,tmax),
         xlab="weeks", ylab="plague deaths")
draw_confband(traj_ci(tvals, nls.parameters, vcov(nlsfit)), col=col.nlsCI)
##DE: skipping fitode confband because there is too much overlap:
##draw_confband(SIR_confband, col=col.fitodenbCI)
curve(KM_approx(x, a=a.KM, omega=omega.KM, phi=phi.KM), from=0, to=tmax, n=1000, add=TRUE,
      lwd=lwd, xpd=NA, col=col.KM)
curve(KM_approx(x, a=a.nls, omega=omega.nls, phi=phi.nls), from=0, to=tmax, n=1000, add=TRUE,
      lwd=lwd, xpd=NA, col=col.nls)
lines(estimate ~ times, data=SIR_confband, col=col.fitode.nb, lwd=lwd*0.60, lty="dotted")
points(mort ~ week, data = fitode::bombay, xpd=NA, pch=21, bg="white", lwd=2)
legend("topleft", bty="n", lwd=c(2,lwd,lwd,lwd*0.60),
       lty=c(NA,"solid","solid","dotted"),
       col=c("black",col.KM,col.nls,col.fitode.nb),
       pch=c(21,NA,NA,NA),
       pt.bg=c("white",NA,NA,NA),
       legend=c("observed data","KM's (1927) fit","our nls fit","fitode (nbinom)"))

setup_plot(ylim=c(0, 1000), xlim=c(0,tmax), #DE: ymin was 5, not sure why
         xlab="weeks", ylab="plague deaths"
         ##, at=c(25, 50, 100, 200, 400, 800)
         )
draw_confband(SIR_confband, col=col.fitodenbCI)
draw_confband(SIR_ols_confband, col=col.fitodeCI)
lines(estimate ~ times, data=SIR_confband, col=col.fitode.nb, lwd=lwd)
lines(estimate ~ times, data=SIR_ols_confband, col=col.fitode.ols, lwd=lwd*0.60)
curve(KM_approx(x, a=a.nls, omega=omega.nls, phi=phi.nls), from=0, to=tmax, n=250, add=TRUE,
      lwd=lwd*0.60, xpd=NA, col=col.nls, lty="dotted")
points(mort ~ week, data = fitode::bombay, xpd=NA, pch=21, bg="white", lwd=2)
legend("topleft", bty="n", lwd=c(2,lwd,lwd*0.60,lwd*0.6),
       lty=c(NA,"solid","solid","dotted"),
       col=c("black",col.fitode.nb,col.fitode.ols,col.nls),
       pch=c(21,NA,NA,NA),
       pt.bg=c("white",NA,NA,NA),
       legend=c("observed data","fitode (nbinom)", "fitode (ols)", "our nls fit"))
@
\caption{\textbf{The plague epidemic in Bombay}, 17 December 1905 to 21 July
  1906, used as an example by KM \cite[p.\,714]{KermMcKe27}.  The data
  (dots) were digitized from \cite[Table~IX, p.\,753]{jhyg1907}.
  \emph{Top panel:} The \KMcol and red curves show the \KM
  approximation \eqref{eq:sech}, as fitted by \KM (\KMcol) and by us
  using \code{nls} (red, with pink confidence band estimated using the
  Delta method; see \cref{sec:Uncertainty}).  The associated parameter
  estimates are given in \cref{tab:Bombay}.  The dotted yellow curve
  shows the \code{fitode} fit of the SIR model \eqref{eq:SIR}, for
  which the associated parameter estimates are given in
  \cref{tab:bombay.fitode} [observation errors are assumed to be
  negative binomially distributed \eqref{eq:NB}].
  %%
  \emph{Bottom panel:} The solid yellow curve is identical to the
  dotted yellow curve in the top panel; the light yellow band is the
  \code{fitode} confidence band obtained by the Delta method [the band
    is shown as a linear interpolation between successive observation
    times because the model \eqref{eq:SIR} is fitted to incidence at
    discrete time points rather than to a continuous curve
    representation of the instantaneous death rate].  The
  \colfitodeols curve shows the \code{fitode} fit obtained by
  minimizing the ordinary least squares \eqref{eq:leastsquares} [\ie
    assuming observation errors are normally
    \eqref{eq:prob.data.given.model} distributed with variance
    $\sigma^2$ estimated from the residuals across all observation
    times].  The dotted red curve is identical to the solid red curve
  in the top panel.  We have separated the two panels because the
  confidence band overlap would make the plots difficult to
  interpret.}
\label{fig:Bombay}
\end{figure}

%%\begin{landscape}
\begin{table}
  \begin{center}
    \caption{\textbf{Fits of \KM's \cite{KermMcKe27} analytical SIR
      approximation \eqref{eq:sech} to Philadelphia flu} (see
      \cref{fig:phila}).  Parameter estimates were obtained using
      nonlinear least squares (\code{nls}) to fit \cref{eq:sech} to
      the reported daily pneumonia and influenza (P\&I) mortality
      during the main wave of the pandemic in 1918.  In order to
      derive estimates of the standard epidemiological parameters, we
      assumed the initial prevalence had the value estimated by
      \code{fitode} for the SIR model (see \cref{tab:phila.fitode}).
      %%
      We do not use the raw population size in our estimate of $\R_0$;
      instead, we account for the fact that reported deaths are
      roughly equal to incidence times the case fatality proportion
      (CFP) by taking $N$ to be the size of population that would
      eventually die if everyone in the city were infected, \ie the
      product of the population size of Philadelphia in 1918
      (\Sexpr{prettyNum(philapop1918, big.mark=",")}) and an assumed
      CFP of $\Sexpr{philacfp}$ \cite{TaubMore06}.
      %%
      The fitted trajectory and confidence band are shown in
      \cref{fig:phila}.  See \cref{sec:phila}.}
      \label{tab:philanls}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c | c c}
    \bfseries Estimated\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries equation} & {\footnotesize\bfseries units}
    & {\footnotesize\bfseries \code{nls}} & {\footnotesize\bfseries 95\% CI} \\\hline
    peak removal rate & $a$& \eqref{eq:a} & $\frac{1}{\textrm{years}}$ &
      \Sexpr{signif(a.phila.nls,dig)} & \Sexpr{ci_fmt("a", ci.phila.nls)} \\
    outbreak speed & $\omega$ & \eqref{eq:omega} & $\frac{1}{\textrm{years}}$ &
      \Sexpr{signif(omega.phila.nls,dig-1)} & \Sexpr{ci_fmt("omega", ci.phila.nls)} \\
    outbreak centre & $\phi$ & \eqref{eq:phi} & -- &
      \Sexpr{signif(phi.phila.nls,dig)} & \Sexpr{ci_fmt("phi", ci.phila.nls)} \\
    \noalign{\vspace{10pt}}
    \bfseries Assumed\break parameter \\\hline
    initial prevalence & $I_0$ & -- & -- &
      \Sexpr{I0.phila.fitode} & -- \\
    effective population size & $N$ & -- & -- &
      \Sexpr{prettyNum(philaS0, big.mark=",")} & -- \\
    \noalign{\vspace{10pt}}
    \bfseries Derived\break parameter \\\hline
    peak time & $\tpeak$ & \eqref{eq:tpeak} & weeks &
      \Sexpr{tpeak.phila.nls*52} & \Sexpr{ci_fmt("tpeak", cc.phila.delta*52)} \\
    effective reproduction number & $\Reff$
      & \eqref{eq:Reffdef}, \eqref{eq:Reff} & -- &
      \Sexpr{Reff.phila.nls} & \Sexpr{ci_fmt("Reff", cc.phila.delta)} \\
    removal rate & $\gamma$ & \eqref{eq:gamma} & $\frac{1}{\textrm{years}}$ &
      \Sexpr{gamma.phila.nls} & \Sexpr{ci_fmt("gamma", cc.phila.delta)} \\
    initial\break susceptibles & $S_0$ & \eqref{eq:S0} & -- &
       \Sexpr{as.integer(S0.phila.nls)} & \Sexpr{ci_fmt("S0", cc.phila.delta, max_ord = 6)} \\
    transmission rate & $\beta$ & \eqref{eq:beta} & $\frac{1}{\textrm{years}}$ &
       \Sexpr{beta.phila.nls} & \Sexpr{ci_fmt("beta", cc.phila.delta)} \\
    mean generation interval & $\Tg$ & \eqref{eq:Tg} & years &
      \Sexpr{signif(Tg.phila.nls,dig)} & \Sexpr{ci_fmt("Tg", cc.phila.delta)} \\
    basic reproduction number & $\R_0$ & \eqref{eq:R0def} & -- &
       \Sexpr{signif(R0.phila.nls,dig)} & \Sexpr{ci_fmt("R0", cc.phila.delta, sn=FALSE, digits=dig)}
  \end{tabular}
  \end{center}
\end{table}
%%\end{landscape}

%%\begin{landscape}
\begin{table}
  \begin{center}
    \caption{\textbf{Fits of numerical SIR model solutions to Philadelphia flu}
      (see \cref{fig:phila}).
      Parameter estimates are based on \code{fitode} fits of the SIR
      model \eqref{eq:SIR} to reported P\&I mortality during the main
      wave of the 1918 influenza pandemic in the city of Philadelphia.
      As in \cref{tab:philanls}, in order to derive an estimate of
      $\R_0$, we assume an effective population size that accounts for
      the data representing deaths rather than cases.
    }\label{tab:phila.fitode}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c c}
  \bfseries Estimated\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries units}
    & {\footnotesize\bfseries \code{nbinom}} & {\footnotesize\bfseries 95\% CI} \\\hline
    transmission rate & $\beta$ & $\frac{1}{\textrm{years}}$ &
      \Sexpr{beta.phila.fitode} & \Sexpr{ci_fmt("beta", ci.phila.fitode)} \\
      recovery rate & $\gamma$ & $\frac{1}{\textrm{years}}$ & \Sexpr{gamma.phila.fitode} &
      \Sexpr{ci_fmt("gamma", ci.phila.fitode)} \\
    initial\break susceptibles & $S_0$& -- &
      \Sexpr{as.integer(S0.phila.fitode)} & \Sexpr{ci_fmt("S0", ci.phila.fitode, max_ord = 6)}\\
      initial prevalence & $I_0$ & -- &
      \Sexpr{I0.phila.fitode} & \Sexpr{ci_fmt("I0", ci.phila.fitode)}\\
      overdispersion\break parameter & $k$ & -- &
      \Sexpr{k.phila.fitode} & \Sexpr{ci_fmt("k", ci.phila.fitode)}\\
      \noalign{\vspace{10pt}}
      \bfseries Assumed\break parameter \\\hline
      effective population size & $N$ & -- &
      \Sexpr{prettyNum(philaS0, big.mark=",")} &  -- \\
      \noalign{\vspace{10pt}}
      \bfseries Derived\break parameter \\\hline
      effective reproduction number & $\Reff$ & -- &
      \Sexpr{Reff.phila.fitode} &  \Sexpr{ci_fmt("Reff", ci.derived.phila.fitode[,-1])} \\
      mean generation interval & $\Tg$ & days &
      \Sexpr{signif(Tg.phila.fitode*365,dig)} & \Sexpr{ci_fmt("Tg", ci.derived.phila.fitode[,-1]*365)} \\
    basic reproduction number & $\R_0$ & -- &
      \Sexpr{signif(R0.phila.fitode,dig)}
      &  \Sexpr{ci_fmt("R0", ci.derived.phila.fitode[,-1])}
  \end{tabular}
  \end{center}
\end{table}
%%\end{landscape}


\begin{figure}
<<phila-figure, echo=FALSE, fig.height=5>>=
setup_plot(ylim = c(0, 800), xlim = range(phila1918b$time), ylab="daily P&I deaths",
           xaxt='n')
axis(side=1, at=phila1918b$time[c(1, 31, 62)],
     labels=c("Sep", "Oct", "Nov"))
draw_confband(ppp)
tvals <- with(phila1918b, seq(min(time),max(time), length=1000))
t0 <- min(tvals)
tt_phila <- traj_ci(tvals-t0, phila.nls.parameters, vcov(phila.nlsfit))

tt_phila$times <- tt_phila$times + t0

draw_confband(tt_phila, col=col.nlsCI)
curve(KM_approx(x-t0, a=a.phila, omega=omega.phila, phi=phi.phila)/365, from=t0, to=max(tvals), n=1000, add=TRUE, xpd=NA, col=col.fitode.nb, lty="dotted", lwd=lwd*0.60)
lines(tvals,
      with(as.list(phila.nls.parameters),KM_approx(tvals-t0, a=a, omega=omega, phi=phi)),
      xpd=NA, lwd=lwd, col=col.nls)
lines(estimate ~ times, data=ppp, col=col.fitode.nb, lwd=lwd*0.8, lty="solid")
points(mort ~ time,
       data = phila1918b, xpd=NA, pch=21, bg="white", lwd=2)
legend("topleft", bty="n", lwd=c(2,lwd,lwd*0.8,lwd*0.60),
       lty=c(NA,"solid","solid","dotted"),
       col=c("black",col.nls,col.fitode.nb,col.fitode.nb),
       pch=c(21,NA,NA,NA),
       pt.bg=c("white",NA,NA,NA),
       legend=c("observed data","nls fit of KM approx",
                "fitode (nbinom)","KM approx to fitode fit"))
@
\caption{\textbf{The main wave of the 1918 influenza epidemic in the city of
  Philadelphia}, 1 September 1918 to 31 December 1918
  \cite{Roge20,Gold+09}.  Reported daily deaths from pneumonia and
  influenza (P\&I) are shown with dots.  The red curve and pink
  confidence band show a nonlinear least squares (\code{nls}) fit of
  \KM's approximation \eqref{eq:sech}; the parameter estimates are
  given in \cref{tab:philanls}.  The solid yellow curve and light
  yellow confidence band show the \code{fitode} fit of the SIR model
  \eqref{eq:SIR}, for which the parameter estimates are given in
  \cref{tab:phila.fitode}.  The dotted yellow curve shows the \KM
  approximation using the parameters estimated with \code{fitode}.
}
\label{fig:phila}
\end{figure}

%%\begin{landscape}
\begin{table}
  \begin{center}
    \caption{\textbf{Fits of \KM's \cite{KermMcKe27} analytical SIR
      approximation \eqref{eq:sech} to an epidemic simulated using the
      standard stochastic SIR model \cite{AndeBrit00b}} (see
      \cref{sec:stoch,fig:stoch}).
      %%
      The parameter values in the ``true'' column are those used to
      generate the stochastic simulation ($S_0$, $I_0$, $\R_0$ and
      $\Tg$) and the values of other parameters derived from these
      true parameter values using the indicated equations.  The
      \code{nls} column lists our estimates and confidence intervals
      obtained by fitting \cref{eq:sech} to the simulated data using
      nonlinear least squares and the Delta method.
    }\label{tab:stoch.nls}
    \medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c | c | c c}
  \bfseries Assumed\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries equation} & {\footnotesize\bfseries units} & {\footnotesize\bfseries true}
    & {\footnotesize\bfseries \code{nls}} & {\footnotesize\bfseries 95\% CI} \\\hline
    initial prevalence & $I_0$ & -- & -- &
      \Sexpr{iii[[2]]} & \Sexpr{iii[[2]]} & -- \\
    %%
    \noalign{\vspace{10pt}}
    \bfseries Estimated\break parameter \\\hline
    peak removal rate & $a$& \eqref{eq:a} & $\frac{1}{\textrm{weeks}}$ & \Sexpr{signif(a.det,dig)} &
      \Sexpr{signif(a.nls.stoch,dig)} & \Sexpr{ci_fmt("a", cc.nls.stoch)} \\
    outbreak speed & $\omega$ & \eqref{eq:omega} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{signif(omega.det,dig)} & \Sexpr{signif(omega.nls.stoch,dig-1)} & \Sexpr{ci_fmt("omega", cc.nls.stoch)} \\
    outbreak centre & $\phi$ & \eqref{eq:phi} & -- & \Sexpr{signif(phi.det,dig)} &
      \Sexpr{signif(phi.nls.stoch,dig)} & \Sexpr{ci_fmt("phi", cc.nls.stoch)} \\
    \noalign{\vspace{10pt}}
    \bfseries Derived\break parameter \\\hline
    peak time & $\tpeak$ & \eqref{eq:tpeak} & weeks &
      \Sexpr{tpeak.KM.stoch} & \Sexpr{tpeak.nls.stoch} & \Sexpr{ci_fmt("tpeak", cc.delta.stoch)} \\
    effective reproduction number & $\Reff$
      & \eqref{eq:Reffdef}, \eqref{eq:Reff} & -- &
      \Sexpr{Reff.KM.stoch} & \Sexpr{Reff.nls.stoch} & \Sexpr{ci_fmt("Reff", cc.delta.stoch)} \\
    removal rate & $\gamma$ & \eqref{eq:gamma} & $\frac{1}{\textrm{weeks}}$ &
      \Sexpr{gamma.KM.stoch} & \Sexpr{gamma.nls.stoch} & \Sexpr{ci_fmt("gamma", cc.delta.stoch)} \\
    initial\break susceptibles & $S_0$ & \eqref{eq:S0} & -- &
      \Sexpr{as.integer(S0.KM.stoch)} & \Sexpr{as.integer(S0.nls.stoch)} & \Sexpr{ci_fmt("S0", cc.delta.stoch, max_ord = 6)} \\
    transmission rate & $\beta$ & \eqref{eq:beta} & $\frac{1}{\textrm{years}}$ &
      \Sexpr{52*beta.KM.stoch} & \Sexpr{signif(52*beta.nls.stoch,dig)} & \Sexpr{ci_fmt("beta", 52*cc.delta.stoch)} \\
    mean generation interval & $\Tg$ & \eqref{eq:Tg} & days &
      \Sexpr{signif(7*Tg.KM.stoch,dig)} & \Sexpr{signif(7*Tg.nls.stoch,dig)} & \Sexpr{ci_fmt("Tg", 7*cc.delta.stoch)} \\
    basic reproduction number & $\R_0$ & \eqref{eq:R0def} & -- &
      \Sexpr{signif(2000*Reff.KM.stoch/S0.KM.stoch,dig)}
      & \Sexpr{signif(2000*Reff.nls.stoch/S0.nls.stoch,dig)} & \Sexpr{ci_fmt("R0", cc.delta.stoch)}
  \end{tabular}
  \end{center}
\end{table}
%%\end{landscape}

%%\begin{landscape}
\begin{table}
  \begin{center}
    \caption{\textbf{Fits of numerical (deterministic) SIR model solutions to
      an epidemic simulated using the standard stochastic SIR model
      \cite{AndeBrit00b}} (see \cref{sec:stoch,fig:stoch}). Parameter
      estimates we obtained using \code{fitode} to fit the SIR model
      \eqref{eq:SIR} to the simulated data, assuming deviations from
      the deterministic curve were generated by negative binomially
      \eqref{eq:NB} distributed observation errors.
    }\label{tab:stoch.fitode}
\medskip
    %% making use of ragged2e and array packages:
    \RaggedRight
  \begin{tabular}{ m{3cm} | c | c | c | c c}
  \bfseries Estimated\break parameter & {\footnotesize\bfseries symbol}
    & {\footnotesize\bfseries units} & {\footnotesize\bfseries true}
    & {\footnotesize\bfseries \code{nbinom}} & {\footnotesize\bfseries 95\% CI} \\\hline
    transmission rate & $\beta$ & $\frac{1}{\textrm{years}}$ & \Sexpr{52*R0*mm$gamma/2000} &
      \Sexpr{signif(52*beta.stoch.fitode,dig)} & \Sexpr{ci_fmt("beta", 52*ci.stoch.fitode, sn=FALSE, digits=dig)} \\
    recovery rate & $\gamma$ & $\frac{1}{\textrm{weeks}}$ & \Sexpr{mm$gamma} &
        \Sexpr{gamma.stoch.fitode} & \Sexpr{ci_fmt("gamma", ci.stoch.fitode)}\\
    initial\break susceptibles & $S_0$& -- & \Sexpr{iii[[1]]} &
      \Sexpr{as.integer(S0.stoch.fitode)} & \Sexpr{ci_fmt("S0", ci.stoch.fitode, max_ord = 6)}\\
      initial prevalence & $I_0$ & -- & \Sexpr{iii[[2]]} &
      \Sexpr{I0.stoch.fitode} & \Sexpr{ci_fmt("I0", ci.stoch.fitode)}\\
      overdispersion\break parameter & $k$ & -- & -- &
      \Sexpr{k.stoch.fitode} & \Sexpr{ci_fmt("k", ci.stoch.fitode, sn=FALSE, digits=dig)}\\
      \noalign{\vspace{10pt}}
      \bfseries Derived\break parameter \\\hline
      mean generation interval & $\Tg$ & days & \Sexpr{7/mm$gamma} & \Sexpr{signif(7*1/gamma.stoch.fitode,dig)} & \Sexpr{ci_fmt("gamma", 7*1/ci.stoch.fitode)}\\
      effective reproduction number & $\Reff$ & -- & \Sexpr{R0*iii[[1]]/2000} &
      \Sexpr{Reff.stoch.fitode} &  \Sexpr{ci_fmt("Reff", ci.derived.stoch.fitode[,-1])} \\
    basic reproduction number & $\R_0$ & -- & \Sexpr{R0} &
      \Sexpr{signif(R0.stoch.fitode,dig)}
      &  \Sexpr{ci_fmt("R0", ci.derived.stoch.fitode[,-1])}
  \end{tabular}
  \end{center}
\end{table}
%%\end{landscape}

\begin{figure}
<<plot_stochastic_SIR, echo=FALSE, fig.height=5, fig.show="hold", out.width="95%">>=
my.tmax <- 8
plot(NA, NA, bty="L", xlab="", ylab="", las=1, xaxs="i", yaxs="i",
     ##xlim=range(tt),
     xlim=c(0,my.tmax), # FIX: hack: max(tt) in sirstoch.RData is 10, which is too large
     ylim=range(sir.stoch.data$mort, na.rm=TRUE))
draw_confband(sss, col=col.fitodenbCI)
lines(estimate ~ times, data=sss, col=col.fitode.nb, lwd=lwd)
lines(sir.det$tau, 1 * sir.det$I * N/7, col=my.green, lwd=lwd*0.8) ## dR/dt
curve(KM_approx(x, a=a.det, omega=omega.det, phi=phi.det)/7, from=0, to=my.tmax, n=1000, add=TRUE,
      xpd=NA, col=my.green, lty="dotted", lwd=lwd*0.6)
points(mort ~ time, data = sir.stoch.data, xpd=NA, pch=21, bg="white", lwd=2)
legend("topright", bty="n", lwd=c(2,lwd,lwd*0.8,lwd*0.6),
       lty=c(NA,"solid","solid","dotted"),
       col=c("black",col.fitode.nb,my.green,my.green),
       pch=c(21,NA,NA,NA),
       pt.bg=c("white",NA,NA,NA),
       adj = c(0, 0.8),  ## shift text so first line aligns with guide line
       legend=c("simulated data","fitode (nbinom)","deterministic","KM approx\nto deterministic fit"))

plot(NA, NA, bty="L", xlab="", ylab="", las=1, xaxs="i", yaxs="i",
     ##xlim=range(tt),
     xlim=c(0,my.tmax), # FIX: hack: max(tt) in sirstoch.RData is 10, which is too large
     ylim=range(sir.stoch.data$mort, na.rm=TRUE))
draw_confband(traj_ci((0:(my.tmax*7))/7, nls.stoch.parameters, vcov(nlsfit.stoch)), col=col.nlsCI)
draw_confband(sss, col=col.fitodenbCI)
lines(estimate ~ times, data=sss, col=col.fitode.nb, lwd=lwd)
curve(KM_approx(x, a=a.nls.stoch, omega=omega.nls.stoch, phi=phi.nls.stoch),
      from=0, to=my.tmax, n=1000, add=TRUE,
      xpd=NA, col=col.nls, lwd=lwd*0.8, lty="solid")
points(mort ~ time, data = sir.stoch.data, xpd=NA, pch=21, bg="white", lwd=2)
legend("topright", bty="n", lwd=c(2,lwd,lwd*0.8),
       lty=c(NA,"solid","solid"),
       col=c("black",col.fitode.nb,col.nls),
       pch=c(21,NA,NA),
       pt.bg=c("white",NA,NA),
       legend=c("simulated data","fitode (nbinom)","nls fit of KM approx"))
@
\caption{\textbf{Deterministic fits to daily incidence generated by a
  stochastic SIR simulation} with initial state
  $(S_0,I_0,R_0)=(1998,2,0)$, basic reproduction number $\R_0=5$, and
  mean generation interval $\Tg=1$ week.
  The simulated data points show the numbers of newly recovered
  individuals each day.
  %%
  In both panels, the yellow curve and confidence band show the
  \code{fitode} fit to the simulated data.  \emph{Top panel:} The
  solid green curve shows the solution of deterministic SIR model
  \eqref{eq:SIR} with the initial conditions and parameters used for
  the stochastic simulation.  The dotted green curve shows the \KM
  approximation \eqref{eq:sech} to this deterministic trajectory.  The
  time shift between the green and yellow curves arises because there
  is a random delay until the stochastic trajectory begins to grow
  exponentially.
  %%
  \emph{Bottom panel:} The red curve shows the \KM approximation
  \eqref{eq:sech}, fitted to the stochastic simulation using \code{nls}.
  Since the \KM approximation is symmetric about its maximum, it is
  impossible to obtain a good fit in situations like this, where
  the rise of the epidemic is faster than the fall.
}\label{fig:stoch}
\end{figure}

\pagebreak

%% \section*{Supplementary figures}
%% \setcounter{figure}{0}
%% \renewcommand{\thefigure}{S\arabic{figure}}

\begin{figure}[!h]
\includegraphics[width=\textwidth]{../misc/bombay_confint_dnbinom.pdf}
\caption{\textbf{Unidentifiability of the mean generation interval
    $\Tg$ (or, equivalently, the removal rate $\gamma$) for the Bombay
    plague epidemic shown in \cref{fig:Bombay}.}
    %%
  (\textsf{A}) The profile likelihood---briefly discussed at the end
  of \cref{sec:Uncertainty}---is calculated by fixing $\gamma$ to a
  series of given values and, for each value,
  maximizing the likelihood by estimating all other
  parameters \cite{Bolk08}. (The maximum value is shifted to 0
  without loss of generality.) A flat profile-likelihood surface
  indicates parameter unidentifiability, meaning that we can obtain
  very similar fits across a wide range of values
  of the focal parameter ($\gamma$).  (\textsf{B--E}) The
  corresponding best parameter estimates for a given value of
  $\gamma$.}\label{fig:proflikgamma}
\end{figure}

\end{document}
