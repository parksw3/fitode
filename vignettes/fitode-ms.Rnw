\documentclass[12pt]{article}
%% vignette index specifications need to be *after* \documentclass{}
%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{A beginner's guide to fitting ordinary differential equations to data}
%\VignettePackage{fitode}
%\VignetteDepends{ggplot2}
%% alternative class: \documentclass[nojss]{jss}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{amsmath}
\bibliographystyle{chicago}
%% https://tex.stackexchange.com/questions/59702/suggest-a-nice-font-family-for-my-basic-latex-template-text-and-math/59706
\usepackage{mathpazo}
%% https://tex.stackexchange.com/questions/5223/command-for-argmin-or-argmax
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\code}[1]{{\tt #1}}
\newcommand{\pkg}[1]{\href{https://CRAN.R-project.org/package=#1}{{\tt #1}}}
\newcommand{\bmb}[1]{{\color{red}$\langle$\emph{BMB: #1}$\rangle$}}
\newcommand{\swp}[1]{{\color{blue}$\langle$\emph{SWP: #1}$\rangle$}}
\newcommand{\djde}[1]{{\color{magenta}$\langle$\emph{DJDE: #1}$\rangle$}}
\newcommand{\thickredline}{{\color{red}\bigskip\begin{center}\linethickness{2mm}\line(1,0){250}\end{center}\bigskip}}

\title{A beginner's guide to fitting ordinary differential equations to data}
\author{Sang Woo Park \and Benjamin M. Bolker \and David J. D. Earn
  \thanks{Department of Mathematics \& Statistics, McMaster
    University, Hamilton, Ontario, Canada}}
\date{\today}
\begin{document}
\maketitle

\textbf{Target audience:} students and researchers with some degree
of mathematical sophistication who aren't necessarily great with statistics. They know a reasonable amount about ODEs, probably a little bit about numerical integration, maybe next to nothing about maximum likelihood/process vs observation error/state space models/MCMC/etc..

\textbf{Target journal:} R Journal? JSS? Bull Math Biol? Methods in Ecol \& Evol?

<<setup,include=FALSE>>=
knitr::opts_chunk$set(error=FALSE)  ## stop on error
@
\section{Introduction}

Mathematical models based on ordinary differential equations (ODEs) are
ubiquitous in science and engineering. Researchers studying ODEs are
often highly skilled in dynamical systems theory, but many lack experience
estimating the parameters of their models from data.  When confronted
with observations from the real world, how should one go about fitting
an ODE model to the data?

In principle, the answer is straightforward: compute many
solutions of the ODEs (considering a large range of parameter values and
initial states) and identify the solution curve that is closest to
the observed data.  This approach is sometimes feasible, for simple enough problems.  More often than not, however, fitting an ODE to data is a challenging statistical and computational problem.  In particular:
\begin{itemize}
\item The dimension of the combined of parameter and initial states is
  rarely less than 4 and often much larger, making a na\"ive search
  for the best fitting solution computationally overwhelming.
\item Different measures of goodness of fit may identify different
  ``best'' solutions.
\item Multiple solutions, associated with different regions of
  parameter space, may fit the data equally well, or nearly equally well.
\item Statistical inference --- testing for clear evidence of the operation of a particular process, attaching a measure of confidence to a parameter estimates, or putting confidence intervals on predictions --- requires
  additional knowledge or assumptions about the stochastic processes that generate the variation of observed data around the best-fit solution.
\end{itemize}
As one attempts to address these issues, more subtle challenges arise
and one can quickly reach the limits of the state of the art.
Making meaningful mechanistic inferences from ODE fits to data
can be difficult, especially when fitting complex models to small,
noisy data sets.

At this point you may want to give up and work on something else, but
we hope you won't.  The situation isn't \emph{that} bad.  Our goal
here is to provide---and to present a pedagogical introduction to---an
\texttt{R} package that makes fitting many ODEs to data relatively
painless.  Simultaneously, we aim to explain the
uncertainties and limitations inherent in ODE-fitting, and provide
convenient tools and references for users who wish to delve more
deeply into the subject.

\djde{The general style I'm imagining is similar to \emph{Numerical
    Recipes \citep{Pres+92}}, i.e., provide tools that work but also
  explain clearly and concisely why they work, what they are doing,
  and under what circumstances they can be expected to fail.}

\thickredline

\section{Trajectory matching: from simple to complex}

\subsection{Least-squares fitting}

% \newcommand{\traj}{\ensuremath{{\cal T}}}
\newcommand{\params}{\ensuremath{\boldsymbol{\beta}}}
\newcommand{\state}{\ensuremath{\boldsymbol{y}}}
\newcommand{\traj}{\ensuremath{\hat \state}}

In order to find a solution of an ODE model that is closest to the observed data,
we have to first decide on a measure of the distance between the solution and the data.
One of the most basic measures we can use is the sum of squared differences between the two.
The least-squares solution of an ODE model (i.e., the solution that minimizes the sum of squared differences) will visually resemble the patterns in the data to some extent;
therefore, we can expect it give us a biologically sensible set of parameters.

Suppose we have (1) a gradient function that represents the derivatives $\frac{d\state}{dt}$; (2) a method for numerical integration; (3) a set of observations $\state(t_i)$ where $i = 1, \dots, n$ (for simplicity, from a single time series with $n$ observations); (4) a method for numerical minimization of an objective function.
Define a trajectory (a solution of the ODE for specified parameters $\params$ [and initial conditions] and time points $t_i$) as $\traj(t_i|\params)$.
Then, the method of least-squared fitting seeks to find a parameter vector, $\hat \params$, that minimizes the following objective function: $\sum_{i=1}^n \left( \state(t_i) - \traj(t_i|\params)) \right)^2$.
Such a vector can be formally defined as
\begin{equation}
\hat \params =  \argmin_{\params} \sum_{i=1}^n \left( \state(t_i) - \traj(t_i|\params)) \right)^2.
\end{equation}
The actual process of finding $\hat \beta$ will depend on the minimizer of our choice.

\begin{description}
\item[gradient functions]{should be straightforward (you know what you
  want to model, right? \ldots)}
\item[observations]{as noted above, we're assuming a single time series for now.  Regularity of sampling isn't much of a problem. It's OK to measure a subset of the state variables (e.g. just prevalence of infected in an SIR model, or just predators in a pred-prey model). May need some care to make sure you're comparing observations with ODE properly (e.g. do observations represent measurements of state variables, rates, or integrals of rates? Measuring cumulative properties may have tricky statistical consequences later \ldots)}
\item[integrator]{for simple problems, anything should do. Open question as to whether
  simple/stable (e.g. RK4) is better than something more sophisticated (e.g. LSODA)
  \bmb{in general, the easier the problem, the less the details matter!}}
\item[minimizer]{similar to integrator, i.e. details don't matter if the problem is easy. Derivative-free methods such as Nelder-Mead are robust but slower; derivative-based methods are effective \emph{if sensitivity equations are used} (see below). Some minimizers such as Levenberg-Marquardt are particularly tuned to least-squares (although derivs still matter)}
\item[initial conditions]{if all variables are observed, we can
    pretend the initial observations as known without error and set
    $\hat \state(0) = \state(0)$. Otherwise we have to treat ICs as
    auxiliary parameters.\djde{definition of auxiliary parameters? reference?}}
\end{description}
\swp{Do we just want to put definitions here? Not sure what you're exactly looking for here...}

\swp{Is it OK to extract data from a figure and put it in the package as long as we cite it? This seems like a good example here...}
For example, consider a time series of human tumor cell (GI-101A) growth within a nude mice \citep{worschech2009systemic};
the time series data was extracted from the original article (Figure 1A, control data) using WebPlotDigitizer and is included in the \code{fitode} package (see \code{tumorgrowth}).
<<tumorplot, fig.width=6, fig.height=4, message=FALSE>>=
library(fitode)
plot(tumorgrowth)
@

\noindent
\cite{murphy2016differences} analyzed this time series by fitting seven different ODE models (i.e., exponential, Mendelsohn, logistic, linear, surface, Bertalanffy, and Gompertz) via least-squares and comparing their model predictions.
Here, we demonstrate the fitting of the exponential growth model.

In order to fit the exponential growth model using least-squares, we have to set up the ODE model using an \code{odemodel} function in \code{fitode}.
In particular, we have to specify gradient functions (\code{model} argument), an observation model (\code{observation} argument), initial conditions (\code{initial} argument), and parameters (\code{par} argument) of the model.
The gradient function is simple:
\begin{equation}
\frac{dA}{dt} = rA,
\end{equation}
where $r$ is the exponential growth rate.
In \code{fitode}, gradient functions are expressed as a list of \emph{formulas} (the left hand side (LHS) of the formula determines the state variable and the right hand side (RHS) of the formula determines its gradient function):
<<expgrad>>=
model=list(A ~ r*A)
@
\noindent
Likewise, observation model should specify that we want to minimize the squared sum of differences between the observed \code{volume} and the trajectory of \code{A} (the LHS of the formula determines the observed variable and the RHS of the formula determines the objective function):
<<expobs>>=
observation=list(volume ~ ols(mean=A))
@
\noindent
We have to specify the initial condition for our state variable:
<<expini>>=
initial=list(A~A0)
@
\noindent
Finally, we have to specify all parameters (including the initial conditions) of the model.
In this case, we have two parameters: the exponential growth rate, \code{r}, and the initial condition, \code{A0}.
<<exppars>>=
par=c("r", "A0")
@
\noindent
Putting them altogether, we can create an \code{odemodel} object as follows:
<<expmodel>>=
exp_model <- odemodel(
  model=list(A ~ r*A),
  observation=list(volume ~ ols(mean=A)),
  initial=list(A~A0),
  par=c("r", "A0")
)
@

Before we can start fitting this model, we have to specify a starting parameter for the optimization.
In principle, we can try out multiple starting parameters to test for convergence;
this is not necessary for sufficiently simple models.
For now, we will leave the discussion on starting parameters and pick a starting parameter based on visual inspections:
<<exppick, fig.width=6, fig.height=4>>=
start <- c(r=0.03, A0=150)
ss <- simulate(exp_model, parms=start, times=tumorgrowth$day)
plot(tumorgrowth)
lines(A~times, data=ss)
@

To fit the model, we have to call the \code{odemodel} object through the \code{fitode} function.
<<expfit, message=FALSE>>=
tumor_exp_fit <- fitode(
    model=exp_model,
    data=tumorgrowth,
    start=start,
    tcol="day"
)
@
\noindent
Once the model has been fitted, we can use the \code{plot} function to plot the estimated trajectory along with the data:
<<expfitplot, fig.width=6, fig.height=4>>=
plot(tumor_exp_fit)
@

\noindent
Finally, we can obtain the parameter estimates by using the \code{coef} function:
<<expfitcoef, fig.width=6, fig.height=4>>=
coef(tumor_exp_fit)
@
\bmb{what is our convention for listing functions? \code{fun} or \code{fun()}?}

You might think that this fit is good enough and decide to carry on with our analysis of the ODE model using the estimated parameters.
Least-squares fits provide useful solutions (e.g. for making predictions), but we need more structure if we want to make inferences on the model parameters, or assess the uncertainty of our estimates or predictions.
% BMB: don't think we need this?
% That said, by avoiding inference, you also avoid having to think much about the characteristics of the observations (independence, distribution, etc.).
We hope that you think about these characteristics carefully and continue to follow us through this guide.

\subsection{Gaussian errors}

Suppose we are willing to assume that the residuals ($\state(t_i) - \traj(t_i|\params)$) are \emph{independent}, \emph{identically distributed} Gaussian random variables -- i.e.,
\begin{equation}
\state(t_i) \sim \mathcal{N}(\traj(t_i|\params), \sigma^2)
\end{equation}
where $\sigma^2$ is the variance of the Gaussian distribution.
(This assumption is sometimes equivalently expressed as $\state(t_i) = \traj(t_i) + \varepsilon$, where $\varepsilon \sim \mathcal{N}(0, \sigma^2)$.)
Then we can start making statistical inferences: constructing confidence intervals on parameters and predictions, testing statistical hypotheses about parameters, etc..
\bmb{left out ``prediction intervals'' --- feels like too much detail}
Assuming independence of successive residuals typically means we are assuming that the noise in the data is \emph{observation error};
that is, the underlying process is deterministic (and smooth), and all the noise in the data comes from (independent) imperfect observations of the process.

Under these assumptions, the likelihood (i.e., the probability of the observed data) can be written as follows:
\begin{equation}
\mathcal L(\state(t_i)|\params) = \prod_{i=1}^n \left( \frac{1}{\sigma \sqrt{2\pi}} \exp \left(-\frac{1}{2 \sigma^2} \left(\state(t_i) - \traj(t_i|\params)\right)^2 \right)\right),
\end{equation}
where the parameter set $\params$ now includes parameters of the ODE model, initial conditions, and the variance term $\sigma^2$ of the Gaussian distribution as an auxiliary parameter (i.e., $\sigma^2 \in \params$).
Our goal is to find a set of parameters $\params$ that maximizes the likelihood;
such a set of parameters is referred to as the maximum likelihood estimate (MLE).
Equivalently, the MLE can be expressed as a parameter that minimizes the negative log-likelihood function, which is now our new measure for the distance between the solution and the data:
\begin{equation}
\hat{\beta} = \argmin_{\params} \left( - \frac{n}{2} \log (2 \pi \sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n \left(\state(t_i) - \traj(t_i|\params)\right)^2\right),
\end{equation}
which shows that the Gaussian errors are equivalent to \emph{scaled} least-squares errors.
Using the negative log-likelihood also results in a more stable
numerical computation \djde{Why?} and useful statistical
properties\djde{such as?  why would such a transformation change
the statistical properties?}.

There are several ways in which we can fit an ODE model under the Gaussian assumption.
First, we can estimate all parameters at once.


\begin{itemize}
\item we can fit via explicit Gaussian (with or without normalization terms),
  profiled Gaussian (set $\hat \sigma = \textrm{RSS}/n$), or quasi-likelihood (fit
  with $\sigma=1$, then estimate $\hat \sigma$ from final value of RSS)
\item once we have a Gaussian fit, we can use standard methods (Fisher information/Wald)
  to estimate parameter CIs [still assuming iid Gaussian]
\item for CIs on predictions or forecasts, use delta method or [whatever it's called when we sample from a MVN sampling distribution of parameters; par. bootstrap? PPI?] or importance sampling or \ldots
\item now we are assuming independence, so measuring cumulative variables will result in incorrect inference (cf. ODE papers in epidemiology that do this carelessly??)
\end{itemize}

<<update_ex, message=FALSE>>=
update(tumor_exp_fit,
       observation=list(volume ~ dnorm(mean=A, sd=sigma)),
       start=c(coef(tumor_exp_fit), sigma=1),
       par=c("r", "A0", "sigma"))
@

\subsection{non-Gaussian errors}

Once we've gotten this far, it's fairly easy to extend the model to cover conditional distributions other than Gaussian. For example, we often have count data that would naturally be modeled by a Poisson or negative binomial distribution.

EXAMPLE? (does \code{fitode} have an \code{update} method? that would be
a cute way to make compact examples)

This doesn't raise very many additional problems.

\thickredline

\section{Increasing robustness and efficiency}

So far we've assumed that the numerical methods should Just Work.
People often have really crappy data (e.g., very short time series, complex models).
In R, we're typically depending on derivatives computed by finite differences.

\subsection{Sensitivity equations}

Don't forget to cite \cite{raue2013lessons}.

\subsection{Link functions}

aka, transforming parameters to unconstrained scales.
Don't know if this deserves its own section, but it's a good idea.
Generally improves scaling, applicability of Wald approximations, robustness.

The only caveat (besides increased complexity) is that if a parameter ever
ends up on the boundary of its feasible space, then transforming to an
unconstrained space will push the optimum to infinity, leading to problems \ldots
box constraints may be better in this case, although Wald-based inference will
break in any case.

\subsection{Self-starting models}

\code{fitode} doesn't do this at present; demote to an ``other'' paragraph?

\subsection{Compiled models}

\code{fitode} doesn't do this at present; demote to an ``other'' paragraph?
See \href{https://cran.r-project.org/package=deSolve/vignettes/compiledCode.pdf}{deSolve vignette on compiled gradient functions}: also cf. \pkg{nimble}, \pkg{odeintr}, \pkg{cOde}, \pkg{dMod}, \pkg{rodeo}, others?

Speed is important when using simulation-based methods (SMC, ABC, etc.;
see below) or when fitting a large number of series. (Sensitivity-based
methods can help improve speed, but not as much as compilation.)

\subsection{Multi-start methods}

Slow (hence faster integration would be nice) but deals with multi-modality.
Again cf. \cite{raue2013lessons}: Latin hypercube, Sobol, etc..  They discuss diagnosis of
multi-modality as well.

\subsection{Other}

Should we include regularization here?  Box constraints are a cheaper/easier possibility.
Regularization would make a nice lead-up to Bayesian methods.

\section{Beyond frequentist models}

\bmb{hard to decide on the best order: beyond-freq (i.e. Bayesian) then beyond-trajectories or vice versa? Reasons you would need these do overlap, although not completely}

Discuss advantages of Bayesian approach (priors, integration over uncertainty). Need to think about sampling (Gibbs, Metrop-Hastings, HMC \ldots)

Stan, boms (??), \pkg{deBInfer}

\section{Beyond trajectory-matching}

\subsection{gradient matching etc. (process error only?)}

\pkg{CollocInfer}, papers by Hooker/Ellner/etc.; Kim McAuley

\subsection{state-space models}

Huge range here. Sequential Monte Carlo (pomp etc.). Kalman/extended Kalman filters?
\cite{heydari2014fast} (SDEs + Bayesian + Kalman filter)

Mention ABC/synthetic likelihood?

\subsection{random effects/mixed models}

What do we want to say here? Easier in Bayesian-toolbox contexts such as Stan. \pkg{nlmeODE}; \cite{tornoe2004nonlinear} \ldots

\section{Package comparison}

\section{Miscellaneous}

\begin{itemize}
\item Appropriate citations to PKPD literature, e.g. \pkg{RxODE}, \cite{wang2016tutorial} (although this looks like it is a simulation rather than fitting tool?). In many cases they use particular models with nonlinear but known solutions, e.g. two-compartment linear models.
\item cite that mixed-ODE paper we reviewed.
\item Discuss identifiability/estimability?
\end{itemize}

\bibliography{fitode}
\end{document}
