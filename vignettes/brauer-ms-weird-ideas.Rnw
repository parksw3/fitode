\subsection{DJDE's weird idea of introducing na\"ive confidence bands
  via residual errors only}

One standard way to derive a confidence band is the following: We
imagine that the model \eqref{eq:sech} is a perfect representation of
reality, and we consider the deviations from the model curve in
\cref{fig:Bombay} to be observation errors.  We then imagine
that observation error for each data point is independent and
identically distributed (iid), and drawn from a Normal distribution
with zero mean and standard deviation $\sigma$ equal to the
standard deviation of the residuals (the differences between the model
curve and the observed data).  With this assumption, we could
na\"ively draw confidence bands by adding $\pm1.96\,\sigma$ to our
fitted model curve,\footnote{For a $\normdist(0,\sigma)$
  distribution, 95\% of the probability lies between $-1.96\,\sigma$
  and $+1.96\,\sigma$.}  which would emphasize the folly in assuming
the observation error has the same absolute magnitude throughout the
epidemic (in particular the lower limit would go below zero early and
late in the epidemic).
\bmb{From my point of view this is a little bit weird because it is the
  \emph{opposite} of what is traditionally done in statistics. A \emph{confidence interval}
  on the predictions takes only uncertainty in the parameters into account;
  a \emph{prediction interval} accounts for both parametric uncertainty and
  residual error/observation uncertainty (in traditional/simple approaches, we
  don't consider stochastic processes at all, so parametric uncertainty and
  residual error are the only components of uncertainty). What you're suggesting here,
  taking account only of the residual error, isn't something I've ever seen anyone
  do. That's not to say that we can't proceed this way, but it seems weird. Also,
  opening the can of worms of model misspecification at this point also seems dangerous;
  I'd rather come back to it in a later part of the discussion.}

\subsection*{Model misspecification}

Bear in mind that at each step of this approach, we are making a very
specific, strong assumptions (when we ``imagine'').  To begin with, it
is absurd to suggest that KM's approximation \eqref{eq:sech} to the
solution of the SIR model \eqref{eq:SIR} is a perfect representation
of reality!  The ODEs themselves are far from realistic, not least
because they ignore the discrete nature of individuals and the
stochastic nature of infection and removal events \cite{Bart60,AndeBrit00b}.
Nevertheless, we will continue to focus on estimating uncertainty in
parameter estimates for a given model, since it will be necessary even
if we ignore uncertainty in the specification of the model itself.

\subsection*{DJDE's pseudo-Bayesian introduction to maximum likelihood}

One way to get at this is to use Bayes rule for conditional
probabilities, which tells us that
\begin{linenomath*}
\begin{equation}\label{eq:BayesRule}
\Pop(\text{model} \mid \text{data})
= \dfrac{\Pop(\text{data} \mid \text{model})\cdot\Pop(\text{model})}{\Pop(\text{data})}
\,.
\end{equation}
\end{linenomath*}
The probability of the data is not known to us, but it can't depend on
the values of the parameters of the model that we happen to be
proposing, so we can think of $\Pop(\text{data})$ as a nuisance
constant in \cref{eq:BayesRule}.  Consequently, if we ignore it,
we still have a function that tells us something about how likely it is
that our model is good.  With this motivation, we define the
\term{likelihood} (not probability) of the model given the data to be
\begin{linenomath*}
\begin{equation}\label{eq:lik.generic}
\lik(\text{model} \mid \text{data})
= \Pop (\text{data} \mid \text{model}) \cdot \Pop(\text{model})\,.
\end{equation}
\end{linenomath*}
i.e.,
\begin{linenomath*}
\begin{equation}\label{eq:likB}
\lik(\thetavec)
= \Pop (\{y_i\} \mid \thetavec) \cdot \Pop(\thetavec)\,.
\end{equation}
\end{linenomath*}
If we have no prior information about the values of the model
parameters then $\Pop(\text{model})$ can be considered another
nuisance constant that we can ignore.  In that situation, maximizing
$\lik$ with respect to $\thetavec$ or, equivalently, minimizing the
negative log-likelihood
\begin{linenomath*}
\begin{equation}\label{eq:negloglikB}
\min_\thetavec\big(\!\!-\log{\lik(\thetavec)}\big) = \min_\thetavec\Big(
\sum_{i=1}^n \big(f(t_i;\thetavec) - y_i\big)^2
\;+\; \text{constant}
\Big)
\end{equation}
\end{linenomath*}
yields an estimate, say $\thetavechat$, that---lo and behold---agrees
exactly with \eqref{eq:leastsquares}, the least squares solution!  The
standard way of expressing this is to say that the least squares
solution $\thetavechat$ is the \term{maximum likelihood estimate} (MLE)
of $\thetavec$, under the assumption of independent,
identically distributed (i.e. mean-zero, constant-$\sigma$)
Normal observation errors in
the time series.

